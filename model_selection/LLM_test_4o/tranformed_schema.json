{
    "gpt2": {
        "model_name": "GPT-2",
        "developed_by": "OpenAI",
        "model_type": "Causal Language Modeling (CLM)",
        "licensing": "Unknown",
        "installation": {
            "python_version": "Python 3.6+",
            "additional_libraries": "transformers",
            "installation_command": "pip install transformers"
        },
        "usage": {
            "cli_example": "N/A",
            "python_example": ">>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='gpt2')\n>>> set_seed(42)\n>>> generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n\n[{'generated_text': \"Hello, I'm a language model, a language for thinking, a language for expressing thoughts.\"},\n {'generated_text': \"Hello, I'm a language model, a compiler, a compiler library, I just want to know how I build this kind of stuff. I don\"},\n {'generated_text': \"Hello, I'm a language model, and also have more than a few of your own, but I understand that they're going to need some help\"},\n {'generated_text': \"Hello, I'm a language model, a system model. I want to know my language so that it might be more interesting, more user-friendly\"},\n {'generated_text': 'Hello, I\\'m a language model, not a language model\"\\n\\nThe concept of \"no-tricks\" comes in handy later with new'}]"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": "GPT-2, GPT-Large, GPT-Medium, GPT-XL",
            "pretrained_datasets": "WebText",
            "performance_metrics": {
                "model": "GPT-2",
                "size_pixels": "N/A",
                "map_val50_95": "N/A",
                "speed_cpu_onnx_ms": "N/A",
                "speed_a100_tensorrt_ms": "N/A",
                "params_m": "124M",
                "flops_b": "N/A"
            }
        },
        "model_details": {
            "model_description": "GPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. It was trained to guess the next word in sentences. It uses a mask-mechanism to make sure the predictions for the token only use the inputs from 1 to i but not the future tokens. This way, the model learns an inner representation of the English language that can then be used to extract features useful for downstream tasks.",
            "supported_labels": "N/A"
        },
        "limitations_and_biases": {
            "limitations": "The training data contains a lot of unfiltered content from the internet, which is far from neutral. The model does not distinguish fact from fiction.",
            "biases": "Reflects biases inherent to the systems it was trained on. No statistically significant difference in gender, race, and religious bias probes between 774M and 1.5B parameters.",
            "risks": "Not recommended for deployment in systems that interact with humans unless a study of biases relevant to the intended use-case is carried out."
        },
        "recommendations": "You can use the raw model for text generation or fine-tune it to a downstream task. It is best at generating texts from a prompt.",
        "compute_infrastructure": {
            "hardware": "256 cloud TPU v3 cores",
            "software": "transformers library"
        },
        "contact_information": {
            "model_card_contact": "N/A"
        },
        "references": {
            "related_papers_and_resources": "Language Models are Unsupervised Multitask Learners by Radford et al., 2019"
        },
        "example_implementation": {
            "sample_code": "from transformers import GPT2Tokenizer, GPT2Model\n\n# PyTorch implementation\n\n>>> tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n>>> model = GPT2Model.from_pretrained('gpt2')\n>>> text = 'Replace me by any text you\\'d like.'\n>>> encoded_input = tokenizer(text, return_tensors='pt')\n>>> output = model(**encoded_input)\n\n# TensorFlow implementation\n\n>>> from transformers import GPT2Tokenizer, TFGPT2Model\n>>> tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n>>> model = TFGPT2Model.from_pretrained('gpt2')\n>>> text = 'Replace me by any text you\\'d like.'\n>>> encoded_input = tokenizer(text, return_tensors='tf')\n>>> output = model(encoded_input)"
        }
    },
    "openai-gpt": {
        "model_name": "openai-gpt",
        "developed_by": "Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever",
        "model_type": "Transformer-based language model",
        "licensing": "MIT License",
        "installation": {
            "python_version": "Python 3.6+",
            "additional_libraries": "transformers, torch or tensorflow",
            "installation_command": "pip install transformers torch tensorflow"
        },
        "usage": {
            "cli_example": null,
            "python_example": {
                "text_generation": "from transformers import pipeline, set_seed\n\ngenerator = pipeline('text-generation', model='openai-gpt')\nset_seed(42)\ngenerator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)",
                "pytorch": "from transformers import OpenAIGPTTokenizer, OpenAIGPTModel\nimport torch\n\ntokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")\nmodel = OpenAIGPTModel.from_pretrained(\"openai-gpt\")\n\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\noutputs = model(**inputs)\n\nlast_hidden_states = outputs.last_hidden_state",
                "tensorflow": "from transformers import OpenAIGPTTokenizer, TFOpenAIGPTModel\n\ntokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")\nmodel = TFOpenAIGPTModel.from_pretrained(\"openai-gpt\")\n\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\noutputs = model(inputs)\n\nlast_hidden_states = outputs.last_hidden_state"
            }
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": [
                "GPT2",
                "GPT2-Medium",
                "GPT2-Large",
                "GPT2-XL"
            ],
            "pretrained_datasets": "BooksCorpus dataset",
            "performance_metrics": {
                "model": "openai-gpt",
                "size_pixels": null,
                "map_val50_95": null,
                "speed_cpu_onnx_ms": null,
                "speed_a100_tensorrt_ms": null,
                "params_m": "unknown",
                "flops_b": "unknown"
            }
        },
        "model_details": {
            "model_description": "openai-gpt (a.k.a. \"GPT-1\") is the first transformer-based language model created and released by OpenAI. The model is a causal (unidirectional) transformer pre-trained using language modeling on a large corpus with long range dependencies.",
            "supported_labels": "English"
        },
        "limitations_and_biases": {
            "limitations": "The model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.",
            "biases": "Predictions generated by this model can include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups.",
            "risks": "Books and text readily available on the internet do not contain complete or even accurate information about the world. Recent work has shown that certain kinds of information are difficult to learn via just text and models often exploit biases in data distributions."
        },
        "recommendations": "Users (both direct and downstream) should be made aware of the risks, biases, and limitations of the model.",
        "compute_infrastructure": {
            "hardware": "8 P600 GPUs",
            "software": "Python, TensorFlow, PyTorch"
        },
        "contact_information": {
            "model_card_contact": "Hugging Face team"
        },
        "references": {
            "related_papers_and_resources": [
                "Improving language understanding by generative pre-training. Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya. (2018). OpenAI.",
                "BooksCorpus dataset (Zhu et al., 2015)"
            ]
        },
        "example_implementation": {
            "sample_code": "The provided Python examples for text generation, PyTorch, and TensorFlow usage."
        }
    },
    "Meta-Llama-3-8B": {
        "model_name": "Meta-Llama-3-8B",
        "developed_by": "Meta",
        "model_type": "Auto-regressive language model",
        "licensing": "A custom commercial license is available at: https://llama.meta.com/llama3/license",
        "installation": {
            "python_version": "N/A",
            "additional_libraries": "transformers, torch",
            "installation_command": "huggingface-cli download meta-llama/Meta-Llama-3-8B --include 'original/*' --local-dir Meta-Llama-3-8B"
        },
        "usage": {
            "cli_example": "huggingface-cli download meta-llama/Meta-Llama-3-8B --include 'original/*' --local-dir Meta-Llama-3-8B",
            "python_example": "import transformers\nimport torch\nmodel_id = 'meta-llama/Meta-Llama-3-8B'\npipeline = transformers.pipeline('text-generation', model=model_id, model_kwargs={'torch_dtype': torch.bfloat16}, device_map='auto')\npipeline('Hey how are you doing today?')"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": [
                "Meta-Llama-3-8B",
                "Meta-Llama-3-70B"
            ],
            "pretrained_datasets": "A new mix of publicly available online data.",
            "performance_metrics": [
                {
                    "model": "Llama 3 8B",
                    "size_pixels": "N/A",
                    "map_val50_95": "N/A",
                    "speed_cpu_onnx_ms": "N/A",
                    "speed_a100_tensorrt_ms": "N/A",
                    "params_m": "8B",
                    "flops_b": "N/A"
                },
                {
                    "model": "Llama 3 70B",
                    "size_pixels": "N/A",
                    "map_val50_95": "N/A",
                    "speed_cpu_onnx_ms": "N/A",
                    "speed_a100_tensorrt_ms": "N/A",
                    "params_m": "70B",
                    "flops_b": "N/A"
                }
            ]
        },
        "model_details": {
            "model_description": "Llama 3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.",
            "supported_labels": "N/A"
        },
        "limitations_and_biases": {
            "limitations": "Potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts.",
            "biases": "Pretraining and fine-tuning data do not include Meta user data.",
            "risks": "Residual risks will likely remain and developers should assess these risks in the context of their use case."
        },
        "recommendations": "Developers should incorporate safety tools and perform safety testing tailored to their specific applications of the model.",
        "compute_infrastructure": {
            "hardware": "Meta's Research SuperCluster, H100-80GB GPUs",
            "software": "Custom training libraries, third-party cloud compute for fine-tuning, annotation, and evaluation"
        },
        "contact_information": {
            "model_card_contact": "Instructions on how to provide feedback or comments on the model can be found in the model README."
        },
        "references": {
            "related_papers_and_resources": "N/A"
        },
        "example_implementation": {
            "sample_code": "import transformers\nimport torch\nmodel_id = 'meta-llama/Meta-Llama-3-8B'\npipeline = transformers.pipeline('text-generation', model=model_id, model_kwargs={'torch_dtype': torch.bfloat16}, device_map='auto')\npipeline('Hey how are you doing today?')"
        }
    },
    "Llama-2-7b": {
        "model_name": "Llama-2-7b",
        "developed_by": "Meta",
        "model_type": "Generative Text Model",
        "licensing": "Custom commercial license available at: https://ai.meta.com/resources/models-and-libraries/llama-downloads/",
        "installation": {
            "python_version": "3.8+",
            "additional_libraries": "transformers, torch",
            "installation_command": "pip install transformers torch"
        },
        "usage": {
            "cli_example": "python -m transformers.cli llm -m Llama-2-7b --input 'Your text here'",
            "python_example": "from transformers import AutoModelForCausalLM, AutoTokenizer\nmodel = AutoModelForCausalLM.from_pretrained('Llama-2-7b')\ntokenizer = AutoTokenizer.from_pretrained('Llama-2-7b')\ninputs = tokenizer('Your text here', return_tensors='pt')\noutputs = model.generate(**inputs)"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": [
                "Llama-2-7b",
                "Llama-2-13b",
                "Llama-2-70b"
            ],
            "pretrained_datasets": "A new mix of publicly available online data",
            "performance_metrics": {
                "model": "Llama-2-7b",
                "size_pixels": "N/A",
                "map_val50_95": "N/A",
                "speed_cpu_onnx_ms": "N/A",
                "speed_a100_tensorrt_ms": "N/A",
                "params_m": "7B",
                "flops_b": "N/A"
            }
        },
        "model_details": {
            "model_description": "Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B pretrained model. Links to other models can be found in the index at the bottom.",
            "supported_labels": "N/A"
        },
        "limitations_and_biases": {
            "limitations": "Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios.",
            "biases": "The model may in some instances produce inaccurate, biased or other objectionable responses to user prompts.",
            "risks": "Potential outputs cannot be predicted in advance. Developers should perform safety testing and tuning tailored to their specific applications of the model."
        },
        "recommendations": "Before deploying any applications of Llama 2, developers should perform safety testing and tuning tailored to their specific applications of the model.",
        "compute_infrastructure": {
            "hardware": "Meta's Research Super Cluster, A100-80GB GPUs",
            "software": "Custom training libraries"
        },
        "contact_information": {
            "model_card_contact": "https://github.com/facebookresearch/llama"
        },
        "references": {
            "related_papers_and_resources": "Llama-2: Open Foundation and Fine-tuned Chat Models"
        },
        "example_implementation": {
            "sample_code": "from transformers import AutoModelForCausalLM, AutoTokenizer\nmodel = AutoModelForCausalLM.from_pretrained('Llama-2-7b')\ntokenizer = AutoTokenizer.from_pretrained('Llama-2-7b')\ninputs = tokenizer('Your text here', return_tensors='pt')\noutputs = model.generate(**inputs)"
        }
    },
    "CodeLlama-70b-Instruct-hf": {
        "model_name": "CodeLlama-70b-Instruct-hf",
        "developed_by": "Meta",
        "model_type": "Generative Text Model",
        "licensing": "A custom commercial license is available at: https://ai.meta.com/resources/models-and-libraries/llama-downloads/",
        "installation": {
            "python_version": "Not specified",
            "additional_libraries": "transformers, accelerate",
            "installation_command": "pip install transformers accelerate"
        },
        "usage": {
            "cli_example": "Not provided",
            "python_example": "\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel_id = \"meta-llama/CodeLlama-70b-Instruct-hf\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n   model_id,\n   torch_dtype=torch.float16,\n   device_map=\"auto\",\n)\n\nchat = [\n   {\"role\":\"system\",\"content\":\"You are a helpful and honest code assistant expert in JavaScript. Please, provide all answers to programming questions in JavaScript\"},\n   {\"role\":\"user\",\"content\":\"Write a function that computes the set of sums of all contiguous sublists of a given list.\"},\n]\ninputs = tokenizer.apply_chat_template(chat, return_tensors=\"pt\").to(\"cuda\")\n\noutput = model.generate(input_ids=inputs, max_new_tokens=200)\noutput = output[0].to(\"cpu\")\nprint(tokenizer.decode(output))\n\nfrom transformers import AutoTokenizer\nimport transformers\nimport torch\n\nmodel_id = \"meta-llama/CodeLlama-70b-hf\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\npipeline = transformers.pipeline(\"text-generation\",\n   model=model_id,\n   torch_dtype=torch.float16,\n   device_map=\"auto\",\n)\n\nsequences = pipeline('def fibonacci(',\n   do_sample=True,\n   temperature=0.2,\n   top_p=0.9,\n   num_return_sequences=1,\n   eos_token_id=tokenizer.eos_token_id,\n   max_length=100,\n)\nfor seq in sequences:\n   print(f\"Result:{seq['generated_text']}\")"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": "Base Model: meta-llama/CodeLlama-7b-hf, meta-llama/CodeLlama-7b-Python-hf, meta-llama/CodeLlama-7b-Instruct-hf\n13B: meta-llama/CodeLlama-13b-hf, meta-llama/CodeLlama-13b-Python-hf, meta-llama/CodeLlama-13b-Instruct-hf\n34B: meta-llama/CodeLlama-34b-hf, meta-llama/CodeLlama-34b-Python-hf, meta-llama/CodeLlama-34b-Instruct-hf\n70B: meta-llama/CodeLlama-70b-hf, meta-llama/CodeLlama-70b-Python-hf, meta-llama/CodeLlama-70b-Instruct-hf",
            "pretrained_datasets": "Not specified",
            "performance_metrics": {
                "model": "Not specified",
                "size_pixels": "Not specified",
                "map_val50_95": "Not specified",
                "speed_cpu_onnx_ms": "Not specified",
                "speed_a100_tensorrt_ms": "Not specified",
                "params_m": "Not specified",
                "flops_b": "Not specified"
            }
        },
        "model_details": {
            "model_description": "Code Llama is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This model is designed for general code synthesis and understanding.",
            "supported_labels": "Not specified"
        },
        "limitations_and_biases": {
            "limitations": "Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in languages other than English. Use in any other way that is prohibited by the Acceptable Use Policy and Licensing Agreement for Code Llama and its variants.",
            "biases": "Not specified",
            "risks": "Code Llama\u2019s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate or objectionable responses to user prompts. Therefore, before deploying any applications of Code Llama, developers should perform safety testing and tuning tailored to their specific applications of the model."
        },
        "recommendations": "Not specified",
        "compute_infrastructure": {
            "hardware": "A100-80GB (TDP of 350-400W)",
            "software": "Custom training libraries"
        },
        "contact_information": {
            "model_card_contact": "Not specified"
        },
        "references": {
            "related_papers_and_resources": "Research Paper: \"Code Llama: Open Foundation Models for Code\" or its arXiv page."
        },
        "example_implementation": {
            "sample_code": "\nWe'll do our tests with the following made-up dialog:\nchat = [\n    {\"role\":\"system\",\"content\":\"System prompt    \"},\n    {\"role\":\"user\",\"content\":\"First user query\"},\n    {\"role\":\"assistant\",\"content\":\"Model response to first query\"},\n    {\"role\":\"user\",\"content\":\"Second user query\"},\n]\n\nFirst, let's see what the prompt looks like if we use the chat template:\ntokenizer.apply_chat_template(chat, tokenize=False)\n'<s>Source: system\\n\\n System prompt <step> Source: user\\n\\n First user query <step> Source: assistant\\n\\n Model response to first query <step> Source: user\\n\\n Second user query <step> Source: assistant\\nDestination: user\\n\\n '\n\nSo each turn of the conversation has a Source (system, user, or assistant), and then the content appears after two newlines and a space. Turns are separated with the special token <step>. After the last turn (which must necessarily come from the user), we invite the model to respond by using the special syntax Source: assistant\\nDestination: user\\n\\n.\n\nLet's see how we can build the same string ourselves:\noutput = \"<s>\"\nfor m in chat:\n    output += f\"Source:{m['role']}\\n\\n{m['content'].strip()}\"\noutput += \" <step> \"\noutput += \"Source: assistant\\nDestination: user\\n\\n \"\noutput\n'<s>Source: system\\n\\n System prompt <step> Source: user\\n\\n First user query <step> Source: assistant\\n\\n Model response to first query <step> Source: user\\n\\n Second user query <step> Source: assistant\\nDestination: user\\n\\n '\n\nTo verify that we got it right, we'll compare against the reference code in the original GitHub repo. We used the same dialog and tokenized it with the dialog_prompt_tokens function and got the following tokens:\nreference_tokens = [1,7562,29901,1788,13,13,2184,9508,32015,7562,29901,1404,13,13,3824,1404,2346,32015,7562,29901,20255,13,13,8125,2933,304,937,2346,32015,7562,29901,1404,13,13,6440,1404,2346,32015,7562,29901,20255,13,14994,3381,29901,1404,13,13,29871]\n\nLet's see what we get with the string we built using our Python loop. Note that we don't add \"special tokens\" because the string already starts with <s> (the beginning of sentence token):\ntokens = tokenizer.encode(output, add_special_tokens=False)\nassert reference_tokens == tokens\n\nSimilarly, let's verify that the chat template produces the same token sequence:\nassert reference_tokens == tokenizer.apply_chat_template(chat)\n\nAs a final detail, please note that if the dialog does not start with a system turn, the original code will insert one with an empty content string."
        }
    },
    "bert-base-uncased": {
        "model_name": "bert-base-uncased",
        "developed_by": "Hugging Face team based on original BERT model by Google",
        "model_type": "Transformers model pretrained on English language using a masked language modeling (MLM) objective.",
        "licensing": "Not specified",
        "installation": {
            "python_version": "3.6+",
            "additional_libraries": "transformers",
            "installation_command": "pip install transformers"
        },
        "usage": {
            "cli_example": "Not provided",
            "python_example": [
                "from transformers import pipeline",
                "unmasker = pipeline('fill-mask', model='bert-base-uncased')",
                "unmasker(\"Hello I'm a [MASK] model.\")",
                "from transformers import BertTokenizer, BertModel",
                "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')",
                "model = BertModel.from_pretrained('bert-base-uncased')",
                "text = \"Replace me by any text you'd like.\"",
                "encoded_input = tokenizer(text, return_tensors='pt')",
                "output = model(**encoded_input)",
                "from transformers import BertTokenizer, TFBertModel",
                "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')",
                "model = TFBertModel.from_pretrained('bert-base-uncased')",
                "text = \"Replace me by any text you'd like.\"",
                "encoded_input = tokenizer(text, return_tensors='tf')",
                "output = model(encoded_input)"
            ]
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": [
                "bert-base-uncased",
                "bert-large-uncased",
                "bert-base-cased",
                "bert-large-cased",
                "bert-base-chinese",
                "bert-base-multilingual-cased",
                "bert-large-uncased-whole-word-masking",
                "bert-large-cased-whole-word-masking"
            ],
            "pretrained_datasets": [
                "BookCorpus",
                "English Wikipedia"
            ],
            "performance_metrics": {
                "Glue test results": {
                    "MNLI-(m/mm)": "84.6/83.4",
                    "QQP": "71.2",
                    "QNLI": "90.5",
                    "SST-2": "93.5",
                    "CoLA": "52.1",
                    "STS-B": "85.8",
                    "MRPC": "88.9",
                    "RTE": "66.4",
                    "Average": "79.6"
                }
            }
        },
        "model_details": {
            "model_description": "BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labeling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts.",
            "supported_labels": "Not specified"
        },
        "limitations_and_biases": {
            "limitations": "The model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering. For tasks such as text generation you should look at model like GPT2.",
            "biases": "Even if the training data used for this model could be characterized as fairly neutral, this model can have biased predictions.",
            "risks": "This bias will also affect all fine-tuned versions of this model."
        },
        "recommendations": "You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task.",
        "compute_infrastructure": {
            "hardware": "4 cloud TPUs in Pod configuration (16 TPU chips total)",
            "software": "Adam optimizer with a learning rate of 1e-4, \u03b21=0.9, \u03b22=0.999, a weight decay of 0.01, learning rate warmup for 10,000 steps and linear decay of the learning rate after."
        },
        "contact_information": {
            "model_card_contact": "Hugging Face team"
        },
        "references": {
            "related_papers_and_resources": [
                {
                    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                    "authors": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova",
                    "journal": "CoRR",
                    "volume": "abs/1810.04805",
                    "year": "2018",
                    "url": "http://arxiv.org/abs/1810.04805",
                    "bibtex": "@article{DBLP:journals/corr/abs-1810-04805, author = {Jacob Devlin and Ming{-}Wei Chang and Kenton Lee and Kristina Toutanova}, title = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language Understanding}, journal = {CoRR}, volume = {abs/1810.04805}, year = {2018}, url = {http://arxiv.org/abs/1810.04805}, archivePrefix = {arXiv}, eprint = {1810.04805}, timestamp = {Tue, 30 Oct 2018 20:39:56 +0100}, biburl = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib}, bibsource = {dblp computer science bibliography, https://dblp.org}"
                }
            ]
        },
        "example_implementation": {
            "sample_code": [
                ">>> from transformers import pipeline",
                ">>> unmasker = pipeline('fill-mask', model='bert-base-uncased')",
                ">>> unmasker(\"Hello I'm a [MASK] model.\")",
                "# Output:",
                "[{'sequence':\"[CLS] hello i'm a fashion model. [SEP]\",'score':0.1073106899857521,'token':4827,'token_str':'fashion'},",
                " {'sequence':\"[CLS] hello i'm a role model. [SEP]\",'score':0.08774490654468536,'token':2535,'token_str':'role'},",
                " {'sequence':\"[CLS] hello i'm a new model. [SEP]\",'score':0.05338378623127937,'token':2047,'token_str':'new'},",
                " {'sequence':\"[CLS] hello i'm a super model. [SEP]\",'score':0.04667217284440994,'token':3565,'token_str':'super'},",
                " {'sequence':\"[CLS] hello i'm a fine model. [SEP]\",'score':0.027095865458250046,'token':2986,'token_str':'fine'}]"
            ]
        }
    },
    "roberta-base": {
        "model_name": "roberta-base",
        "developed_by": "Hugging Face (originally introduced by a team in the paper 'RoBERTa: A Robustly Optimized BERT Pretraining Approach')",
        "model_type": "Masked Language Model (MLM)",
        "licensing": "N/A",
        "installation": {
            "python_version": "N/A",
            "additional_libraries": "transformers",
            "installation_command": "pip install transformers"
        },
        "usage": {
            "cli_example": "N/A",
            "python_example": "```python\nfrom transformers import pipeline\nunmasker = pipeline('fill-mask', model='roberta-base')\nunmasker(\"Hello I'm a <mask> model.\")\n```"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": "roberta-base",
            "pretrained_datasets": "BookCorpus, English Wikipedia, CC-News, OpenWebText, Stories",
            "performance_metrics": {
                "model": "N/A",
                "size_pixels": "N/A",
                "map_val50_95": "N/A",
                "speed_cpu_onnx_ms": "N/A",
                "speed_a100_tensorrt_ms": "N/A",
                "params_m": "N/A",
                "flops_b": "N/A"
            }
        },
        "model_details": {
            "model_description": "RoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. It was pretrained on the raw texts only, with no humans labeling them in any way. Specifically, it was pretrained with the Masked Language Modeling (MLM) objective, where 15% of the words in the input are masked, and the model has to predict the masked words.",
            "supported_labels": "N/A"
        },
        "limitations_and_biases": {
            "limitations": "The model can have biased predictions due to the unfiltered content from the internet used for training.",
            "biases": "The training data contains biases from the internet, which affects the model's predictions.",
            "risks": "The model's bias affects all fine-tuned versions of the model."
        },
        "recommendations": "This model is primarily aimed at being fine-tuned on tasks that use the whole sentence to make decisions, such as sequence classification, token classification, or question answering.",
        "compute_infrastructure": {
            "hardware": "1024 V100 GPUs",
            "software": "N/A"
        },
        "contact_information": {
            "model_card_contact": "Hugging Face team"
        },
        "references": {
            "related_papers_and_resources": "RoBERTa: A Robustly Optimized BERT Pretraining Approach (http://arxiv.org/abs/1907.11692)"
        },
        "example_implementation": {
            "sample_code": "```python\nfrom transformers import RobertaTokenizer, RobertaModel\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nmodel = RobertaModel.from_pretrained('roberta-base')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n\n# For TensorFlow\nfrom transformers import RobertaTokenizer, TFRobertaModel\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nmodel = TFRobertaModel.from_pretrained('roberta-base')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```"
        }
    },
    "t5-base": {
        "model_name": "t5-base",
        "developed_by": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu",
        "model_type": "Language model",
        "licensing": "Apache 2.0",
        "installation": {
            "python_version": "3.6+",
            "additional_libraries": "transformers",
            "installation_command": "pip install transformers"
        },
        "usage": {
            "cli_example": "Not provided",
            "python_example": "from transformers import T5Tokenizer, T5Model\n\ntokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\nmodel = T5Model.from_pretrained(\"t5-base\")\n\ninput_ids = tokenizer(\"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\").input_ids  # Batch size 1\ndecoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"pt\").input_ids  # Batch size 1\n# forward pass\noutputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\nlast_hidden_states = outputs.last_hidden_state"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": "All T5 Checkpoints",
            "pretrained_datasets": "C4, Wiki-DPR, CoLA, SST-2, MRPC, STS-B, QQP, MNLI, QNLI, RTE, CB, COPA, WIC, MultiRC, ReCoRD, BoolQ",
            "performance_metrics": {
                "model": "t5-base",
                "size_pixels": "Not provided",
                "map_val50_95": "Not provided",
                "speed_cpu_onnx_ms": "Not provided",
                "speed_a100_tensorrt_ms": "Not provided",
                "params_m": "220",
                "flops_b": "Not provided"
            }
        },
        "model_details": {
            "model_description": "T5-Base is the checkpoint with 220 million parameters. The T5 framework proposes reframing all NLP tasks into a unified text-to-text format where the input and output are always text strings.",
            "supported_labels": "English, French, Romanian, German"
        },
        "limitations_and_biases": {
            "limitations": "More information needed.",
            "biases": "More information needed.",
            "risks": "More information needed."
        },
        "recommendations": "More information needed.",
        "compute_infrastructure": {
            "hardware": "Google Cloud TPU Pods",
            "software": "Google Cloud Platform"
        },
        "contact_information": {
            "model_card_contact": "This model card was written by the team at Hugging Face."
        },
        "references": {
            "related_papers_and_resources": "Research paper, Google's T5 Blog Post, GitHub Repo, Hugging Face T5 Docs"
        },
        "example_implementation": {
            "sample_code": "from transformers import T5Tokenizer, T5Model\n\ntokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\nmodel = T5Model.from_pretrained(\"t5-base\")\n\ninput_ids = tokenizer(\"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\").input_ids  # Batch size 1\ndecoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"pt\").input_ids  # Batch size 1\n# forward pass\noutputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\nlast_hidden_states = outputs.last_hidden_state"
        }
    },
    "bloom": {
        "model_name": "BLOOM",
        "developed_by": "BigScience",
        "model_type": "Transformer-based Language Model",
        "licensing": "RAIL License v1.0",
        "installation": {
            "python_version": "pytorch-1.11 w/ CUDA-11.5",
            "additional_libraries": "transformers, accelerate, Megatron-DeepSpeed, apex",
            "installation_command": "pip install transformers accelerate"
        },
        "usage": {
            "cli_example": "Use HuggingFace's ecosystem to deploy the model.",
            "python_example": "from transformers import BloomModel\nmodel = BloomModel.from_pretrained('bigscience/bloom')"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": "BLOOM-176B",
            "pretrained_datasets": "46 natural languages, 13 programming languages",
            "performance_metrics": {
                "model": "BLOOM-176B",
                "size_pixels": "N/A",
                "map_val50_95": "N/A",
                "speed_cpu_onnx_ms": "N/A",
                "speed_a100_tensorrt_ms": "N/A",
                "params_m": "176,247,271,424",
                "flops_b": "N/A"
            }
        },
        "model_details": {
            "model_description": "BLOOM is an autoregressive Large Language Model (LLM), trained to continue text from a prompt on vast amounts of text data using industrial-scale computational resources. It can output coherent text in 46 languages and 13 programming languages that is almost indistinguishable from text written by humans.",
            "supported_labels": "Text generation, Information Extraction, Question Answering, Summarization"
        },
        "limitations_and_biases": {
            "limitations": "Model outputs content that appears factual but may not be correct. Not designed for high-stakes settings or critical decisions.",
            "biases": "Model may overrepresent some viewpoints and underrepresent others. It may contain stereotypes and personal information.",
            "risks": "Generating hateful, abusive, or violent language; discriminatory or prejudicial language; generating incorrect information as factual."
        },
        "recommendations": "Indirect users should be made aware when the content they're working with is created by the LLM. Users should provide mechanisms for those affected to provide feedback.",
        "compute_infrastructure": {
            "hardware": "Jean Zay Public Supercomputer: 384 A100 80GB GPUs (48 nodes), additional 32 A100 80GB GPUs (4 nodes) in reserve, 8 GPUs per node using NVLink 4 inter-gpu connects, 4 OmniPath links, CPU: AMD, CPU memory: 512GB per node, GPU memory: 640GB per node, Inter-node connect: Omni-Path Architecture (OPA), NCCL-communications network: a fully dedicated subnet, Disc IO network: shared network with other types of nodes",
            "software": "Megatron-DeepSpeed, DeepSpeed, PyTorch, apex"
        },
        "contact_information": {
            "model_card_contact": "bigscience-contact@googlegroups.com"
        },
        "references": {
            "related_papers_and_resources": "https://huggingface.co/bigscience/tr11-176B-ml-logs, https://bigscience.huggingface.co/blog/building-a-tb-scale-multilingual-dataset-for-language-modeling, https://bigscience.huggingface.co/blog/what-language-model-to-train-if-you-have-two-million-gpu-hours, https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml"
        },
        "example_implementation": {
            "sample_code": "import torch\nfrom transformers import BloomTokenizerFast, BloomForCausalLM\n\ntokenizer = BloomTokenizerFast.from_pretrained('bigscience/bloom')\nmodel = BloomForCausalLM.from_pretrained('bigscience/bloom')\n\ninputs = tokenizer('Hello, my name is', return_tensors='pt')\noutputs = model.generate(inputs['input_ids'], max_length=50)\nprint(tokenizer.decode(outputs[0]))"
        }
    },
    "xlnet-base-cased": {
        "model_name": "xlnet-base-cased",
        "developed_by": "Yang et al.",
        "model_type": "Unsupervised language representation learning",
        "licensing": "The team releasing XLNet did not provide specific licensing information; this model card was written by Hugging Face.",
        "installation": {
            "python_version": "3.x",
            "additional_libraries": "transformers",
            "installation_command": "pip install transformers"
        },
        "usage": {
            "cli_example": null,
            "python_example": "from transformers import XLNetTokenizer, XLNetModel\n\ntokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\nmodel = XLNetModel.from_pretrained('xlnet-base-cased')\n\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\noutputs = model(**inputs)\n\nlast_hidden_states = outputs.last_hidden_state"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": "xlnet-base-cased",
            "pretrained_datasets": null,
            "performance_metrics": {
                "model": "XLNet",
                "size_pixels": null,
                "map_val50_95": null,
                "speed_cpu_onnx_ms": null,
                "speed_a100_tensorrt_ms": null,
                "params_m": null,
                "flops_b": null
            }
        },
        "model_details": {
            "model_description": "XLNet is a new unsupervised language representation learning method based on a novel generalized permutation language modeling objective. Additionally, XLNet employs Transformer-XL as the backbone model, exhibiting excellent performance for language tasks involving long context. Overall, XLNet achieves state-of-the-art (SOTA) results on various downstream language tasks including question answering, natural language inference, sentiment analysis, and document ranking.",
            "supported_labels": null
        },
        "limitations_and_biases": {
            "limitations": "The model is mostly intended to be fine-tuned on a downstream task. Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering. For tasks such as text generation, you should look at models like GPT2.",
            "biases": null,
            "risks": null
        },
        "recommendations": "See the model hub to look for fine-tuned versions on a task that interests you.",
        "compute_infrastructure": {
            "hardware": null,
            "software": null
        },
        "contact_information": {
            "model_card_contact": "Hugging Face team"
        },
        "references": {
            "related_papers_and_resources": "@article{DBLP:journals/corr/abs-1906-08237,\n  author    = {Zhilin Yang and\n               Zihang Dai and\n               Yiming Yang and\n               Jaime G. Carbonell and\n               Ruslan Salakhutdinov and\n               Quoc V. Le},\n  title     = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},\n  journal   = {CoRR},\n  volume    = {abs/1906.08237},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1906.08237},\n  eprinttype = {arXiv},\n  eprint    = {1906.08237},\n  timestamp = {Mon, 24 Jun 2019 17:28:45 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1906-08237.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}"
        },
        "example_implementation": {
            "sample_code": "from transformers import XLNetTokenizer, XLNetModel\n\ntokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\nmodel = XLNetModel.from_pretrained('xlnet-base-cased')\n\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\noutputs = model(**inputs)\n\nlast_hidden_states = outputs.last_hidden_state"
        }
    },
    "Mixtral-8x7B-v0.1": {
        "model_name": "Mixtral-8x7B-v0.1",
        "developed_by": "The Mistral AI Team",
        "model_type": "Large Language Model (LLM)",
        "licensing": "Not specified in the original model card",
        "installation": {
            "python_version": "Not specified in the original model card",
            "additional_libraries": "transformers, torch, bitsandbytes",
            "installation_command": "Not specified in the original model card"
        },
        "usage": {
            "cli_example": "Not specified in the original model card",
            "python_example": [
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"mistralai/Mixtral-8x7B-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\n\ntext = \"Hello my name is\"\ninputs = tokenizer(text, return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))",
                "import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"mistralai/Mixtral-8x7B-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16).to(0)\ntext = \"Hello my name is\"\ninputs = tokenizer(text, return_tensors=\"pt\").to(0)\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))",
                "import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"mistralai/Mixtral-8x7B-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, load_in_4bit=True)\ntext = \"Hello my name is\"\ninputs = tokenizer(text, return_tensors=\"pt\").to(0)\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))",
                "import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"mistralai/Mixtral-8x7B-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, use_flash_attention_2=True)\ntext = \"Hello my name is\"\ninputs = tokenizer(text, return_tensors=\"pt\").to(0)\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))"
            ]
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": "Mixtral-8x7B-v0.1",
            "pretrained_datasets": "Not specified in the original model card",
            "performance_metrics": {
                "model": "Mixtral-8x7B-v0.1",
                "size_pixels": "N/A",
                "map_val50_95": "N/A",
                "speed_cpu_onnx_ms": "N/A",
                "speed_a100_tensorrt_ms": "N/A",
                "params_m": "N/A",
                "flops_b": "N/A"
            }
        },
        "model_details": {
            "model_description": "The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts. The Mistral-8x7B outperforms Llama 2 70B on most benchmarks we tested. For full details of this model please read our release blog post.",
            "supported_labels": "Not specified in the original model card"
        },
        "limitations_and_biases": {
            "limitations": "Mixtral-8x7B is a pretrained base model and therefore does not have any moderation mechanisms.",
            "biases": "Not specified in the original model card",
            "risks": "Not specified in the original model card"
        },
        "recommendations": "Not specified in the original model card",
        "compute_infrastructure": {
            "hardware": "GPU for half-precision (float16) and other optimizations",
            "software": "transformers, torch, bitsandbytes"
        },
        "contact_information": {
            "model_card_contact": "Not specified in the original model card"
        },
        "references": {
            "related_papers_and_resources": "For full details of this model please read our release blog post."
        },
        "example_implementation": {
            "sample_code": [
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"mistralai/Mixtral-8x7B-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\n\ntext = \"Hello my name is\"\ninputs = tokenizer(text, return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))",
                "import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"mistralai/Mixtral-8x7B-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16).to(0)\ntext = \"Hello my name is\"\ninputs = tokenizer(text, return_tensors=\"pt\").to(0)\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))",
                "import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"mistralai/Mixtral-8x7B-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, load_in_4bit=True)\ntext = \"Hello my name is\"\ninputs = tokenizer(text, return_tensors=\"pt\").to(0)\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))",
                "import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"mistralai/Mixtral-8x7B-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, use_flash_attention_2=True)\ntext = \"Hello my name is\"\ninputs = tokenizer(text, return_tensors=\"pt\").to(0)\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))"
            ]
        }
    },
    "distilbert-base-uncased": {
        "model_name": "distilbert-base-uncased",
        "developed_by": "Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf",
        "model_type": "Distilled Transformer",
        "licensing": "N/A",
        "installation": {
            "python_version": ">=3.6",
            "additional_libraries": "transformers, torch, tensorflow",
            "installation_command": "pip install transformers torch tensorflow"
        },
        "usage": {
            "cli_example": "N/A",
            "python_example": {
                "masked_language_modeling": [
                    "from transformers import pipeline",
                    "unmasker = pipeline('fill-mask', model='distilbert-base-uncased')",
                    "unmasker(\"Hello I'm a [MASK] model.\")"
                ],
                "feature_extraction_pytorch": [
                    "from transformers import DistilBertTokenizer, DistilBertModel",
                    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')",
                    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')",
                    "text = \"Replace me by any text you'd like.\"",
                    "encoded_input = tokenizer(text, return_tensors='pt')",
                    "output = model(**encoded_input)"
                ],
                "feature_extraction_tensorflow": [
                    "from transformers import DistilBertTokenizer, TFDistilBertModel",
                    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')",
                    "model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')",
                    "text = \"Replace me by any text you'd like.\"",
                    "encoded_input = tokenizer(text, return_tensors='tf')",
                    "output = model(encoded_input)"
                ]
            }
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": "distilbert-base-uncased",
            "pretrained_datasets": "BookCorpus, English Wikipedia",
            "performance_metrics": {
                "model": "distilbert-base-uncased",
                "size_pixels": "N/A",
                "map_val50_95": "N/A",
                "speed_cpu_onnx_ms": "N/A",
                "speed_a100_tensorrt_ms": "N/A",
                "params_m": "N/A",
                "flops_b": "N/A"
            }
        },
        "model_details": {
            "model_description": "DistilBERT is a transformers model, smaller and faster than BERT, which was pretrained on the same corpus in a self-supervised fashion, using the BERT base model as a teacher. This means it was pretrained on the raw texts only, with no humans labelling them in any way with an automatic process to generate inputs and labels from those texts using the BERT base model. More precisely, it was pretrained with three objectives: Distillation loss, Masked language modeling (MLM), and Cosine embedding loss.",
            "supported_labels": "N/A"
        },
        "limitations_and_biases": {
            "limitations": "This model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification, or question answering.",
            "biases": "Even if the training data used for this model could be characterized as fairly neutral, this model can have biased predictions. It also inherits some of the bias of its teacher model.",
            "risks": "This bias will also affect all fine-tuned versions of this model."
        },
        "recommendations": "For tasks such as text generation, you should look at model like GPT2.",
        "compute_infrastructure": {
            "hardware": "8 16 GB V100 GPUs",
            "software": "N/A"
        },
        "contact_information": {
            "model_card_contact": "N/A"
        },
        "references": {
            "related_papers_and_resources": {
                "paper": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
                "citation": "@article{Sanh2019DistilBERTAD, title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter}, author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf}, journal={ArXiv}, year={2019}, volume={abs/1910.01108}}",
                "code": "https://github.com/huggingface/transformers"
            }
        },
        "example_implementation": {
            "sample_code": {
                "masked_language_modeling": [
                    "from transformers import pipeline",
                    "unmasker = pipeline('fill-mask', model='distilbert-base-uncased')",
                    "unmasker(\"Hello I'm a [MASK] model.\")"
                ],
                "feature_extraction_pytorch": [
                    "from transformers import DistilBertTokenizer, DistilBertModel",
                    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')",
                    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')",
                    "text = \"Replace me by any text you'd like.\"",
                    "encoded_input = tokenizer(text, return_tensors='pt')",
                    "output = model(**encoded_input)"
                ],
                "feature_extraction_tensorflow": [
                    "from transformers import DistilBertTokenizer, TFDistilBertModel",
                    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')",
                    "model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')",
                    "text = \"Replace me by any text you'd like.\"",
                    "encoded_input = tokenizer(text, return_tensors='tf')",
                    "output = model(encoded_input)"
                ]
            }
        }
    },
    "gemma-2-9b": {
        "model_name": "gemma-2-9b",
        "developed_by": "Google",
        "model_type": "Text-to-Text, Decoder-only Large Language Model",
        "licensing": "Terms of Use: Terms",
        "installation": {
            "python_version": "Not specified",
            "additional_libraries": "transformers, accelerate, bitsandbytes, flash-attn",
            "installation_command": "pip install -U transformers accelerate bitsandbytes flash-attn"
        },
        "usage": {
            "cli_example": "Not provided",
            "python_example": {
                "single_gpu": [
                    "# pip install accelerate",
                    "from transformers import AutoTokenizer, AutoModelForCausalLM",
                    "import torch",
                    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b\")",
                    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-9b\", device_map=\"auto\", torch_dtype=torch.bfloat16)",
                    "input_text =\"Write me a poem about Machine Learning.\"",
                    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")",
                    "outputs = model.generate(**input_ids)",
                    "print(tokenizer.decode(outputs[0]))"
                ],
                "float32_gpu": [
                    "# pip install accelerate",
                    "from transformers import AutoTokenizer, AutoModelForCausalLM",
                    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b\")",
                    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-9b\", device_map=\"auto\")",
                    "input_text =\"Write me a poem about Machine Learning.\"",
                    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")",
                    "outputs = model.generate(**input_ids)",
                    "print(tokenizer.decode(outputs[0]))"
                ],
                "quantized_8bit": [
                    "# pip install bitsandbytes accelerate",
                    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig",
                    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)",
                    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b\")",
                    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-9b\", quantization_config=quantization_config)",
                    "input_text =\"Write me a poem about Machine Learning.\"",
                    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")",
                    "outputs = model.generate(**input_ids)",
                    "print(tokenizer.decode(outputs[0]))"
                ],
                "quantized_4bit": [
                    "# pip install bitsandbytes accelerate",
                    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig",
                    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)",
                    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b\")",
                    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-9b\", quantization_config=quantization_config)",
                    "input_text =\"Write me a poem about Machine Learning.\"",
                    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")",
                    "outputs = model.generate(**input_ids)",
                    "print(tokenizer.decode(outputs[0]))"
                ],
                "flash_attention_2": [
                    "# pip install flash-attn",
                    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-9b\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").to(0)"
                ]
            }
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": [
                "gemma-2-9b",
                "gemma-2-27b"
            ],
            "pretrained_datasets": [
                "Web Documents",
                "Code",
                "Mathematics"
            ],
            "performance_metrics": {
                "MMLU 5-shot, top-1": {
                    "gemma-2-9b": "71.3",
                    "gemma-2-27b": "75.2"
                },
                "HellaSwag 10-shot": {
                    "gemma-2-9b": "81.9",
                    "gemma-2-27b": "86.4"
                },
                "PIQA 0-shot": {
                    "gemma-2-9b": "81.7",
                    "gemma-2-27b": "83.2"
                },
                "SocialIQA 0-shot": {
                    "gemma-2-9b": "53.4",
                    "gemma-2-27b": "53.7"
                },
                "BoolQ 0-shot": {
                    "gemma-2-9b": "84.2",
                    "gemma-2-27b": "84.8"
                },
                "WinoGrande partial score": {
                    "gemma-2-9b": "80.6",
                    "gemma-2-27b": "83.7"
                },
                "ARC-e 0-shot": {
                    "gemma-2-9b": "88.0",
                    "gemma-2-27b": "88.6"
                },
                "ARC-c 25-shot": {
                    "gemma-2-9b": "68.4",
                    "gemma-2-27b": "71.4"
                },
                "TriviaQA 5-shot": {
                    "gemma-2-9b": "76.6",
                    "gemma-2-27b": "83.7"
                },
                "Natural Questions 5-shot": {
                    "gemma-2-9b": "29.2",
                    "gemma-2-27b": "34.5"
                },
                "HumanEval pass@1": {
                    "gemma-2-9b": "40.2",
                    "gemma-2-27b": "51.8"
                },
                "MBPP 3-shot": {
                    "gemma-2-9b": "52.4",
                    "gemma-2-27b": "62.6"
                },
                "GSM8K 5-shot, maj@1": {
                    "gemma-2-9b": "68.6",
                    "gemma-2-27b": "74.0"
                },
                "MATH 4-shot": {
                    "gemma-2-9b": "36.6",
                    "gemma-2-27b": "42.3"
                },
                "AGIEval 3-5-shot": {
                    "gemma-2-9b": "52.8",
                    "gemma-2-27b": "55.1"
                },
                "BIG-Bench 3-shot, CoT": {
                    "gemma-2-9b": "68.2",
                    "gemma-2-27b": "74.9"
                }
            }
        },
        "model_details": {
            "model_description": "Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. They are text-to-text, decoder-only large language models, available in English, with open weights for both pre-trained variants and instruction-tuned variants. Gemma models are well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as a laptop, desktop or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone.",
            "supported_labels": "Not specified"
        },
        "limitations_and_biases": {
            "limitations": [
                "Training Data: Biases or gaps in the training data can lead to limitations in the model's responses.",
                "Context and Task Complexity: Open-ended or highly complex tasks might be challenging.",
                "Language Ambiguity and Nuance: LLMs might struggle to grasp subtle nuances, sarcasm, or figurative language.",
                "Factual Accuracy: LLMs may generate incorrect or outdated factual statements.",
                "Common Sense: LLMs might lack the ability to apply common sense reasoning in certain situations."
            ],
            "biases": [
                "Bias and Fairness: LLMs trained on large-scale, real-world text data can reflect socio-cultural biases embedded in the training material."
            ],
            "risks": [
                "Misinformation and Misuse: LLMs can be misused to generate text that is false, misleading, or harmful.",
                "Generation of harmful content: Mechanisms and guidelines for content safety are essential.",
                "Misuse for malicious purposes: Technical limitations and developer and end-user education can help mitigate against malicious applications of LLMs.",
                "Privacy violations: Models were trained on data filtered for removal of PII (Personally Identifiable Information)."
            ]
        },
        "recommendations": "Developers are encouraged to exercise caution and implement appropriate content safety safeguards based on their specific product policies and application use cases. Continuous monitoring using evaluation metrics and human review is recommended to mitigate biases and harmful content generation.",
        "compute_infrastructure": {
            "hardware": "Tensor Processing Unit (TPU) hardware (TPUv5p)",
            "software": "JAX and ML Pathways"
        },
        "contact_information": {
            "model_card_contact": "Not provided"
        },
        "references": {
            "related_papers_and_resources": [
                "Gemma 2 model card",
                "Responsible Generative AI Toolkit",
                "Gemma on Kaggle",
                "Gemma on Vertex Model Garden",
                "Paper about the Gemini family of models"
            ]
        },
        "example_implementation": {
            "sample_code": [
                "# pip install accelerate",
                "from transformers import AutoTokenizer, AutoModelForCausalLM",
                "import torch",
                "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b\")",
                "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-9b\", device_map=\"auto\", torch_dtype=torch.bfloat16)",
                "input_text =\"Write me a poem about Machine Learning.\"",
                "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")",
                "outputs = model.generate(**input_ids)",
                "print(tokenizer.decode(outputs[0]))"
            ]
        }
    },
    "gemma-2-27b-it": {
        "model_name": "gemma-2-27b-it",
        "developed_by": "Google",
        "model_type": "Text-to-Text, Decoder-Only Large Language Model",
        "licensing": "Terms of Use: Terms",
        "installation": {
            "python_version": "Python 3.x",
            "additional_libraries": "transformers, torch, accelerate, bitsandbytes, flash-attn",
            "installation_command": "pip install -U transformers torch accelerate bitsandbytes flash-attn"
        },
        "usage": {
            "cli_example": "Not provided",
            "python_example": "from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-27b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-27b-it\",\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16\n)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": "gemma-2-27b-it, gemma-2-9b-it",
            "pretrained_datasets": "13 trillion tokens for 27B model, 8 trillion tokens for 9B model",
            "performance_metrics": {
                "model": "Gemma PT 9B, Gemma PT 27B",
                "size_pixels": "Not applicable",
                "map_val50_95": "Not applicable",
                "speed_cpu_onnx_ms": "Not provided",
                "speed_a100_tensorrt_ms": "Not provided",
                "params_m": "Not provided",
                "flops_b": "Not provided"
            }
        },
        "model_details": {
            "model_description": "Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. They are text-to-text, decoder-only large language models, available in English, with open weights for both pre-trained variants and instruction-tuned variants. Gemma models are well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning.",
            "supported_labels": "Not provided"
        },
        "limitations_and_biases": {
            "limitations": "Training Data: The quality and diversity of the training data significantly influence the model's capabilities. Context and Task Complexity: LLMs are better at tasks that can be framed with clear prompts and instructions. Language Ambiguity and Nuance: LLMs might struggle to grasp subtle nuances, sarcasm, or figurative language. Factual Accuracy: LLMs generate responses based on information they learned from their training datasets, but they are not knowledge bases. Common Sense: LLMs might lack the ability to apply common sense reasoning in certain situations.",
            "biases": "Bias and Fairness: LLMs trained on large-scale, real-world text data can reflect socio-cultural biases embedded in the training material.",
            "risks": "Misinformation and Misuse: LLMs can be misused to generate text that is false, misleading, or harmful. Privacy violations: Models were trained on data filtered for removal of PII (Personally Identifiable Information)."
        },
        "recommendations": "Developers are encouraged to adhere to privacy regulations with privacy-preserving techniques, perform continuous monitoring using evaluation metrics, human review, and explore de-biasing techniques during model training, fine-tuning, and other use cases.",
        "compute_infrastructure": {
            "hardware": "Tensor Processing Unit (TPU) hardware (TPUv5p)",
            "software": "JAX and ML Pathways"
        },
        "contact_information": {
            "model_card_contact": "Not provided"
        },
        "references": {
            "related_papers_and_resources": "Gemma Team. Gemma. Kaggle, 2024. URL: https://www.kaggle.com/m/3301, DOI: 10.34740/KAGGLE/M/3301"
        },
        "example_implementation": {
            "sample_code": "from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-27b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-27b-it\",\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16\n)\n\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))"
        }
    },
    "codegen-350M-multi": {
        "model_name": "codegen-350M-multi",
        "developed_by": "Salesforce",
        "model_type": "autoregressive language model",
        "licensing": "Original release in a repository",
        "installation": {
            "python_version": ">=3.6",
            "additional_libraries": "transformers",
            "installation_command": "pip install transformers"
        },
        "usage": {
            "cli_example": "N/A",
            "python_example": "from transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-350M-multi\")\nmodel = AutoModelForCausalLM.from_pretrained(\"Salesforce/codegen-350M-multi\")\n\ntext = \"def hello_world():\"\ninput_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n\ngenerated_ids = model.generate(input_ids, max_length=128)\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": "350M, 2B, 6B, 16B",
            "pretrained_datasets": "BigQuery, a large-scale dataset of multiple programming languages from GitHub repositories",
            "performance_metrics": {
                "model": "CodeGen-Multi 350M",
                "size_pixels": "N/A",
                "map_val50_95": "N/A",
                "speed_cpu_onnx_ms": "N/A",
                "speed_a100_tensorrt_ms": "N/A",
                "params_m": "350M",
                "flops_b": "N/A"
            }
        },
        "model_details": {
            "model_description": "CodeGen is a family of autoregressive language models for program synthesis. The model is initialized with CodeGen-NL 350M and further pre-trained on a dataset of multiple programming languages.",
            "supported_labels": "HumanEval, MTPB"
        },
        "limitations_and_biases": {
            "limitations": "The model is intended for and best at program synthesis, generating executable code given English prompts in the form of a comment string.",
            "biases": "N/A",
            "risks": "N/A"
        },
        "recommendations": "N/A",
        "compute_infrastructure": {
            "hardware": "multiple TPU-v4-512 by Google",
            "software": "N/A"
        },
        "contact_information": {
            "model_card_contact": "N/A"
        },
        "references": {
            "related_papers_and_resources": "A Conversational Paradigm for Program Synthesis by Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong"
        },
        "example_implementation": {
            "sample_code": "from transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-350M-multi\")\nmodel = AutoModelForCausalLM.from_pretrained(\"Salesforce/codegen-350M-multi\")\n\ntext = \"def hello_world():\"\ninput_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n\ngenerated_ids = model.generate(input_ids, max_length=128)\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))"
        }
    }
}