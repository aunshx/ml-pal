{
    "gpt2": {
        "model_name": "gpt2",
        "model_architecture": "Transformers model pretrained on a very large corpus of English data",
        "training_objective": "Causal language modeling (CLM)",
        "parameters": "124M",
        "primary_use_case": "Text generation",
        "performance_metrics": "Dataset: LAMBADA, LAMBADA, CBT-CN, CBT-NE, WikiText2, PTB, enwiki8, text8, WikiText103, 1BW, (metric): (PPL), (ACC), (ACC), (ACC), (PPL), (PPL), (BPB), (BPC), (PPL), (PPL), 35.13, 45.99, 87.65, 83.42, 9.41, 65.85, 1.16, 1,173, 7.50, 75.20",
        "training_data": "Web pages from outbound links on Reddit which received at least 3 karma. All Wikipedia pages were removed from this dataset.",
        "inference_speed": "Not specified",
        "memory_compute_requirements": "Not specified",
        "fine_tuning_capability": "Yes",
        "bias_fairness": "Contains biases inherent to the systems they were trained on, the team releasing GPT-2 recommends a study of biases relevant to the intended use-case before deployment into systems that interact with humans.",
        "model_size": "Smallest version of GPT-2",
        "licensing": "Not specified",
        "advantages": "Best at generating texts from a prompt, can be used to extract features useful for downstream tasks"
    },
    "openai-gpt": {
        "model_name": "openai-gpt",
        "model_architecture": "Transformer-based language model",
        "training_objective": "Causal (unidirectional) transformer pre-trained using language modeling on a large corpus with long range dependencies.",
        "parameters": "12-layer decoder-only transformer with masked self-attention heads (768 dimensional states and 12 attention heads).",
        "primary_use_case": "Language modeling tasks. Potential downstream uses include tasks that leverage language models such as natural language inference (NLI), question answering, semantic similarity, and text classification.",
        "performance_metrics": "Accuracy on Textual Entailment, Semantic Similarity, Reading Comprehension, Commonsense Reasoning, Sentiment Analysis, Linguistic Acceptability and Multi Task Benchmark.",
        "training_data": "BooksCorpus dataset (Zhu et al., 2015) containing over 7,000 unique unpublished books from a variety of genres including Adventure, Fantasy, and Romance.",
        "inference_speed": "Not Specified",
        "memory_compute_requirements": "Requires an expensive pre-training step - 1 month on 8 GPUs. It is also a large model (in comparison to prior work) and consequently uses more compute and memory.",
        "fine_tuning_capability": "The model does fine-tune to new tasks very quickly which helps mitigate the additional resource requirements.",
        "bias_fairness": "Predictions generated by this model can include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups.",
        "model_size": "Not Specified",
        "licensing": "MIT License",
        "advantages": "Improved lexical robustness over previous purely neural approaches to textual entailment. Performance improvement across a broad range of tasks."
    },
    "Meta-Llama-3-8B": {
        "model_name": "Meta-Llama-3-8B",
        "model_architecture": "Llama 3 is an auto-regressive language model that uses an optimized transformer architecture.",
        "training_objective": "The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks.",
        "parameters": "8B and 70B",
        "primary_use_case": "Llama 3 is intended for commercial and research use in English. Instruction tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks.",
        "performance_metrics": "Not provided",
        "training_data": "A new mix of publicly available online data. Token counts refer to pretraining data only. Both the 8 and 70B versions use Grouped-Query Attention (GQA) for improved inference scalability.",
        "inference_speed": "Not provided",
        "memory_compute_requirements": "Not provided",
        "fine_tuning_capability": "The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.",
        "bias_fairness": "Not provided",
        "model_size": "8B and 70B",
        "licensing": "A custom commercial license is available at: https://llama.meta.com/llama3/license",
        "advantages": "The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks."
    },
    "Llama-2-7b": {
        "model_name": "Llama-2-7b",
        "model_architecture": "Auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align to human preferences for helpfulness and safety.",
        "training_objective": "Pretrained and fine-tuned for dialogue use cases.",
        "parameters": "7 billion parameters",
        "primary_use_case": "Commercial and research use in English. Tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks.",
        "performance_metrics": "Llama 2 family of models outperform open-source chat models on most benchmarks and are on par with some popular closed-source models like ChatGPT and PaLM.",
        "training_data": "Trained on mix of publicly available online data totalling 2.0T tokens.",
        "inference_speed": "Not provided",
        "memory_compute_requirements": "Trained with a global batch-size of 4M tokens. Bigger models -  70B -- use Grouped-Query Attention (GQA) for improved inference scalability.",
        "fine_tuning_capability": "Yes, with supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF).",
        "bias_fairness": "May produce inaccurate, biased or other objectionable responses to user prompts.",
        "model_size": "7 billion parameters",
        "licensing": "Use of this model is governed by the Meta license. A custom commercial license is available.",
        "advantages": "Llama 2 family of models outperform open-source chat models on most benchmarks and are on par with some popular closed-source models like ChatGPT and PaLM."
    },
    "CodeLlama-70b-Instruct-hf": {
        "model_name": "CodeLlama-70b-Instruct-hf",
        "model_architecture": "Code Llama is an auto-regressive language model that uses an optimized transformer architecture. It was fine-tuned with up to 16k tokens. This variant does not support long context of up to 100k tokens.",
        "training_objective": "The model is designed for general code synthesis and understanding.",
        "parameters": "70 billion",
        "primary_use_case": "Code Llama and its variants are intended for commercial and research use in English and relevant programming languages. The base model Code Llama can be adapted for a variety of code synthesis and understanding tasks, Code Llama - Python is designed specifically to handle the Python programming language, and Code Llama - Instruct is intended to be safer to use for code assistant and generation applications.",
        "performance_metrics": "See evaluations for the main models and detailed ablations in Section 3 and safety evaluations in Section 4 of the research paper.",
        "training_data": "This is a static model trained on an offline dataset.",
        "inference_speed": "Not specified",
        "memory_compute_requirements": "Not specified",
        "fine_tuning_capability": "Not specified",
        "bias_fairness": "Code Llama and its variants are a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Code Llama\u2019s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate or objectionable responses to user prompts.",
        "model_size": "70 billion parameters",
        "licensing": "A custom commercial license is available at: https://ai.meta.com/resources/models-and-libraries/llama-downloads/",
        "advantages": "Code Llama comes in four model sizes, and three variants: Code Llama: base models designed for general code synthesis and understanding Code Llama - Python: designed specifically for Python Code Llama - Instruct: for instruction following and safer deployment. All variants are available in sizes of 7B, 13B, 34B, and 70B parameters."
    },
    "bert-base-uncased": {
        "model_name": "bert-base-uncased",
        "model_architecture": "BERT",
        "training_objective": "Masked language modeling (MLM) and Next sentence prediction (NSP)",
        "parameters": "110M",
        "primary_use_case": "Sentence classification, token classification or question answering",
        "performance_metrics": {
            "MNLI-(m/mm)": "84.6/83.4",
            "QQP": "71.2",
            "QNLI": "90.5",
            "SST-2": "93.5",
            "CoLA": "52.1",
            "STS-B": "85.8",
            "MRPC": "88.9",
            "RTE": "66.4"
        },
        "training_data": "BookCorpus, a dataset consisting of 11,038 unpublished books and English Wikipedia",
        "inference_speed": "Not provided",
        "memory_compute_requirements": "Trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size of 256",
        "fine_tuning_capability": "Yes, fine-tuned versions of a task that interests you can be found in the model hub",
        "bias_fairness": "This model can have biased predictions",
        "model_size": "Not provided",
        "licensing": "Not provided",
        "advantages": "BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labeling them in any way."
    },
    "roberta-base": {
        "model_name": "roberta-base",
        "model_architecture": "RoBERTa",
        "training_objective": "Masked Language Modeling (MLM)",
        "parameters": "Adam optimizer with learning rate of 6e-4, Beta1=0.9, Beta2=0.98, Epsilon=1e-6, weight decay of 0.01, learning rate warmup for 24,000 steps and linear decay of the learning rate after.",
        "primary_use_case": "Intended to be fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification, or question answering.",
        "performance_metrics": {
            "Glue test results": {
                "MNLI": 87.6,
                "QQP": 91.9,
                "QNLI": 92.8,
                "SST-2": 94.8,
                "CoLA": 63.6,
                "STS-B": 91.2,
                "MRPC": 90.2,
                "RTE": 78.7
            }
        },
        "training_data": "RoBERTa model was pretrained on the reunion of five datasets: BookCorpus, English Wikipedia, CC-News, OpenWebText, Stories. Together these datasets weigh 160GB of text.",
        "inference_speed": "Not Provided",
        "memory_compute_requirements": "Trained on 1024 V100 GPUs",
        "fine_tuning_capability": "Highly encouraged to fine-tune on a downstream task.",
        "bias_fairness": "The training data contains a lot of unfiltered content from the internet, which is far from neutral. Therefore, the model can have biased predictions.",
        "model_size": "Not Provided",
        "licensing": "Not Provided",
        "advantages": "Pretrained on the raw texts only, with an automatic process to generate inputs and labels from those texts. Can learn a bidirectional representation of the sentence."
    },
    "t5-base": {
        "model_name": "t5-base",
        "model_architecture": "Text-To-Text Transfer Transformer (T5)",
        "training_objective": "Reframe all NLP tasks into a unified text-to-text-format where the input and output are always text strings",
        "parameters": "220 million",
        "primary_use_case": "Machine translation, document summarization, question answering, and classification tasks such as sentiment analysis",
        "performance_metrics": "Evaluated on 24 tasks, full results in research paper",
        "training_data": "Colossal Clean Crawled Corpus (C4), Wiki-DPR, CoLA, SST-2, MRPC, STS-B, QQP, MNLI, QNLI, RTE, CB, COPA, WIC, MultiRC, ReCoRD, BoolQ",
        "inference_speed": "Not provided",
        "memory_compute_requirements": "Not provided",
        "fine_tuning_capability": "Same model, loss function, and hyperparameters can be applied on any NLP task",
        "bias_fairness": "Not provided",
        "model_size": "Not provided",
        "licensing": "Apache 2.0",
        "advantages": "Flexible framework that can handle a variety of NLP tasks, unified training procedure"
    },
    "bloom": {
        "bloom": {
            "model_name": "BigScience Large Open-science Open-access Multilingual Language ModelVersion 1.3 / 6 July 2022Current Checkpoint:Training Iteration  95000",
            "model_architecture": "Transformer-based Language Model",
            "training_objective": "Cross Entropy with mean reduction",
            "parameters": "176,247,271,424 parameters:3,596,615,680 embedding parameters70 layers, 112 attention headsHidden layers are 14336-dimensionalSequence length of 2048 tokens used",
            "primary_use_case": "Text generation, information extraction, question answering, summarization",
            "performance_metrics": "Perplexity, Cross EntropyLoss",
            "training_data": "46 natural languages and 13 programming languages in 1.6TB of pre-processed text, converted into 350B unique tokens",
            "inference_speed": "About 150 TFLOP per GPU per second",
            "memory_compute_requirements": "384 A100 80GB GPUs and 32 A100 80GB GPUs in reserve, 8 GPUs per node Using NVLink 4 inter-gpu connects, 4 OmniPath linksCPU memory: 512GB per nodeGPU memory: 640GB per nodeInter-node connect: Omni-Path Architecture",
            "fine_tuning_capability": "Can be fine-tuned for specific tasks",
            "bias_fairness": "May overrepresent some viewpoints and underrepresent others, may contain stereotypes",
            "model_size": "Checkpoint size: Bf16 weights: 329GBFull checkpoint with optimizer states: 2.3TB",
            "licensing": "RAIL License v1.0",
            "advantages": "Able to output coherent text in 46 languages and 13 programming languages, can perform text tasks it hasn't been explicitly trained for by casting them as text generation tasks"
        }
    },
    "xlnet-base-cased": {
        "model_name": "xlnet-base-cased",
        "model_architecture": "XLNet model pre-trained on English language",
        "training_objective": "XLNet is a new unsupervised language representation learning method based on a novel generalized permutation language modeling objective. Additionally, XLNet employs Transformer-XL as the backbone model, exhibiting excellent performance for language tasks involving long context",
        "parameters": "Not Specified",
        "primary_use_case": "The model is mostly intended to be fine-tuned on a downstream task. Primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering",
        "performance_metrics": "XLNet achieves state-of-the-art (SOTA) results on various downstream language tasks including question answering, natural language inference, sentiment analysis, and document ranking",
        "training_data": "Pre-trained on English language",
        "inference_speed": "Not Specified",
        "memory_compute_requirements": "Not Specified",
        "fine_tuning_capability": "Yes, this model is intended to be fine-tuned on a downstream task",
        "bias_fairness": "Not Specified",
        "model_size": "base-sized model",
        "licensing": "Not Specified",
        "advantages": "XLNet is a new unsupervised language representation learning method based on a novel generalized permutation language modeling objective. Additionally, XLNet employs Transformer-XL as the backbone model, exhibiting excellent performance for language tasks involving long context"
    },
    "Mixtral-8x7B-v0.1": {
        "model_name": "Mixtral-8x7B-v0.1",
        "model_architecture": "Large Language Model (LLM)",
        "training_objective": "Not specified",
        "parameters": "Based on Sparse Mixture of Experts",
        "primary_use_case": "Generative tasks",
        "performance_metrics": "Outperforms Llama 2 70B on most benchmarks tested",
        "training_data": "Not specified",
        "inference_speed": "Not specified",
        "memory_compute_requirements": "Can be optimized by using half-precision, lower precision (8-bit & 4-bit) and Flash Attention 2",
        "fine_tuning_capability": "Not specified",
        "bias_fairness": "No moderation mechanisms",
        "model_size": "Not specified",
        "licensing": "Not specified",
        "advantages": "Based on the original Mixtraltorrent release, but the file format and parameter names are different. Can be instantiated with Hugging Face transformers library and vLLMserving"
    },
    "distilbert-base-uncased": {
        "model_name": "distilbert-base-uncased",
        "model_architecture": "DistilBERT",
        "training_objective": "Distillation loss, Masked language modeling (MLM), Cosine embedding loss",
        "parameters": "Not specified",
        "primary_use_case": "Masked language modeling or next sentence prediction",
        "performance_metrics": "See the Glue test results section",
        "training_data": "BookCorpus, a dataset consisting of 11,038 unpublished books and English Wikipedia",
        "inference_speed": "Faster than BERT",
        "memory_compute_requirements": "Requires 8 16 GB V100 for training",
        "fine_tuning_capability": "Yes, primarily aimed at being fine-tuned on tasks that use the whole sentence",
        "bias_fairness": "Can have biased predictions, inherits some ofthe bias of its teacher model",
        "model_size": "Smaller than BERT",
        "licensing": "Not specified",
        "advantages": "Smaller and faster than BERT, trained on large publicly available data with an automatic process to generate inputs and labels from those texts using the BERT base model"
    },
    "gemma-2-9b": {
        "model_name": "gemma-2-9b",
        "model_architecture": "Gemma models are text-to-text, decoder-only large language models, available in English, with open weights for both pre-trained variants and instruction-tuned variants.",
        "training_objective": "Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models.",
        "parameters": "Weight parameters are available in both pre-trained variants and instruction-tuned variants.",
        "primary_use_case": "Gemma models are well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning.",
        "performance_metrics": "These models were evaluated against a large collection of different datasets and metrics to cover different aspects of text generation.",
        "training_data": "These models were trained on a dataset of text data that includes a wide variety of sources. The 27B model was trained with 13 trillion tokens and the 9B model was trained with 8 trillion tokens.",
        "inference_speed": "Running the model on a single / multi GPU",
        "memory_compute_requirements": "Gemma was trained using the latest generation of Tensor Processing Unit (TPU) hardware (TPUv5p).",
        "fine_tuning_capability": "Gemma models are available in both pre-trained variants and instruction-tuned variants.",
        "bias_fairness": "Our evaluation methods include structured evaluations and internal red-teaming testing of relevant content policies.",
        "model_size": "Their relatively small size makes it possible to deploy them in environments with limited resources such as a laptop, desktop or your own cloud infrastructure.",
        "licensing": "Terms of Use: Terms",
        "advantages": "At the time of release, this family of models provides high-performance open large language model implementations designed from the ground up for Responsible AI development compared to similarly sized models."
    },
    "gemma-2-27b-it": {
        "model_name": "gemma-2-27b-it",
        "model_architecture": "Text-to-Text, decoder-only large language models",
        "training_objective": "Well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning",
        "parameters": "N/A",
        "primary_use_case": "Text generation, Chatbots and Conversational AI, Text Summarization, Natural Language Processing (NLP) Research, Language Learning Tools, Knowledge Exploration",
        "performance_metrics": "Evaluated against a large collection of different datasets and metrics to cover different aspects of text generation",
        "training_data": "Trained on a dataset of text data that includes a wide variety of sources such as Web Documents, Code, Mathematics",
        "inference_speed": "N/A",
        "memory_compute_requirements": "Trained using the latest generation of Tensor Processing Unit (TPU) hardware (TPUv5p)",
        "fine_tuning_capability": "N/A",
        "bias_fairness": "Models underwent careful scrutiny, input data pre-processing described and posterior evaluations reported in this card",
        "model_size": "27B model was trained with 13 trillion tokens",
        "licensing": "Terms of Use: Terms",
        "advantages": "Offers high-performance open large language model implementations designed from the ground up for Responsible AI development compared to similarly sized models"
    },
    "codegen-350M-multi": {
        "model_name": "codegen-350M-multi",
        "model_architecture": "CodeGen",
        "training_objective": "Cross-entropy loss to maximize the likelihood of sequential inputs",
        "parameters": "350M",
        "primary_use_case": "Program synthesis, generating executable code given English prompts",
        "performance_metrics": "Evaluated on two code generation benchmark: HumanEval and MTPB",
        "training_data": "BigQuery, a large-scale dataset of multiple programming languages from GitHub repositories including C, C++, Go, Java, JavaScript, and Python",
        "inference_speed": "Not provided",
        "memory_compute_requirements": "Trained using multiple TPU-v4-512 by Google, leveraging data and model parallelism",
        "fine_tuning_capability": "Not provided",
        "bias_fairness": "Not provided",
        "model_size": "350M",
        "licensing": "Not provided",
        "advantages": "Capable of extracting features from given natural language and programming language texts, and calculating the likelihood of them"
    }
}