{
    "detr-resnet-50": {
        "model_name": "DETR-ResNet-50",
        "developed_by": "Facebook AI Research",
        "model_type": "Object Detection",
        "licensing": "Apache 2.0",
        "installation": {
            "python_version": ">=3.6",
            "additional_libraries": [
                "transformers",
                "torch",
                "PIL",
                "requests"
            ],
            "installation_command": "pip install transformers"
        },
        "usage": {
            "cli_example": "Not provided",
            "python_example": "See the example in the original JSON"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": [
                "DETR-ResNet-50"
            ],
            "pretrained_datasets": [
                "COCO 2017"
            ],
            "performance_metrics": {
                "example_metrics_table": [
                    {
                        "model": "DETR-ResNet-50",
                        "size_pixels": "800x1333",
                        "map_val50_95": "42.0",
                        "speed_cpu_onnx_ms": "Not provided",
                        "speed_a100_tensorrt_ms": "Not provided",
                        "params_m": "Not provided",
                        "flops_b": "Not provided"
                    }
                ]
            }
        },
        "model_details": {
            "model_description": "DETR (End-to-End Object Detection) model with ResNet-50 backbone. It uses object queries to detect objects in images. The model is trained using a bipartite matching loss.",
            "supported_labels": "COCO object detection labels"
        },
        "limitations_and_biases": {
            "limitations": "Not provided",
            "biases": "Not provided",
            "risks": "Not provided"
        },
        "recommendations": "Not provided",
        "compute_infrastructure": {
            "hardware": "16 V100 GPUs",
            "software": "PyTorch"
        },
        "contact_information": {
            "model_card_contact": "Not provided"
        },
        "references": {
            "related_papers_and_resources": [
                "End-to-End Object Detection with Transformers"
            ]
        },
        "example_implementation": {
            "sample_code": "See the example in the original JSON"
        }
    },
    "YOLOv8": {
        "model_name": "YOLOv8",
        "developed_by": "Ultralytics",
        "model_type": "Object Detection, Segmentation, Pose Estimation",
        "licensing": [
            "AGPL-3.0 License (open-source)",
            "Enterprise License (commercial use)"
        ],
        "installation": {
            "python_version": ">=3.8",
            "additional_libraries": [
                "requirements"
            ],
            "installation_command": "pip install ultralytics"
        },
        "usage": {
            "cli_example": "yolo predict model=yolov8n.pt source='https://ultralytics.com/images/bus.jpg'",
            "python_example": "model = YOLO(\"yolov8n.yaml\")"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": [
                "YOLOv8n-det (COCO)",
                "YOLOv8s-det (COCO)",
                "YOLOv8m-det (COCO)",
                "YOLOv8l-det (COCO)",
                "YOLOv8x-det (COCO)",
                "YOLOv8n-det (Open Image V7)",
                "YOLOv8s-det (Open Image V7)",
                "YOLOv8m-det (Open Image V7)",
                "YOLOv8l-det (Open Image V7)",
                "YOLOv8x-det (Open Image V7)",
                "YOLOv8n-seg (COCO-Seg)",
                "YOLOv8s-seg (COCO-Seg)",
                "YOLOv8m-seg (COCO-Seg)",
                "YOLOv8l-seg (COCO-Seg)",
                "YOLOv8x-seg (COCO-Seg)",
                "YOLOv8n-pose (COCO-Pose)",
                "YOLOv8s-pose (COCO-Pose)",
                "YOLOv8m-pose (COCO-Pose)",
                "YOLOv8l-pose (COCO-Pose)",
                "YOLOv8x-pose (COCO-Pose)",
                "YOLOv8n-obb (DOTAv1)",
                "YOLOv8s-obb (DOTAv1)",
                "YOLOv8m-obb (DOTAv1)",
                "YOLOv8l-obb (DOTAv1)",
                "YOLOv8x-obb (DOTAv1)",
                "YOLOv8n-cls (ImageNet)",
                "YOLOv8s-cls (ImageNet)",
                "YOLOv8m-cls (ImageNet)",
                "YOLOv8l-cls (ImageNet)",
                "YOLOv8x-cls (ImageNet)"
            ],
            "pretrained_datasets": [
                "COCO",
                "Open Image V7",
                "COCO-Seg",
                "COCO-Pose",
                "DOTAv1",
                "ImageNet"
            ],
            "performance_metrics": {
                "example_metrics_table": [
                    {
                        "model": "YOLOv8n-det (COCO)",
                        "size_pixels": "640",
                        "map_val50_95": "37.38",
                        "speed_cpu_onnx_ms": "0.993",
                        "speed_a100_tensorrt_ms": "8.7",
                        "params_m": null,
                        "flops_b": null
                    },
                    {
                        "model": "YOLOv8s-det (COCO)",
                        "size_pixels": "640",
                        "map_val50_95": "44.91",
                        "speed_cpu_onnx_ms": "1.201",
                        "speed_a100_tensorrt_ms": "11.2",
                        "params_m": null,
                        "flops_b": null
                    },
                    {
                        "model": "YOLOv8m-det (COCO)",
                        "size_pixels": "640",
                        "map_val50_95": "50.22",
                        "speed_cpu_onnx_ms": "1.832",
                        "speed_a100_tensorrt_ms": "25.9",
                        "params_m": null,
                        "flops_b": null
                    },
                    {
                        "model": "YOLOv8l-det (COCO)",
                        "size_pixels": "640",
                        "map_val50_95": "52.93",
                        "speed_cpu_onnx_ms": "2.394",
                        "speed_a100_tensorrt_ms": "43.7",
                        "params_m": null,
                        "flops_b": null
                    },
                    {
                        "model": "YOLOv8x-det (COCO)",
                        "size_pixels": "640",
                        "map_val50_95": "53.94",
                        "speed_cpu_onnx_ms": "3.536",
                        "speed_a100_tensorrt_ms": "68.2",
                        "params_m": null,
                        "flops_b": null
                    }
                ]
            }
        },
        "model_details": {
            "model_description": "YOLOv8 is a cutting-edge, state-of-the-art (SOTA) model that builds upon the success of previous YOLO versions and introduces new features and improvements to further boost performance and flexibility. YOLOv8 is designed to be fast, accurate, and easy to use, making it an excellent choice for a wide range of object detection and tracking, instance segmentation, image classification and pose estimation tasks.",
            "supported_labels": [
                "80 pre-trained classes (COCO)",
                "600 pre-trained classes (Open Image V7)",
                "80 pre-trained classes (COCO-Seg)",
                "1 pre-trained class (COCO-Pose)",
                "15 pre-trained classes (DOTAv1)",
                "1000 pretrained classes (ImageNet)"
            ]
        },
        "limitations_and_biases": {
            "limitations": [],
            "biases": [],
            "risks": []
        },
        "recommendations": [],
        "compute_infrastructure": {
            "hardware": null,
            "software": "Ultralytics HUB"
        },
        "contact_information": {
            "model_card_contact": null
        },
        "references": {
            "related_papers_and_resources": []
        },
        "example_implementation": {
            "sample_code": null
        }
    },
    "table-transformer-detection": {
        "model_name": "Table Transformer (DETR)",
        "developed_by": "Smock et al.",
        "model_type": "Table Detection",
        "licensing": "Not specified",
        "installation": {
            "python_version": "Not specified",
            "additional_libraries": "Not specified",
            "installation_command": "Not specified"
        },
        "usage": {
            "cli_example": "Not provided",
            "python_example": "Not provided"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": [
                "Table Transformer (DETR) trained on PubTables1M"
            ],
            "pretrained_datasets": [
                "PubTables1M"
            ],
            "performance_metrics": {
                "example_metrics_table": []
            }
        },
        "model_details": {
            "model_description": "The Table Transformer is equivalent to DETR, a Transformer-based object detection model. Note that the authors decided to use the \"normalize before\" setting of DETR, which means that layernorm is applied before self- and cross-attention.",
            "supported_labels": [
                "Tables"
            ]
        },
        "limitations_and_biases": {
            "limitations": [],
            "biases": [],
            "risks": []
        },
        "recommendations": [],
        "compute_infrastructure": {
            "hardware": "Not specified",
            "software": "Not specified"
        },
        "contact_information": {
            "model_card_contact": "Not provided"
        },
        "references": {
            "related_papers_and_resources": [
                "PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents"
            ]
        },
        "example_implementation": {
            "sample_code": "Not provided"
        }
    },
    "web-form-ui-field-detection": {
        "model_name": "web-form-ui-field-detection",
        "developed_by": "Nehul Agrawal and Rahul parihar",
        "model_type": "Object Detection",
        "licensing": "Not specified",
        "installation": {
            "python_version": "Not specified",
            "additional_libraries": [
                "ultralyticsplus==0.0.28",
                "ultralytics==8.0.43"
            ],
            "installation_command": "pip install ultralyticsplusimportYOLO, render_result"
        },
        "usage": {
            "cli_example": "Not provided",
            "python_example": "from ultralyticsplusimportYOLO, render_result\n# load model\nmodel = YOLO('foduucom/web-form-ui-field-detection')\n# set model parameters\nmodel.overrides['conf'] =0.25\n# NMS confidence threshold\nmodel.overrides['iou'] =0.45\n# NMS IoU threshold\nmodel.overrides['agnostic_nms'] =False\n# NMS class-agnostic\nmodel.overrides['max_det'] =1000\n# maximum number of detections per image\n# set image\nimage ='/path/to/your/document/images'\n# perform inferenceresults = model.predict(image)\n# observe results\nprint(results[0].boxes)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": [
                "web-form-ui-field-detection"
            ],
            "pretrained_datasets": [
                "Dataset of annotated ui form images"
            ],
            "performance_metrics": {
                "example_metrics_table": [
                    {
                        "model": "web-form-ui-field-detection",
                        "size_pixels": "Not specified",
                        "map_val50_95": "Not specified",
                        "speed_cpu_onnx_ms": "Not specified",
                        "speed_a100_tensorrt_ms": "Not specified",
                        "params_m": "Not specified",
                        "flops_b": "Not specified"
                    }
                ]
            }
        },
        "model_details": {
            "model_description": "The web-form-Detect model is a yolov8 object detection model trained to detect and locate ui form fields in images. It is built upon the ultralytics library and fine-tuned using a dataset of annotated ui form images.",
            "supported_labels": [
                "Name",
                "number",
                "email",
                "password",
                "button",
                "redio bullet"
            ]
        },
        "limitations_and_biases": {
            "limitations": [
                "Performance may vary based on input data distribution and quality",
                "May struggle with detecting ui form fields in cases of extreme occlusion",
                "May not generalize well to non-standard ui form formats or variations"
            ],
            "biases": "Not specified",
            "risks": "Not specified"
        },
        "recommendations": "Not provided",
        "compute_infrastructure": {
            "hardware": "NVIDIA GeForce RTX 3090 GPU",
            "software": "Jupyter Notebook environment"
        },
        "contact_information": {
            "model_card_contact": "info@foduu.com"
        },
        "references": {
            "related_papers_and_resources": [
                "Not provided"
            ]
        },
        "example_implementation": {
            "sample_code": "Not provided"
        }
    },
    "rtdetr_r50vd_coco_o365": {
        "model_name": "rtdetr_r50vd_coco_o365",
        "developed_by": "Yian Zhao and Sangbum Choi",
        "model_type": "RT-DETR",
        "licensing": "Apache-2.0",
        "installation": {
            "python_version": "Unspecified",
            "additional_libraries": [
                "torch",
                "requests",
                "PIL",
                "transformers"
            ],
            "installation_command": "pip install transformers"
        },
        "usage": {
            "cli_example": "Not provided",
            "python_example": "Refer to the original JSON for the code snippet."
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": [],
            "pretrained_datasets": [
                "COCO 2017",
                "Objects365"
            ],
            "performance_metrics": {
                "example_metrics_table": []
            }
        },
        "model_details": {
            "model_description": "Real-Time DEtection TRansformer (RT-DETR) - an end-to-end object detector that aims to balance speed and accuracy.",
            "supported_labels": [
                "sofa",
                "cat",
                "remote"
            ]
        },
        "limitations_and_biases": {
            "limitations": [],
            "biases": [],
            "risks": []
        },
        "recommendations": [],
        "compute_infrastructure": {
            "hardware": "Unspecified",
            "software": "Unspecified"
        },
        "contact_information": {
            "model_card_contact": "Sangbum Choi"
        },
        "references": {
            "related_papers_and_resources": [
                "https://arxiv.org/abs/2304.08069"
            ]
        },
        "example_implementation": {
            "sample_code": "Refer to the original JSON for the code snippet."
        }
    },
    "yolov10x": {
        "model_name": "yolov10x",
        "developed_by": "THU-MIG",
        "model_type": "Object Detection",
        "licensing": null,
        "installation": {
            "python_version": null,
            "additional_libraries": null,
            "installation_command": "pip install git+https://github.com/THU-MIG/yolov10.git"
        },
        "usage": {
            "cli_example": null,
            "python_example": "fromultralyticsimportYOLOv10\n\nmodel = YOLOv10.from_pretrained('jameslahm/yolov10x')\nsource ='http://images.cocodataset.org/val2017/000000039769.jpg'model.predict(source=source, save=True)"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": [
                "jameslahm/yolov10x"
            ],
            "pretrained_datasets": [],
            "performance_metrics": {
                "example_metrics_table": []
            }
        },
        "model_details": {
            "model_description": "YOLOv10: Real-Time End-to-End Object Detection",
            "supported_labels": []
        },
        "limitations_and_biases": {
            "limitations": [],
            "biases": [],
            "risks": []
        },
        "recommendations": [],
        "compute_infrastructure": {
            "hardware": null,
            "software": null
        },
        "contact_information": {
            "model_card_contact": null
        },
        "references": {
            "related_papers_and_resources": [
                "https://arxiv.org/abs/2405.14458v1",
                "https://github.com/THU-MIG/yolov10"
            ]
        },
        "example_implementation": {
            "sample_code": null
        }
    },
    "YOLOv10-Document-Layout-Analysis": {
        "model_name": "YOLOv10-Document-Layout-Analysis",
        "developed_by": "Omar Moured",
        "model_type": "Object Detection",
        "licensing": "Not specified",
        "installation": {
            "python_version": "Not specified",
            "additional_libraries": "Not specified",
            "installation_command": "Not specified"
        },
        "usage": {
            "cli_example": "Not specified",
            "python_example": "Not specified"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": [
                "YOLOv10-x",
                "YOLOv10-b",
                "YOLOv10-l",
                "YOLOv10-m",
                "YOLOv10-s",
                "YOLOv10-n"
            ],
            "pretrained_datasets": [
                "Doclaynet-base"
            ],
            "performance_metrics": {
                "example_metrics_table": [
                    {
                        "model": "YOLOv10-x",
                        "size_pixels": "Not specified",
                        "map_val50_95": "0.740",
                        "speed_cpu_onnx_ms": "Not specified",
                        "speed_a100_tensorrt_ms": "Not specified",
                        "params_m": "Not specified",
                        "flops_b": "Not specified"
                    },
                    {
                        "model": "YOLOv10-b",
                        "size_pixels": "Not specified",
                        "map_val50_95": "0.732",
                        "speed_cpu_onnx_ms": "Not specified",
                        "speed_a100_tensorrt_ms": "Not specified",
                        "params_m": "Not specified",
                        "flops_b": "Not specified"
                    },
                    {
                        "model": "YOLOv10-l",
                        "size_pixels": "Not specified",
                        "map_val50_95": "0.732",
                        "speed_cpu_onnx_ms": "Not specified",
                        "speed_a100_tensorrt_ms": "Not specified",
                        "params_m": "Not specified",
                        "flops_b": "Not specified"
                    },
                    {
                        "model": "YOLOv10-m",
                        "size_pixels": "Not specified",
                        "map_val50_95": "0.737",
                        "speed_cpu_onnx_ms": "Not specified",
                        "speed_a100_tensorrt_ms": "Not specified",
                        "params_m": "Not specified",
                        "flops_b": "Not specified"
                    },
                    {
                        "model": "YOLOv10-s",
                        "size_pixels": "Not specified",
                        "map_val50_95": "0.713",
                        "speed_cpu_onnx_ms": "Not specified",
                        "speed_a100_tensorrt_ms": "Not specified",
                        "params_m": "Not specified",
                        "flops_b": "Not specified"
                    },
                    {
                        "model": "YOLOv10-n",
                        "size_pixels": "Not specified",
                        "map_val50_95": "0.685",
                        "speed_cpu_onnx_ms": "Not specified",
                        "speed_a100_tensorrt_ms": "Not specified",
                        "params_m": "Not specified",
                        "flops_b": "Not specified"
                    }
                ]
            }
        },
        "model_details": {
            "model_description": "YOLOv10-Document-Layout-Analysis is a real-time object detection model designed for document layout analysis. It is based on the YOLOv10 architecture and has been fine-tuned on the Doclaynet-base dataset for detecting text, tables, and images in document images.",
            "supported_labels": [
                "Text",
                "Tables",
                "Images"
            ]
        },
        "limitations_and_biases": {
            "limitations": [
                "Not specified"
            ],
            "biases": [
                "Not specified"
            ],
            "risks": [
                "Not specified"
            ]
        },
        "recommendations": [
            "Not specified"
        ],
        "compute_infrastructure": {
            "hardware": "4xA100 GPUs",
            "software": "Not specified"
        },
        "contact_information": {
            "model_card_contact": "Omar Moured"
        },
        "references": {
            "related_papers_and_resources": [
                "YOLOv10: Real-Time End-to-End Object Detection",
                "DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis"
            ]
        },
        "example_implementation": {
            "sample_code": "Not specified"
        }
    },
    "Object-Detection-RetinaNet": {
        "model_name": "Object-Detection-RetinaNet",
        "developed_by": "Kavya Bisht",
        "model_type": "Object Detection",
        "licensing": "Not specified",
        "installation": {
            "python_version": "Not specified",
            "additional_libraries": "Not specified",
            "installation_command": "Not specified"
        },
        "usage": {
            "cli_example": "Not specified",
            "python_example": "Not specified"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": [],
            "pretrained_datasets": [],
            "performance_metrics": {
                "example_metrics_table": []
            }
        },
        "model_details": {
            "model_description": "Model descriptionImplementing RetinaNet: Focal Loss for Dense Object Detection.This repo contains the model for the notebookObject Detection with RetinaNetHere the model is tasked with localizing the objects present in an image, and at the same time, classifying them into different categories. In this, RetinaNet has been implemented, a popularsingle-stage detector, which is accurate and runs fast. RetinaNet uses afeature pyramid networkto efficiently detect objects at multiple scales and introduces a new loss, theFocal loss function, to alleviate the problem of the extreme foreground-background class imbalance.Full credits go toSrihari HumbarwadiReferencesRetinaNet PaperFeature Pyramid Network Paper",
            "supported_labels": []
        },
        "limitations_and_biases": {
            "limitations": [],
            "biases": [],
            "risks": []
        },
        "recommendations": [],
        "compute_infrastructure": {
            "hardware": "Not specified",
            "software": "Not specified"
        },
        "contact_information": {
            "model_card_contact": "Not specified"
        },
        "references": {
            "related_papers_and_resources": [
                "RetinaNet Paper",
                "Feature Pyramid Network Paper"
            ]
        },
        "example_implementation": {
            "sample_code": "Not specified"
        }
    },
    "yolos-small-finetuned-license-plate-detection": {
        "model_name": "yolos-small-finetuned-license-plate-detection",
        "developed_by": "nickmuchi",
        "model_type": "Object Detection",
        "licensing": "TBD",
        "installation": {
            "python_version": "TBD",
            "additional_libraries": "TBD",
            "installation_command": "TBD"
        },
        "usage": {
            "cli_example": "TBD",
            "python_example": "TBD"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": [
                "yolos-small-finetuned-license-plate-detection"
            ],
            "pretrained_datasets": [
                "COCO 2017 object detection",
                "license-plate-recognition"
            ],
            "performance_metrics": {
                "example_metrics_table": [
                    {
                        "model": "yolos-small-finetuned-license-plate-detection",
                        "size_pixels": "TBD",
                        "map_val50_95": "49.0",
                        "speed_cpu_onnx_ms": "TBD",
                        "speed_a100_tensorrt_ms": "TBD",
                        "params_m": "TBD",
                        "flops_b": "TBD"
                    }
                ]
            }
        },
        "model_details": {
            "model_description": "YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN).",
            "supported_labels": [
                "license plate"
            ]
        },
        "limitations_and_biases": {
            "limitations": [
                "TBD"
            ],
            "biases": [
                "TBD"
            ],
            "risks": [
                "TBD"
            ]
        },
        "recommendations": [
            "TBD"
        ],
        "compute_infrastructure": {
            "hardware": "TBD",
            "software": "TBD"
        },
        "contact_information": {
            "model_card_contact": "TBD"
        },
        "references": {
            "related_papers_and_resources": [
                "TBD"
            ]
        },
        "example_implementation": {
            "sample_code": "TBD"
        }
    },
    "yolos-fashionpedia": {
        "model_name": "yolos-fashionpedia",
        "developed_by": "N/A",
        "model_type": "Object Detection",
        "licensing": "N/A",
        "installation": {
            "python_version": "N/A",
            "additional_libraries": "N/A",
            "installation_command": "N/A"
        },
        "usage": {
            "cli_example": "N/A",
            "python_example": "N/A"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": [],
            "pretrained_datasets": [],
            "performance_metrics": {
                "example_metrics_table": []
            }
        },
        "model_details": {
            "model_description": "A fine-tuned object detection model for fashion.",
            "supported_labels": [
                "shirt, blouse",
                "top, t-shirt, sweatshirt",
                "sweater",
                "cardigan",
                "jacket",
                "vest",
                "pants",
                "shorts",
                "skirt",
                "coat",
                "dress",
                "jumpsuit",
                "cape",
                "glasses",
                "hat",
                "headband, head covering, hair accessory",
                "tie",
                "glove",
                "watch",
                "belt",
                "leg warmer",
                "tights, stockings",
                "sock",
                "shoe",
                "bag, wallet",
                "scarf",
                "umbrella",
                "hood",
                "collar",
                "lapel",
                "epaulette",
                "sleeve",
                "pocket",
                "neckline",
                "buckle",
                "zipper",
                "applique",
                "bead",
                "bow",
                "flower",
                "fringe",
                "ribbon",
                "rivet",
                "ruffle",
                "sequin",
                "tassel"
            ]
        },
        "limitations_and_biases": {
            "limitations": [],
            "biases": [],
            "risks": []
        },
        "recommendations": [],
        "compute_infrastructure": {
            "hardware": "N/A",
            "software": "N/A"
        },
        "contact_information": {
            "model_card_contact": "N/A"
        },
        "references": {
            "related_papers_and_resources": []
        },
        "example_implementation": {
            "sample_code": "N/A"
        }
    },
    "mmdet-yolox-tiny": {
        "model_name": "mmdet-yolox-tiny",
        "developed_by": "Fatih Cagatay Akyon, Sinan Onur Altinuc, Alptekin Temizel",
        "model_type": "Object Detection",
        "licensing": "Not specified",
        "installation": {
            "python_version": "Not specified",
            "additional_libraries": [
                "sahi",
                "mmdet",
                "mmcv-full",
                "torch"
            ],
            "installation_command": "pip install -U sahi\npip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.11.0/index.html\npip install mmdet==2.26.0"
        },
        "usage": {
            "cli_example": "Not provided",
            "python_example": "from sahi import AutoDetectionModel\nfrom sahi.utils.file import download_from_url\nfrom sahi.predictimport get_prediction\nfrom sahi.cv import read_image_as_pil\n\ndownload_from_url(\n  MMDET_YOLOX_TINY_MODEL_URL,\n  MMDET_YOLOX_TINY_MODEL_PATH,\n)\ndownload_from_url(\n  MMDET_YOLOX_TINY_CONFIG_URL,\n  MMDET_YOLOX_TINY_CONFIG_PATH,\n)\n\ndetection_model = AutoDetectionModel.from_pretrained(\n    model_type='mmdet',\n    model_path=MMDET_YOLOX_TINY_MODEL_PATH,\n    config_path=MMDET_YOLOX_TINY_CONFIG_PATH,\n    confidence_threshold=0.5,\n    device=\"cuda:0\",# or 'cpu')\n\nimage = read_image_as_pil(IMAGE_URL)\n\nprediction_result = get_prediction(\n  image=image,\n  detection_model=detection_model\n)\nprediction_result.export_predictions(export_dir='results/')\nprediction_result.object_prediction_list"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": [
                "YOLOX-Tiny"
            ],
            "pretrained_datasets": [
                "ImageNet-1k",
                "COCO 2017"
            ],
            "performance_metrics": {
                "example_metrics_table": []
            }
        },
        "model_details": {
            "model_description": "Improved anchor-free YOLO architecture for object detection task.",
            "supported_labels": [
                "Not specified"
            ]
        },
        "limitations_and_biases": {
            "limitations": [
                "Not specified"
            ],
            "biases": [
                "Not specified"
            ],
            "risks": [
                "Not specified"
            ]
        },
        "recommendations": [
            "Not specified"
        ],
        "compute_infrastructure": {
            "hardware": "Not specified",
            "software": "Not specified"
        },
        "contact_information": {
            "model_card_contact": "Not specified"
        },
        "references": {
            "related_papers_and_resources": [
                "YOLOX: Exceeding YOLO Series in 2021",
                "Slicing Aided Hyper Inference and Fine-tuning for Small Object Detection"
            ]
        },
        "example_implementation": {
            "sample_code": "Not provided"
        }
    },
    "deta-resnet-50": {
        "model_name": "DETA-ResNet-50",
        "model_description": "Detection Transformers with Assignment\nTL; DR.\nDetection Transformers with Assignment (DETA) re-introduce IoU assignment and NMS for transformer-based detectors. DETA trains and tests comparably as fast as Deformable-DETR and converges much faster (50.2 mAP in 12 epochs on COCO).",
        "developed_by": "Jeffrey Ouyang-Zhang, Jang Hyun Cho, Xingyi Zhou, Philipp Kr\u00e4henb\u00fchl",
        "model_type": "Object Detection",
        "licensing": "Not specified",
        "installation": {
            "python_version": "Not specified",
            "additional_libraries": "Not specified",
            "installation_command": "Not specified"
        },
        "usage": {
            "cli_example": "Not specified",
            "python_example": "Not specified"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": [
                "DETA-ResNet-50"
            ],
            "pretrained_datasets": [
                "COCO"
            ],
            "performance_metrics": {
                "example_metrics_table": []
            }
        },
        "limitations_and_biases": {
            "limitations": [],
            "biases": [],
            "risks": []
        },
        "recommendations": [],
        "compute_infrastructure": {
            "hardware": "Not specified",
            "software": "Not specified"
        },
        "contact_information": {
            "model_card_contact": "Not specified"
        },
        "references": {
            "related_papers_and_resources": [
                "NMS Strikes Back"
            ]
        },
        "example_implementation": {
            "sample_code": "Not specified"
        }
    },
    "yolov8s-plane-detection": {
        "model_name": "yolov8s-plane-detection",
        "developed_by": "keremberke",
        "model_type": "Object Detection",
        "licensing": "Unknown",
        "installation": {
            "python_version": "Unknown",
            "additional_libraries": [
                "ultralyticsplus==0.0.23",
                "ultralytics==8.0.21"
            ],
            "installation_command": "pip install ultralyticsplus==0.0.23 ultralytics==8.0.21"
        },
        "usage": {
            "cli_example": "Unknown",
            "python_example": "from ultralyticsplusimportYOLO, render_result\n# load model\nmodel = YOLO('keremberke/yolov8s-plane-detection')\n# set model parameters\nmodel.overrides['conf'] =0.25\n# NMS confidence threshold\nmodel.overrides['iou'] =0.45\n# NMS IoU threshold\nmodel.overrides['agnostic_nms'] =False\n# NMS class-agnostic\nmodel.overrides['max_det'] =1000\n# maximum number of detections per image\n# set image\nimage ='https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\n# perform inferenceresults = model.predict(image)\n# observe results\nprint(results[0].boxes)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": [],
            "pretrained_datasets": [],
            "performance_metrics": {
                "example_metrics_table": []
            }
        },
        "model_details": {
            "model_description": "Supported Labels['planes']",
            "supported_labels": [
                "planes"
            ]
        },
        "limitations_and_biases": {
            "limitations": [],
            "biases": [],
            "risks": []
        },
        "recommendations": [],
        "compute_infrastructure": {
            "hardware": "Unknown",
            "software": "Unknown"
        },
        "contact_information": {
            "model_card_contact": "Unknown"
        },
        "references": {
            "related_papers_and_resources": []
        },
        "example_implementation": {
            "sample_code": "Unknown"
        }
    },
    "yolov8m-nlf-head-detection": {
        "model_name": "yolov8m-nlf-head-detection",
        "developed_by": "keremberke",
        "model_type": "Object Detection",
        "licensing": "Proprietary",
        "installation": {
            "python_version": "3.8+",
            "additional_libraries": [
                "ultralyticsplus==0.0.24",
                "ultralytics==8.0.23"
            ],
            "installation_command": "pip install ultralyticsplus==0.0.24 ultralytics==8.0.23"
        },
        "usage": {
            "cli_example": "N/A",
            "python_example": "from ultralyticsplusimport YOLO, render_result\nmodel = YOLO('keremberke/yolov8m-nlf-head-detection')\nmodel.overrides['conf'] =0.25\nmodel.overrides['iou'] =0.45\nmodel.overrides['agnostic_nms'] =False\nmodel.overrides['max_det'] =1000\nimage ='https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\nresults = model.predict(image)\nprint(results[0].boxes)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": [
                "keremberke/yolov8m-nlf-head-detection"
            ],
            "pretrained_datasets": [
                "N/A"
            ],
            "performance_metrics": {
                "example_metrics_table": []
            }
        },
        "model_details": {
            "model_description": "Object detection model trained to detect helmets in images.",
            "supported_labels": [
                "Helmet",
                "Helmet-Blurred",
                "Helmet-Difficult",
                "Helmet-Partial",
                "Helmet-Sideline"
            ]
        },
        "limitations_and_biases": {
            "limitations": [],
            "biases": [],
            "risks": []
        },
        "recommendations": [],
        "compute_infrastructure": {
            "hardware": "GPU or CPU",
            "software": "N/A"
        },
        "contact_information": {
            "model_card_contact": "N/A"
        },
        "references": {
            "related_papers_and_resources": []
        },
        "example_implementation": {
            "sample_code": "N/A"
        }
    },
    "deta-swin-large": {
        "model_name": "deta-swin-large",
        "model_description": "Detection Transformers with AssignmentByJeffrey Ouyang-Zhang,Jang Hyun Cho,Xingyi Zhou,Philipp Kr\u00e4henb\u00fchlFrom the paperNMS Strikes Back.TL; DR.DetectionTransformers withAssignment (DETA) re-introduce IoU assignment and NMS for transformer-based detectors. DETA trains and tests comparibly as fast as Deformable-DETR and converges much faster (50.2 mAP in 12 epochs on COCO).",
        "references": {
            "related_papers_and_resources": []
        }
    },
    "fashionfail": {
        "model_name": "FashionFail",
        "developed_by": "Riza Velioglu, Robin Chan, Barbara Hammer",
        "model_type": "Object Detection and Segmentation",
        "licensing": "Server Side Public License (SSPL)",
        "installation": {
            "python_version": "Not specified",
            "additional_libraries": [
                "torchvision",
                "onnxruntime",
                "huggingface_hub"
            ],
            "installation_command": "Not specified"
        },
        "usage": {
            "cli_example": "Not provided",
            "python_example": "See original JSON for usage instructions"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": [
                "facere_base.onnx",
                "facere_plus.onnx"
            ],
            "pretrained_datasets": [
                "Fashionpedia-train",
                "FashionFail-train"
            ],
            "performance_metrics": {
                "example_metrics_table": []
            }
        },
        "model_details": {
            "model_description": "Pre-trained Mask R-CNN fine-tuned on Fashionpedia-train and FashionFail-train datasets.",
            "supported_labels": [
                "Not specified"
            ]
        },
        "limitations_and_biases": {
            "limitations": [],
            "biases": [],
            "risks": []
        },
        "recommendations": [],
        "compute_infrastructure": {
            "hardware": "Not specified",
            "software": "Not specified"
        },
        "contact_information": {
            "model_card_contact": "Not specified"
        },
        "references": {
            "related_papers_and_resources": [
                "[paper](https://arxiv.org/abs/2404.08582)"
            ]
        },
        "example_implementation": {
            "sample_code": "See original JSON for example implementation"
        }
    },
    "screenlist-slicing": {
        "Model Overview": "N/A",
        "model_name": "<Model Name>",
        "developed_by": "<Developed By>",
        "model_type": "<Model Type>",
        "licensing": "<Licensing>",
        "installation": {
            "python_version": "<Python Version>",
            "additional_libraries": "<Additional Libraries>",
            "installation_command": "<Installation Command>"
        },
        "usage": {
            "cli_example": "<CLI Example>",
            "python_example": "<Python Example>"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": [
                "<Available Models>"
            ],
            "pretrained_datasets": [
                "<Pretrained Datasets>"
            ],
            "performance_metrics": {
                "example_metrics_table": [
                    {
                        "model": "<Model>",
                        "size_pixels": "<Size (pixels)>",
                        "map_val50_95": "<mAPval50-95>",
                        "speed_cpu_onnx_ms": "<SpeedCPU ONNX (ms)>",
                        "speed_a100_tensorrt_ms": "<SpeedA100 TensorRT (ms)>",
                        "params_m": "<Params (M)>",
                        "flops_b": "<FLOPs (B)>"
                    }
                ]
            }
        },
        "model_details": {
            "model_description": "<Model Description>",
            "supported_labels": [
                "<Supported Labels>"
            ]
        },
        "limitations_and_biases": {
            "limitations": [
                "<Limitations>"
            ],
            "biases": [
                "<Biases>"
            ],
            "risks": [
                "<Risks>"
            ]
        },
        "recommendations": [
            "<Recommendations>"
        ],
        "compute_infrastructure": {
            "hardware": "<Hardware>",
            "software": "<Software>"
        },
        "contact_information": {
            "model_card_contact": "<Model Card Contact>"
        },
        "references": {
            "related_papers_and_resources": [
                "<Related Papers and Resources>"
            ]
        },
        "example_implementation": {
            "sample_code": "<Sample Code>"
        }
    },
    "yolov8x-visdrone": {
        "model_name": "yolov8x-visdrone",
        "developed_by": "mshamrai",
        "model_type": "Object Detection",
        "licensing": "Unknown",
        "installation": {
            "python_version": "Unknown",
            "additional_libraries": [
                "ultralyticsplus==0.0.28",
                "ultralytics==8.0.43"
            ],
            "installation_command": "pip install ultralyticsplus==0.0.28 ultralytics==8.0.43"
        },
        "usage": {
            "cli_example": "Not provided",
            "python_example": "from ultralyticsplusimportYOLO, render_result\nmodel = YOLO('mshamrai/yolov8x-visdrone')\nmodel.overrides['conf'] =0.25\nmodel.overrides['iou'] =0.45\nmodel.overrides['agnostic_nms'] =False\nmodel.overrides['max_det'] =1000\nimage ='https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\nresults = model.predict(image)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": [
                "yolov8x-visdrone"
            ],
            "pretrained_datasets": [
                "VisDrone"
            ],
            "performance_metrics": {
                "example_metrics_table": []
            }
        },
        "model_details": {
            "model_description": "Object detection model trained on the VisDrone dataset.",
            "supported_labels": [
                "pedestrian",
                "people",
                "bicycle",
                "car",
                "van",
                "truck",
                "tricycle",
                "awning-tricycle",
                "bus",
                "motor"
            ]
        },
        "limitations_and_biases": {
            "limitations": [],
            "biases": [],
            "risks": []
        },
        "recommendations": [],
        "compute_infrastructure": {
            "hardware": "Unknown",
            "software": "Unknown"
        },
        "contact_information": {
            "model_card_contact": "Not provided"
        },
        "references": {
            "related_papers_and_resources": []
        },
        "example_implementation": {
            "sample_code": "from ultralyticsplusimportYOLO, render_result\nmodel = YOLO('mshamrai/yolov8x-visdrone')\nmodel.overrides['conf'] =0.25\nmodel.overrides['iou'] =0.45\nmodel.overrides['agnostic_nms'] =False\nmodel.overrides['max_det'] =1000\nimage ='https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\nresults = model.predict(image)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()"
        }
    },
    "yolov7-lego": {
        "model_name": "yolov7-lego",
        "developed_by": "thedreamfactor",
        "model_type": "Object Detection",
        "licensing": "Not specified",
        "installation": {
            "python_version": "Not specified",
            "additional_libraries": "Not specified",
            "installation_command": "Not specified"
        },
        "usage": {
            "cli_example": "Not specified",
            "python_example": "Not specified"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": [
                "zero-shot-1000-single-class.pt"
            ],
            "pretrained_datasets": [
                "thedreamfactor/biggest-lego-dataset-600-parts"
            ],
            "performance_metrics": {
                "example_metrics_table": []
            }
        },
        "model_details": {
            "model_description": "Model trained to detect Lego objects without differentiating specific Lego classes. Limited performance and prone to false positives on non-Lego objects.",
            "supported_labels": [
                "Lego"
            ]
        },
        "limitations_and_biases": {
            "limitations": [
                "Does not differentiate Lego classes.",
                "Limited dataset size (1000 images).",
                "High false positive rate on non-Lego objects."
            ],
            "biases": [],
            "risks": []
        },
        "recommendations": [],
        "compute_infrastructure": {
            "hardware": "Not specified",
            "software": "Not specified"
        },
        "contact_information": {
            "model_card_contact": "Not specified"
        },
        "references": {
            "related_papers_and_resources": []
        },
        "example_implementation": {
            "sample_code": "Not specified"
        }
    },
    "table-detection-and-extraction": {
        "model_name": "YOLOv8s Table Detection and Extraction",
        "developed_by": "FODUU AI",
        "model_type": "Object Detection",
        "licensing": "Not specified",
        "installation": {
            "python_version": "Not specified",
            "additional_libraries": [
                "ultralyticsplus==0.0.28",
                "ultralytics==8.0.43"
            ],
            "installation_command": "pip install ultralyticsplus==0.0.28 ultralytics==8.0.43"
        },
        "usage": {
            "cli_example": "Not provided",
            "python_example": "from ultralyticsplusimport YOLO, render_result\n# load model\nmodel = YOLO('foduucom/table-detection-and-extraction')\n# set model parameters\nmodel.overrides['conf'] =0.25\n# NMS confidence threshold\nmodel.overrides['iou'] =0.45\n# NMS IoU threshold\nmodel.overrides['agnostic_nms'] =False\n# NMS class-agnostic\nmodel.overrides['max_det'] =1000\n# maximum number of detections per image\n# set image\nimage ='/path/to/your/document/images'\n# perform inferenceresults = model.predict(image)\n# observe results\nprint(results[0].boxes)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": [
                "YOLOv8s"
            ],
            "pretrained_datasets": [
                "Diverse dataset containing images of tables from various sources"
            ],
            "performance_metrics": {
                "example_metrics_table": [
                    {
                        "model": "YOLOv8s",
                        "size_pixels": "Not specified",
                        "map_val50_95": "0.962",
                        "speed_cpu_onnx_ms": "Not specified",
                        "speed_a100_tensorrt_ms": "Not specified",
                        "params_m": "Not specified",
                        "flops_b": "Not specified"
                    }
                ]
            }
        },
        "model_details": {
            "model_description": "The YOLOv8s Table Detection and Extraction model is an object detection model based on the YOLO framework. It is designed to detect tables, whether they are bordered or borderless, in images. The model has been fine-tuned on a vast dataset and achieved high accuracy in detecting tables and distinguishing between bordered and borderless ones. It plays a crucial role in addressing the complexities of unstructured documents by employing advanced techniques such as bounding box delineation and seamless integration with Optical Character Recognition (OCR) technology.",
            "supported_labels": [
                "bordered",
                "borderless"
            ]
        },
        "limitations_and_biases": {
            "limitations": [
                "Performance may vary based on the quality, diversity, and representativeness of the training data.",
                "The model may face challenges in detecting tables with intricate designs or complex arrangements.",
                "Accuracy may be affected by variations in lighting conditions, image quality, and resolution.",
                "Detection of very small or distant tables might be less accurate.",
                "The model's ability to classify bordered and borderless tables may be influenced by variations in design."
            ],
            "biases": [
                "Not specified"
            ],
            "risks": [
                "Not specified"
            ]
        },
        "recommendations": [
            "Users should be informed about the model's limitations and potential biases.",
            "Further testing and validation are advised for specific use cases to evaluate its performance accurately."
        ],
        "compute_infrastructure": {
            "hardware": "NVIDIA GeForce RTX 3060 card",
            "software": "Jupyter Notebook environment"
        },
        "contact_information": {
            "model_card_contact": "info@foduu.com"
        },
        "references": {
            "related_papers_and_resources": [
                "Not specified"
            ]
        },
        "example_implementation": {
            "sample_code": "Not provided"
        }
    },
    "human-detector": {
        "model_name": "Human Detector",
        "developed_by": "Monet-Joe",
        "model_type": "Age and Gender Recognition",
        "licensing": "Not specified",
        "installation": {
            "python_version": "Not specified",
            "additional_libraries": "Not specified",
            "installation_command": "Not specified"
        },
        "usage": {
            "cli_example": "Not provided",
            "python_example": "Not provided"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": [],
            "pretrained_datasets": [],
            "performance_metrics": {}
        },
        "model_details": {
            "model_description": "MiVOLO (Multi-Input VOLO) is a simple approach to age and gender estimation utilizing the state-of-the-art Vision Transformer. It integrates these two tasks into a unified two-input/output model that utilizes not only facial information but also person image data. This improves the generalization ability of the model, allowing it to provide satisfactory results even when faces are not visible in the image.",
            "supported_labels": [
                "Age",
                "Gender"
            ]
        },
        "limitations_and_biases": {
            "limitations": [],
            "biases": [],
            "risks": []
        },
        "recommendations": [],
        "compute_infrastructure": {
            "hardware": "Not specified",
            "software": "Not specified"
        },
        "contact_information": {
            "model_card_contact": "Not provided"
        },
        "references": {
            "related_papers_and_resources": [
                "https://github.com/WildChlamydia/MiVOLO"
            ]
        },
        "example_implementation": {
            "sample_code": "Not provided"
        }
    }
}