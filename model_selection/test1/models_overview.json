{
    "detr-resnet-50": {
        "Model Overview": "DETR (End-to-End Object Detection) model with ResNet-50 backboneDEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paperEnd-to-End Object Detection with Transformersby Carion et al. and first released inthis repository.Disclaimer: The team releasing DETR did not write a model card for this model so this model card has been written by the Hugging Face team.Model descriptionThe DETR model is an encoder-decoder transformer with a convolutional backbone. Two heads are added on top of the decoder outputs in order to perform object detection: a linear layer for the class labels and a MLP (multi-layer perceptron) for the bounding boxes. The model uses so-called object queries to detect objects in an image. Each object query looks for a particular object in the image. For COCO, the number of object queries is set to 100.The model is trained using a \"bipartite matching loss\": one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a \"no object\" as class and \"no bounding box\" as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.Intended uses & limitationsYou can use the raw model for object detection. See themodel hubto look for all available DETR models.How to useHere is how to use this model:fromtransformersimportDetrImageProcessor, DetrForObjectDetectionimporttorchfromPILimportImageimportrequests\n\nurl =\"http://images.cocodataset.org/val2017/000000039769.jpg\"image = Image.open(requests.get(url, stream=True).raw)# you can specify the revision tag if you don't want the timm dependencyprocessor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")\nmodel = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")\n\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)# convert outputs (bounding boxes and class logits) to COCO API# let's only keep detections with score > 0.9target_sizes = torch.tensor([image.size[::-1]])\nresults = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]forscore, label, boxinzip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n    box = [round(i,2)foriinbox.tolist()]print(f\"Detected{model.config.id2label[label.item()]}with confidence \"f\"{round(score.item(),3)}at location{box}\")This should output:Detected remote with confidence 0.998 at location [40.16, 70.81, 175.55, 117.98]\nDetected remote with confidence 0.996 at location [333.24, 72.55, 368.33, 187.66]\nDetected couch with confidence 0.995 at location [-0.02, 1.15, 639.73, 473.76]\nDetected cat with confidence 0.999 at location [13.24, 52.05, 314.02, 470.93]\nDetected cat with confidence 0.999 at location [345.4, 23.85, 640.37, 368.72]Currently, both the feature extractor and model support PyTorch.Training dataThe DETR model was trained onCOCO 2017 object detection, a dataset consisting of 118k/5k annotated images for training/validation respectively.Training procedurePreprocessingThe exact details of preprocessing of images during training/validation can be foundhere.Images are resized/rescaled such that the shortest side is at least 800 pixels and the largest side at most 1333 pixels, and normalized across the RGB channels with the ImageNet mean (0.485, 0.456, 0.406) and standard deviation (0.229, 0.224, 0.225).TrainingThe model was trained for 300 epochs on 16 V100 GPUs. This takes 3 days, with 4 images per GPU (hence a total batch size of 64).Evaluation resultsThis model achieves an AP (average precision) of42.0on COCO 2017 validation. For more details regarding evaluation results, we refer to table 1 of the original paper.BibTeX entry and citation info@article{DBLP:journals/corr/abs-2005-12872,\n  author    = {Nicolas Carion and\n               Francisco Massa and\n               Gabriel Synnaeve and\n               Nicolas Usunier and\n               Alexander Kirillov and\n               Sergey Zagoruyko},\n  title     = {End-to-End Object Detection with Transformers},\n  journal   = {CoRR},\n  volume    = {abs/2005.12872},\n  year      = {2020},\n  url       = {https://arxiv.org/abs/2005.12872},\n  archivePrefix = {arXiv},\n  eprint    = {2005.12872},\n  timestamp = {Thu, 28 May 2020 17:38:09 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2005-12872.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}"
    },
    "YOLOv8": {
        "Model Overview": "\u4e2d\u6587|\ud55c\uad6d\uc5b4|\u65e5\u672c\u8a9e|\u0420\u0443\u0441\u0441\u043a\u0438\u0439|Deutsch|Fran\u00e7ais|Espa\u00f1ol|Portugu\u00eas|\u0939\u093f\u0928\u094d\u0926\u0940|\u0627\u0644\u0639\u0631\u0628\u064a\u0629UltralyticsYOLOv8is a cutting-edge, state-of-the-art (SOTA) model that builds upon the success of previous YOLO versions and introduces new features and improvements to further boost performance and flexibility. YOLOv8 is designed to be fast, accurate, and easy to use, making it an excellent choice for a wide range of object detection and tracking, instance segmentation, image classification and pose estimation tasks.We hope that the resources here will help you get the most out of YOLOv8. Please browse the YOLOv8Docsfor details, raise an issue onGitHubfor support, and join ourDiscordcommunity for questions and discussions!To request an Enterprise License please complete the form atUltralytics Licensing.DocumentationSee below for a quickstart installation and usage example, and see theYOLOv8 Docsfor full documentation on training, validation, prediction and deployment.InstallPip install the ultralytics package including allrequirementsin aPython>=3.8environment withPyTorch>=1.8.pip install ultralyticsFor alternative installation methods includingConda,Docker, and Git, please refer to theQuickstart Guide.UsageCLIYOLOv8 may be used directly in the Command Line Interface (CLI) with ayolocommand:yolo predict model=yolov8n.ptsource='https://ultralytics.com/images/bus.jpg'yolocan be used for a variety of tasks and modes and accepts additional arguments, i.e.imgsz=640. See the YOLOv8CLI Docsfor examples.PythonYOLOv8 may also be used directly in a Python environment, and accepts the sameargumentsas in the CLI example above:fromultralyticsimportYOLO# Load a modelmodel = YOLO(\"yolov8n.yaml\")# build a new model from scratchmodel = YOLO(\"yolov8n.pt\")# load a pretrained model (recommended for training)# Use the modelmodel.train(data=\"coco128.yaml\", epochs=3)# train the modelmetrics = model.val()# evaluate model performance on the validation setresults = model(\"https://ultralytics.com/images/bus.jpg\")# predict on an imagepath = model.export(format=\"onnx\")# export the model to ONNX formatSee YOLOv8Python Docsfor more examples.NotebooksUltralytics provides interactive notebooks for YOLOv8, covering training, validation, tracking, and more. Each notebook is paired with aYouTubetutorial, making it easy to learn and implement advanced YOLOv8 features.DocsNotebookYouTubeYOLOv8 Train, Val, Predict and Export ModesUltralytics HUB QuickStartYOLOv8 Multi-Object Tracking in VideosYOLOv8 Object Counting in VideosYOLOv8 Heatmaps in VideosUltralytics Datasets Explorer with SQL and OpenAI Integration \ud83d\ude80 NewModelsYOLOv8Detect,SegmentandPosemodels pretrained on theCOCOdataset are available here, as well as YOLOv8Classifymodels pretrained on theImageNetdataset.Trackmode is available for all Detect, Segment and Pose models.AllModelsdownload automatically from the latest Ultralyticsreleaseon first use.Detection (COCO)SeeDetection Docsfor usage examples with these models trained onCOCO, which include 80 pre-trained classes.Modelsize(pixels)mAPval50-95SpeedCPU ONNX(ms)SpeedA100 TensorRT(ms)params(M)FLOPs(B)YOLOv8n64037.380.40.993.28.7YOLOv8s64044.9128.41.2011.228.6YOLOv8m64050.2234.71.8325.978.9YOLOv8l64052.9375.22.3943.7165.2YOLOv8x64053.9479.13.5368.2257.8mAPvalvalues are for single-model single-scale onCOCO val2017dataset.Reproduce byyolo val detect data=coco.yaml device=0Speedaveraged over COCO val images using anAmazon EC2 P4dinstance.Reproduce byyolo val detect data=coco.yaml batch=1 device=0|cpuDetection (Open Image V7)SeeDetection Docsfor usage examples with these models trained onOpen Image V7, which include 600 pre-trained classes.Modelsize(pixels)mAPval50-95SpeedCPU ONNX(ms)SpeedA100 TensorRT(ms)params(M)FLOPs(B)YOLOv8n64018.4142.41.213.510.5YOLOv8s64027.7183.11.4011.429.7YOLOv8m64033.6408.52.2626.280.6YOLOv8l64034.9596.92.4344.1167.4YOLOv8x64036.3860.63.5668.7260.6mAPvalvalues are for single-model single-scale onOpen Image V7dataset.Reproduce byyolo val detect data=open-images-v7.yaml device=0Speedaveraged over Open Image V7 val images using anAmazon EC2 P4dinstance.Reproduce byyolo val detect data=open-images-v7.yaml batch=1 device=0|cpuSegmentation (COCO)SeeSegmentation Docsfor usage examples with these models trained onCOCO-Seg, which include 80 pre-trained classes.Modelsize(pixels)mAPbox50-95mAPmask50-95SpeedCPU ONNX(ms)SpeedA100 TensorRT(ms)params(M)FLOPs(B)YOLOv8n-seg64036.730.596.11.213.412.6YOLOv8s-seg64044.636.8155.71.4711.842.6YOLOv8m-seg64049.940.8317.02.1827.3110.2YOLOv8l-seg64052.342.6572.42.7946.0220.5YOLOv8x-seg64053.443.4712.14.0271.8344.1mAPvalvalues are for single-model single-scale onCOCO val2017dataset.Reproduce byyolo val segment data=coco-seg.yaml device=0Speedaveraged over COCO val images using anAmazon EC2 P4dinstance.Reproduce byyolo val segment data=coco-seg.yaml batch=1 device=0|cpuPose (COCO)SeePose Docsfor usage examples with these models trained onCOCO-Pose, which include 1 pre-trained class, person.Modelsize(pixels)mAPpose50-95mAPpose50SpeedCPU ONNX(ms)SpeedA100 TensorRT(ms)params(M)FLOPs(B)YOLOv8n-pose64050.480.1131.81.183.39.2YOLOv8s-pose64060.086.2233.21.4211.630.2YOLOv8m-pose64065.088.8456.32.0026.481.0YOLOv8l-pose64067.690.0784.52.5944.4168.6YOLOv8x-pose64069.290.21607.13.7369.4263.2YOLOv8x-pose-p6128071.691.24088.710.0499.11066.4mAPvalvalues are for single-model single-scale onCOCO Keypoints val2017dataset.Reproduce byyolo val pose data=coco-pose.yaml device=0Speedaveraged over COCO val images using anAmazon EC2 P4dinstance.Reproduce byyolo val pose data=coco-pose.yaml batch=1 device=0|cpuOBB (DOTAv1)SeeOBB Docsfor usage examples with these models trained onDOTAv1, which include 15 pre-trained classes.Modelsize(pixels)mAPtest50SpeedCPU ONNX(ms)SpeedA100 TensorRT(ms)params(M)FLOPs(B)YOLOv8n-obb102478.0204.773.573.123.3YOLOv8s-obb102479.5424.884.0711.476.3YOLOv8m-obb102480.5763.487.6126.4208.6YOLOv8l-obb102480.71278.4211.8344.5433.8YOLOv8x-obb102481.361759.1013.2369.5676.7mAPtestvalues are for single-model multi-scale onDOTAv1dataset.Reproduce byyolo val obb data=DOTAv1.yaml device=0 split=testand submit merged results toDOTA evaluation.Speedaveraged over DOTAv1 val images using anAmazon EC2 P4dinstance.Reproduce byyolo val obb data=DOTAv1.yaml batch=1 device=0|cpuClassification (ImageNet)SeeClassification Docsfor usage examples with these models trained onImageNet, which include 1000 pretrained classes.Modelsize(pixels)acctop1acctop5SpeedCPU ONNX(ms)SpeedA100 TensorRT(ms)params(M)FLOPs(B) at 640YOLOv8n-cls22469.088.312.90.312.74.3YOLOv8s-cls22473.891.723.40.356.413.5YOLOv8m-cls22476.893.585.40.6217.042.7YOLOv8l-cls22476.893.5163.00.8737.599.7YOLOv8x-cls22479.094.6232.01.0157.4154.8accvalues are model accuracies on theImageNetdataset validation set.Reproduce byyolo val classify data=path/to/ImageNet device=0Speedaveraged over ImageNet val images using anAmazon EC2 P4dinstance.Reproduce byyolo val classify data=path/to/ImageNet batch=1 device=0|cpuIntegrationsOur key integrations with leading AI platforms extend the functionality of Ultralytics' offerings, enhancing tasks like dataset labeling, training, visualization, and model management. Discover how Ultralytics, in collaboration withRoboflow, ClearML,Comet, Neural Magic andOpenVINO, can optimize your AI workflow.RoboflowClearML \u2b50 NEWComet \u2b50 NEWNeural Magic \u2b50 NEWLabel and export your custom datasets directly to YOLOv8 for training withRoboflowAutomatically track, visualize and even remotely train YOLOv8 usingClearML(open-source!)Free forever,Cometlets you save YOLOv8 models, resume training, and interactively visualize and debug predictionsRun YOLOv8 inference up to 6x faster withNeural Magic DeepSparseUltralytics HUBExperience seamless AI withUltralytics HUB\u2b50, the all-in-one solution for data visualization, YOLOv5 and YOLOv8 \ud83d\ude80 model training and deployment, without any coding. Transform images into actionable insights and bring your AI visions to life with ease using our cutting-edge platform and user-friendlyUltralytics App. Start your journey forFreenow!ContributeWe love your input! YOLOv5 and YOLOv8 would not be possible without help from our community. Please see ourContributing Guideto get started, and fill out ourSurveyto send us feedback on your experience. Thank you \ud83d\ude4f to all our contributors!LicenseUltralytics offers two licensing options to accommodate diverse use cases:AGPL-3.0 License: ThisOSI-approvedopen-source license is ideal for students and enthusiasts, promoting open collaboration and knowledge sharing. See theLICENSEfile for more details.Enterprise License: Designed for commercial use, this license permits seamless integration of Ultralytics software and AI models into commercial goods and services, bypassing the open-source requirements of AGPL-3.0. If your scenario involves embedding our solutions into a commercial offering, reach out throughUltralytics Licensing.ContactFor Ultralytics bug reports and feature requests please visitGitHub Issues, and join ourDiscordcommunity for questions and discussions!"
    },
    "rtdetr_r50vd": {
        "Model Overview": "Model Card for RT-DETRTable of ContentsModel DetailsModel SourcesHow to Get Started with the ModelTraining DetailsEvaluationModel Architecture and ObjectiveCitationModel DetailsThe YOLO series has become the most popular framework for real-time object detection due to its reasonable trade-off between speed and accuracy. \nHowever, we observe that the speed and accuracy of YOLOs are negatively affected by the NMS. \nRecently, end-to-end Transformer-based detectors (DETRs) have provided an alternative to eliminating NMS. \nNevertheless, the high computational cost limits their practicality and hinders them from fully exploiting the advantage of excluding NMS. \nIn this paper, we propose the Real-Time DEtection TRansformer (RT-DETR), the first real-time end-to-end object detector to our best knowledge that addresses the above dilemma. \nWe build RT-DETR in two steps, drawing on the advanced DETR: \nfirst we focus on maintaining accuracy while improving speed, followed by maintaining speed while improving accuracy. \nSpecifically, we design an efficient hybrid encoder to expeditiously process multi-scale features by decoupling intra-scale interaction and cross-scale fusion to improve speed. \nThen, we propose the uncertainty-minimal query selection to provide high-quality initial queries to the decoder, thereby improving accuracy. \nIn addition, RT-DETR supports flexible speed tuning by adjusting the number of decoder layers to adapt to various scenarios without retraining. \nOur RT-DETR-R50 / R101 achieves 53.1% / 54.3% AP on COCO and 108 / 74 FPS on T4 GPU, outperforming previously advanced YOLOs in both speed and accuracy. \nWe also develop scaled RT-DETRs that outperform the lighter YOLO detectors (S and M models). \nFurthermore, RT-DETR-R50 outperforms DINO-R50 by 2.2% AP in accuracy and about 21 times in FPS. \nAfter pre-training with Objects365, RT-DETR-R50 / R101 achieves 55.3% / 56.2% AP. The project page: thishttps URL.This is the model card of a \ud83e\udd17transformersmodel that has been pushed on the Hub.Developed by:Yian Zhao and Sangbum ChoiFunded by:National Key R&D Program of China (No.2022ZD0118201), Natural Science Foundation of China (No.61972217, 32071459, 62176249, 62006133, 62271465),\nand the Shenzhen Medical Research Funds in China (No.\nB2302037).Shared by:Sangbum ChoiModel type:RT-DETRLicense:Apache-2.0Model SourcesHF Docs:RT-DETRRepository:https://github.com/lyuwenyu/RT-DETRPaper:https://arxiv.org/abs/2304.08069Demo:RT-DETR TrackingHow to Get Started with the ModelUse the code below to get started with the model.importtorchimportrequestsfromPILimportImagefromtransformersimportRTDetrForObjectDetection, RTDetrImageProcessor\n\nurl ='http://images.cocodataset.org/val2017/000000039769.jpg'image = Image.open(requests.get(url, stream=True).raw)\n\nimage_processor = RTDetrImageProcessor.from_pretrained(\"PekingU/rtdetr_r50vd\")\nmodel = RTDetrForObjectDetection.from_pretrained(\"PekingU/rtdetr_r50vd\")\n\ninputs = image_processor(images=image, return_tensors=\"pt\")withtorch.no_grad():\n    outputs = model(**inputs)\n\nresults = image_processor.post_process_object_detection(outputs, target_sizes=torch.tensor([image.size[::-1]]), threshold=0.3)forresultinresults:forscore, label_id, boxinzip(result[\"scores\"], result[\"labels\"], result[\"boxes\"]):\n        score, label = score.item(), label_id.item()\n        box = [round(i,2)foriinbox.tolist()]print(f\"{model.config.id2label[label]}:{score:.2f}{box}\")This should outputsofa: 0.97 [0.14, 0.38, 640.13, 476.21]\ncat: 0.96 [343.38, 24.28, 640.14, 371.5]\ncat: 0.96 [13.23, 54.18, 318.98, 472.22]\nremote: 0.95 [40.11, 73.44, 175.96, 118.48]\nremote: 0.92 [333.73, 76.58, 369.97, 186.99]Training DetailsTraining DataThe RTDETR model was trained onCOCO 2017 object detection, a dataset consisting of 118k/5k annotated images for training/validation respectively.Training ProcedureWe conduct experiments on COCO and Objects365 datasets, where RT-DETR is trained on COCO train2017 and validated on COCO val2017 dataset. \nWe report the standard COCO metrics, including AP (averaged over uniformly sampled IoU thresholds ranging from 0.50-0.95 with a step size of 0.05), \nAP50, AP75, as well as AP at different scales: APS, APM, APL.PreprocessingImages are resized to 640x640 pixels and rescaled withimage_mean=[0.485, 0.456, 0.406]andimage_std=[0.229, 0.224, 0.225].Training HyperparametersTraining regime:EvaluationModel#Epochs#Params (M)GFLOPsFPS_bs=1AP (val)AP50 (val)AP75 (val)AP-s (val)AP-m (val)AP-l (val)RT-DETR-R18722060.721746.563.850.428.449.863.0RT-DETR-R34723191.017248.566.252.330.251.966.2RT-DETR R50724213610853.171.357.734.858.070.0RT-DETR R10172762597454.372.758.636.058.872.1RT-DETR-R18 (Objects 365 pretrained)60206121749.266.653.533.252.364.8RT-DETR-R50 (Objects 365 pretrained)244213610855.373.460.137.959.971.8RT-DETR-R101 (Objects 365 pretrained)24762597456.274.661.338.360.573.5Model Architecture and ObjectiveOverview of RT-DETR. We feed the features from the last three stages of the backbone into the encoder. The efficient hybrid\nencoder transforms multi-scale features into a sequence of image features through the Attention-based Intra-scale Feature Interaction (AIFI)\nand the CNN-based Cross-scale Feature Fusion (CCFF). Then, the uncertainty-minimal query selection selects a fixed number of encoder\nfeatures to serve as initial object queries for the decoder. Finally, the decoder with auxiliary prediction heads iteratively optimizes object\nqueries to generate categories and boxes.CitationBibTeX:@misc{lv2023detrs,\n      title={DETRs Beat YOLOs on Real-time Object Detection},\n      author={Yian Zhao and Wenyu Lv and Shangliang Xu and Jinman Wei and Guanzhong Wang and Qingqing Dang and Yi Liu and Jie Chen},\n      year={2023},\n      eprint={2304.08069},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}Model Card AuthorsSangbum ChoiPavel Iakubovskii"
    },
    "yolos-tiny": {
        "Model Overview": "YOLOS (tiny-sized) modelYOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paperYou Only Look at One Sequence: Rethinking Transformer in Vision through Object Detectionby Fang et al. and first released inthis repository.Disclaimer: The team releasing YOLOS did not write a model card for this model so this model card has been written by the Hugging Face team.Model descriptionYOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN).The model is trained using a \"bipartite matching loss\": one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a \"no object\" as class and \"no bounding box\" as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.Intended uses & limitationsYou can use the raw model for object detection. See themodel hubto look for all available YOLOS models.How to useHere is how to use this model:fromtransformersimportYolosImageProcessor, YolosForObjectDetectionfromPILimportImageimporttorchimportrequests\n\nurl =\"http://images.cocodataset.org/val2017/000000039769.jpg\"image = Image.open(requests.get(url, stream=True).raw)\n\nmodel = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\nimage_processor = YolosImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n\ninputs = image_processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)# model predicts bounding boxes and corresponding COCO classeslogits = outputs.logits\nbboxes = outputs.pred_boxes# print resultstarget_sizes = torch.tensor([image.size[::-1]])\nresults = image_processor.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)[0]forscore, label, boxinzip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n    box = [round(i,2)foriinbox.tolist()]print(f\"Detected{model.config.id2label[label.item()]}with confidence \"f\"{round(score.item(),3)}at location{box}\")Currently, both the feature extractor and model support PyTorch.Training dataThe YOLOS model was pre-trained onImageNet-1kand fine-tuned onCOCO 2017 object detection, a dataset consisting of 118k/5k annotated images for training/validation respectively.TrainingThe model was pre-trained for 300 epochs on ImageNet-1k and fine-tuned for 300 epochs on COCO.Evaluation resultsThis model achieves an AP (average precision) of28.7on COCO 2017 validation. For more details regarding evaluation results, we refer to the original paper.BibTeX entry and citation info@article{DBLP:journals/corr/abs-2106-00666,\n  author    = {Yuxin Fang and\n               Bencheng Liao and\n               Xinggang Wang and\n               Jiemin Fang and\n               Jiyang Qi and\n               Rui Wu and\n               Jianwei Niu and\n               Wenyu Liu},\n  title     = {You Only Look at One Sequence: Rethinking Transformer in Vision through\n               Object Detection},\n  journal   = {CoRR},\n  volume    = {abs/2106.00666},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2106.00666},\n  eprinttype = {arXiv},\n  eprint    = {2106.00666},\n  timestamp = {Fri, 29 Apr 2022 19:49:16 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2106-00666.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}"
    },
    "stockmarket-future-prediction": {
        "Model Overview": "# Model Card for YOLOv8s Stock Market future trends prediction on Live Trading Video DataModel SummaryThe YOLOv8s Stock Market future trends prediction model is an object detection model based on the YOLO (You Only Look Once) framework. It is designed to detect various chart patterns in real-time stock market trading video data. The model aids traders and investors by automating the analysis of chart patterns, providing timely insights for informed decision-making. The model has been fine-tuned on a diverse dataset and achieved high accuracy in detecting and classifying stock market future trend detection in live trading scenarios.Model DetailsModel DescriptionThe YOLOv8s Stock Market future trends prediction model offers a transformative solution for traders and investors by enabling real-time detection of crucial chart patterns within live trading video data. As stock markets evolve rapidly, this model's capabilities empower users with timely insights, allowing them to make informed decisions with speed and accuracy.The model seamlessly integrates into live trading systems, providing instant trends prediction and classification. By leveraging advanced bounding box techniques and pattern-specific feature extraction, the model excels in identifying patterns such as 'Down','Up'. This enables traders to optimize their strategies, automate trading decisions, and respond to market trends in real-time.To facilitate integration into live trading systems or to inquire about customization, please contact us atinfo@foduu.com. Your collaboration and feedback are instrumental in refining and enhancing the model's performance in dynamic trading environments.Developed by:FODUU AIModel type:Object DetectionTask:Stock Market future trends prediction on Live Trading Video DataThe YOLOv8s Stock Market Pattern Detection model is designed to adapt to the fast-paced nature of live trading environments. Its ability to operate on real-time video data allows traders and investors to harness pattern-based insights without delay.Supported Labels['Down','Up']UsesDirect UseThe YOLOv8s Stock Market future trends prediction model can be directly integrated into live trading systems to provide real-time detection and classification of chart patterns or classify the upcoming trends. Traders can utilize the model's insights for timely decision-making.Downstream UseThe model's real-time capabilities can be leveraged to automate trading strategies, generate alerts for specific patterns or trends, and enhance overall trading performance.Out-of-Scope UseThe model is not designed for unrelated object detection tasks or scenarios outside the scope of stock market trends prediction in live trading video data.Bias, Risks, and LimitationsThe YOLOv8s Stock Market future prediction model may exhibit some limitations and biases:Performance may be affected by variations in video quality, lighting conditions, and pattern complexity within live trading data.Rapid market fluctuations and noise in video data may impact the model's accuracy and responsiveness.Market-specific patterns or anomalies not well-represented in the training data may pose challenges for detection.RecommendationsUsers should be aware of the model's limitations and potential biases. Thorough testing and validation within live trading simulations are advised before deploying the model in real trading environments.How to Get Started with the ModelTo begin using the YOLOv8s Stock Market future prediction model on live trading video data, follow these steps:pip install ultralyticsplus==0.0.28 ultralytics==8.0.43Load model and perform real-time prediction:fromultralyticsplusimportYOLO, render_resultimportcv2# load modelmodel = YOLO('foduucom/stockmarket-future-prediction')# set model parametersmodel.overrides['conf'] =0.25# NMS confidence thresholdmodel.overrides['iou'] =0.45# NMS IoU thresholdmodel.overrides['agnostic_nms'] =False# NMS class-agnosticmodel.overrides['max_det'] =1000# maximum number of detections per image# set imageimage ='/path/to/your/document/images'# perform inferenceresults = model.predict(image)# observe resultsprint(results[0].boxes)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()Training DetailsTraining DataThe model is trained on a diverse dataset containing stock market chart images with various chart patterns, capturing different market conditions and scenarios.Training ProcedureThe training process involves extensive computation and is conducted over multiple epochs. The model's weights are adjusted to minimize detection loss and optimize performance for stock market pattern detection.MetricsmAP@0.5(box): 0.65All patterns: 0.90Individual patterns: Varies based on pattern typeModel Architecture and ObjectiveThe YOLOv8s architecture incorporates modifications tailored to stock market future prediction. It features a specialized backbone network, self-attention mechanisms, and trends-specific feature extraction modules.Compute InfrastructureHardwareNVIDIA GeForce RTX 3080 cardSoftwareThe model was trained and fine-tuned using a Jupyter Notebook environment.Model Card ContactFor inquiries and contributions, please contact us atinfo@foduu.com.@ModelCard{\n  author    = {Nehul Agrawal and\n               Rahul parihar},\n  title     = {YOLOv8s Stock Market future prediction on Live Trading Video Data},\n  year      = {2023}\n}"
    },
    "table-transformer-detection": {
        "Model Overview": "Table Transformer (fine-tuned for Table Detection)Table Transformer (DETR) model trained on PubTables1M. It was introduced in the paperPubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documentsby Smock et al. and first released inthis repository.Disclaimer: The team releasing Table Transformer did not write a model card for this model so this model card has been written by the Hugging Face team.Model descriptionThe Table Transformer is equivalent toDETR, a Transformer-based object detection model. Note that the authors decided to use the \"normalize before\" setting of DETR, which means that layernorm is applied before self- and cross-attention.UsageYou can use the raw model for detecting tables in documents. See thedocumentationfor more info."
    },
    "web-form-ui-field-detection": {
        "Model Overview": "Model architectureModel OverviewThe web-form-Detect model is a yolov8 object detection model trained to detect and locate ui form fields in images. It is built upon the ultralytics library and fine-tuned using a dataset of annotated ui form images.Intended UseThe model is intended to be used for detecting details like Name,number,email,password,button,redio bullet and so on fields in images. It can be incorporated into applications that require automated detection ui form fields from images.PerformanceThe model has been evaluated on a held-out test dataset and achieved the following performance metrics:Average Precision (AP): 0.51\nPrecision: 0.80\nRecall: 0.70\nF1 Score: 0.71\nPlease note that the actual performance may vary based on the input data distribution and quality.How to Get Started with the ModelTo get started with the YOLOv8s object Detection model use for web ui detection, follow these steps:pip install ultralyticsplus==0.0.28 ultralytics==8.0.43Load model and perform prediction:fromultralyticsplusimportYOLO, render_result# load modelmodel = YOLO('foduucom/web-form-ui-field-detection')# set model parametersmodel.overrides['conf'] =0.25# NMS confidence thresholdmodel.overrides['iou'] =0.45# NMS IoU thresholdmodel.overrides['agnostic_nms'] =False# NMS class-agnosticmodel.overrides['max_det'] =1000# maximum number of detections per image# set imageimage ='/path/to/your/document/images'# perform inferenceresults = model.predict(image)# observe resultsprint(results[0].boxes)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()Training DataThe model was trained on a diverse dataset containing images of web ui form data from different sources, resolutions, and lighting conditions. The dataset was annotated with bounding box coordinates to indicate the location of the ui form fields within the image.Total Number of Images: 600\nAnnotation Format: Bounding box coordinates (xmin, ymin, xmax, ymax)Fine-tuning ProcessPretrained Model: TheError: Errors in your YAML metadata model was initialized with a pretrained object detection backbone (e.g. YOLO).Loss Function: Mean Average Precision (mAP) loss was used for optimization during training.Optimizer: Adam optimizer with a learning rate of 1e-4.Batch Size:-1Training Time: 1 hours on a single NVIDIA GeForce RTX 3090 GPU.Model LimitationsThe model's performance is subject to variations in image quality, lighting conditions, and image resolutions.\nThe model may struggle with detecting web ui form in cases of extreme occlusion.\nThe model may not generalize well to non-standard ui form formats or variations.SoftwareThe model was trained and fine-tuned using a Jupyter Notebook environment.Model Card ContactFor inquiries and contributions, please contact us atinfo@foduu.com.@ModelCard{\n  author    = {Nehul Agrawal and\n               Rahul parihar},\n  title     = {YOLOv8s web-form ui fields detection},\n  year      = {2023}\n}"
    },
    "rtdetr_r50vd_coco_o365": {
        "Model Overview": "Model Card for RT-DETRTable of ContentsModel DetailsModel SourcesHow to Get Started with the ModelTraining DetailsEvaluationModel Architecture and ObjectiveCitationModel DetailsThe YOLO series has become the most popular framework for real-time object detection due to its reasonable trade-off between speed and accuracy. \nHowever, we observe that the speed and accuracy of YOLOs are negatively affected by the NMS. \nRecently, end-to-end Transformer-based detectors (DETRs) have provided an alternative to eliminating NMS. \nNevertheless, the high computational cost limits their practicality and hinders them from fully exploiting the advantage of excluding NMS. \nIn this paper, we propose the Real-Time DEtection TRansformer (RT-DETR), the first real-time end-to-end object detector to our best knowledge that addresses the above dilemma. \nWe build RT-DETR in two steps, drawing on the advanced DETR: \nfirst we focus on maintaining accuracy while improving speed, followed by maintaining speed while improving accuracy. \nSpecifically, we design an efficient hybrid encoder to expeditiously process multi-scale features by decoupling intra-scale interaction and cross-scale fusion to improve speed. \nThen, we propose the uncertainty-minimal query selection to provide high-quality initial queries to the decoder, thereby improving accuracy. \nIn addition, RT-DETR supports flexible speed tuning by adjusting the number of decoder layers to adapt to various scenarios without retraining. \nOur RT-DETR-R50 / R101 achieves 53.1% / 54.3% AP on COCO and 108 / 74 FPS on T4 GPU, outperforming previously advanced YOLOs in both speed and accuracy. \nWe also develop scaled RT-DETRs that outperform the lighter YOLO detectors (S and M models). \nFurthermore, RT-DETR-R50 outperforms DINO-R50 by 2.2% AP in accuracy and about 21 times in FPS. \nAfter pre-training with Objects365, RT-DETR-R50 / R101 achieves 55.3% / 56.2% AP. The project page: thishttps URL.This is the model card of a \ud83e\udd17transformersmodel that has been pushed on the Hub.Developed by:Yian Zhao and Sangbum ChoiFunded by:National Key R&D Program of China (No.2022ZD0118201), Natural Science Foundation of China (No.61972217, 32071459, 62176249, 62006133, 62271465),\nand the Shenzhen Medical Research Funds in China (No.\nB2302037).Shared by:Sangbum ChoiModel type:RT-DETRLicense:Apache-2.0Model SourcesHF Docs:RT-DETRRepository:https://github.com/lyuwenyu/RT-DETRPaper:https://arxiv.org/abs/2304.08069Demo:RT-DETR TrackingHow to Get Started with the ModelUse the code below to get started with the model.importtorchimportrequestsfromPILimportImagefromtransformersimportRTDetrForObjectDetection, RTDetrImageProcessor\n\nurl ='http://images.cocodataset.org/val2017/000000039769.jpg'image = Image.open(requests.get(url, stream=True).raw)\n\nimage_processor = RTDetrImageProcessor.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\")\nmodel = RTDetrForObjectDetection.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\")\n\ninputs = image_processor(images=image, return_tensors=\"pt\")withtorch.no_grad():\n    outputs = model(**inputs)\n\nresults = image_processor.post_process_object_detection(outputs, target_sizes=torch.tensor([image.size[::-1]]), threshold=0.3)forresultinresults:forscore, label_id, boxinzip(result[\"scores\"], result[\"labels\"], result[\"boxes\"]):\n        score, label = score.item(), label_id.item()\n        box = [round(i,2)foriinbox.tolist()]print(f\"{model.config.id2label[label]}:{score:.2f}{box}\")This should outputsofa: 0.97 [0.14, 0.38, 640.13, 476.21]\ncat: 0.96 [343.38, 24.28, 640.14, 371.5]\ncat: 0.96 [13.23, 54.18, 318.98, 472.22]\nremote: 0.95 [40.11, 73.44, 175.96, 118.48]\nremote: 0.92 [333.73, 76.58, 369.97, 186.99]Training DetailsTraining DataThe RTDETR model was trained onCOCO 2017 object detection, a dataset consisting of 118k/5k annotated images for training/validation respectively.Training ProcedureWe conduct experiments on COCO and Objects365 datasets, where RT-DETR is trained on COCO train2017 and validated on COCO val2017 dataset. \nWe report the standard COCO metrics, including AP (averaged over uniformly sampled IoU thresholds ranging from 0.50-0.95 with a step size of 0.05), \nAP50, AP75, as well as AP at different scales: APS, APM, APL.PreprocessingImages are resized to 640x640 pixels and rescaled withimage_mean=[0.485, 0.456, 0.406]andimage_std=[0.229, 0.224, 0.225].Training HyperparametersTraining regime:EvaluationModel#Epochs#Params (M)GFLOPsFPS_bs=1AP (val)AP50 (val)AP75 (val)AP-s (val)AP-m (val)AP-l (val)RT-DETR-R18722060.721746.563.850.428.449.863.0RT-DETR-R34723191.017248.566.252.330.251.966.2RT-DETR R50724213610853.171.357.734.858.070.0RT-DETR R10172762597454.372.758.636.058.872.1RT-DETR-R18 (Objects 365 pretrained)60206121749.266.653.533.252.364.8RT-DETR-R50 (Objects 365 pretrained)244213610855.373.460.137.959.971.8RT-DETR-R101 (Objects 365 pretrained)24762597456.274.661.338.360.573.5Model Architecture and ObjectiveOverview of RT-DETR. We feed the features from the last three stages of the backbone into the encoder. The efficient hybrid\nencoder transforms multi-scale features into a sequence of image features through the Attention-based Intra-scale Feature Interaction (AIFI)\nand the CNN-based Cross-scale Feature Fusion (CCFF). Then, the uncertainty-minimal query selection selects a fixed number of encoder\nfeatures to serve as initial object queries for the decoder. Finally, the decoder with auxiliary prediction heads iteratively optimizes object\nqueries to generate categories and boxes.CitationBibTeX:@misc{lv2023detrs,\n      title={DETRs Beat YOLOs on Real-time Object Detection},\n      author={Yian Zhao and Wenyu Lv and Shangliang Xu and Jinman Wei and Guanzhong Wang and Qingqing Dang and Yi Liu and Jie Chen},\n      year={2023},\n      eprint={2304.08069},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}Model Card AuthorsSangbum ChoiPavel Iakubovskii"
    },
    "yolov10x": {
        "Model Overview": "Model DescriptionYOLOv10: Real-Time End-to-End Object DetectionarXiv:https://arxiv.org/abs/2405.14458v1github:https://github.com/THU-MIG/yolov10Installationpip install git+https://github.com/THU-MIG/yolov10.gitTraining and validationfromultralyticsimportYOLOv10\n\nmodel = YOLOv10.from_pretrained('jameslahm/yolov10x')# Trainingmodel.train(...)# after training, one can push to the hubmodel.push_to_hub(\"your-hf-username/yolov10-finetuned\")# Validationmodel.val(...)InferenceHere's an end-to-end example showcasing inference on a cats image:fromultralyticsimportYOLOv10\n\nmodel = YOLOv10.from_pretrained('jameslahm/yolov10x')\nsource ='http://images.cocodataset.org/val2017/000000039769.jpg'model.predict(source=source, save=True)which shows:BibTeX Entry and Citation Info@article{wang2024yolov10,\n title={YOLOv10: Real-Time End-to-End Object Detection},\n author={Wang, Ao and Chen, Hui and Liu, Lihao and Chen, Kai and Lin, Zijia and Han, Jungong and Ding, Guiguang},\n journal={arXiv preprint arXiv:2405.14458},\n year={2024}\n}"
    },
    "YOLOv10-Document-Layout-Analysis": {
        "Model Overview": "\ud83e\udd17 Live Demo here:https://huggingface.co/spaces/omoured/YOLOv10-Document-Layout-AnalysisAbout \ud83d\udccbThe models were fine-tuned using 4xA100 GPUs on the Doclaynet-base dataset, which consists of 6910 training images, 648 validation images, and 499 test images.Results \ud83d\udccaModelmAP50mAP50-95Model WeightsYOLOv10-x0.9240.740DownloadYOLOv10-b0.9220.732DownloadYOLOv10-l0.9210.732DownloadYOLOv10-m0.9170.737DownloadYOLOv10-s0.9050.713DownloadYOLOv10-n0.8920.685DownloadCodes \ud83d\udd25Check out our Github repo for inference codes:https://github.com/moured/YOLOv10-Document-Layout-AnalysisReferences \ud83d\udcddYOLOv10BibTeX\n@article{wang2024yolov10,\n  title={YOLOv10: Real-Time End-to-End Object Detection},\n  author={Wang, Ao and Chen, Hui and Liu, Lihao and Chen, Kai and Lin, Zijia and Han, Jungong and Ding, Guiguang},\n  journal={arXiv preprint arXiv:2405.14458},\n  year={2024}\n}DocLayNet@article{doclaynet2022,\n  title = {DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis},  \n  doi = {10.1145/3534678.353904},\n  url = {https://arxiv.org/abs/2206.01062},\n  author = {Pfitzmann, Birgit and Auer, Christoph and Dolfi, Michele and Nassar, Ahmed S and Staar, Peter W J},\n  year = {2022}\n}ContactLinkedIn:https://www.linkedin.com/in/omar-moured/"
    },
    "Object-Detection-RetinaNet": {
        "Model Overview": "Model descriptionImplementing RetinaNet: Focal Loss for Dense Object Detection.This repo contains the model for the notebookObject Detection with RetinaNetHere the model is tasked with localizing the objects present in an image, and at the same time, classifying them into different categories. In this, RetinaNet has been implemented, a popularsingle-stage detector, which is accurate and runs fast. RetinaNet uses afeature pyramid networkto efficiently detect objects at multiple scales and introduces a new loss, theFocal loss function, to alleviate the problem of the extreme foreground-background class imbalance.Full credits go toSrihari HumbarwadiReferencesRetinaNet PaperFeature Pyramid Network PaperTraining and evaluation dataThe dataset used here is aCOCO2017 datasetTraining procedureTraining hyperparametersThe following hyperparameters were used during training:namelearning_ratedecaymomentumnesterovtraining_precisionSGD{'class_name': 'PiecewiseConstantDecay', 'config': {'boundaries': [125, 250, 500, 240000, 360000], 'values': [2.5e-06, 0.000625, 0.00125, 0.0025, 0.00025, 2.5e-05], 'name': None}}0.00.8999999761581421Falsefloat32Model PlotView Model PlotModel Reproduced ByKavya Bisht"
    },
    "yolos-small-finetuned-license-plate-detection": {
        "Model Overview": "YOLOS (small-sized) modelThis model is a fine-tuned version ofhustvl/yolos-smallon thelicesne-plate-recognitiondataset from Roboflow which contains 5200 images in the training set and 380 in the validation set.\nThe original YOLOS model was fine-tuned on COCO 2017 object detection (118k annotated images).Model descriptionYOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN).Intended uses & limitationsYou can use the raw model for object detection. See themodel hubto look for all available YOLOS models.How to useHere is how to use this model:fromtransformersimportYolosFeatureExtractor, YolosForObjectDetectionfromPILimportImageimportrequests\n\nurl ='https://drive.google.com/uc?id=1p9wJIqRz3W50e2f_A0D8ftla8hoXz4T5'image = Image.open(requests.get(url, stream=True).raw)\nfeature_extractor = YolosFeatureExtractor.from_pretrained('nickmuchi/yolos-small-finetuned-license-plate-detection')\nmodel = YolosForObjectDetection.from_pretrained('nickmuchi/yolos-small-finetuned-license-plate-detection')\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)# model predicts bounding boxes and corresponding face mask detection classeslogits = outputs.logits\nbboxes = outputs.pred_boxesCurrently, both the feature extractor and model support PyTorch.Training dataThe YOLOS model was pre-trained onImageNet-1kand fine-tuned onCOCO 2017 object detection, a dataset consisting of 118k/5k annotated images for training/validation respectively.TrainingThis model was fine-tuned for 200 epochs on thelicesne-plate-recognition.Evaluation resultsThis model achieves an AP (average precision) of49.0.Accumulating evaluation results...IoU metric: bboxMetricsMetric ParameterLocationDetsValueAverage Precision(AP) @[ IoU=0.50:0.95area=   allmaxDets=100 ]0.490Average Precision(AP) @[ IoU=0.50area=   allmaxDets=100 ]0.792Average Precision(AP) @[ IoU=0.75area=   allmaxDets=100 ]0.585Average Precision(AP) @[ IoU=0.50:0.95area= smallmaxDets=100 ]0.167Average Precision(AP) @[ IoU=0.50:0.95area=mediummaxDets=100 ]0.460Average Precision(AP) @[ IoU=0.50:0.95area= largemaxDets=100 ]0.824Average Recall(AR) @[ IoU=0.50:0.95area=   allmaxDets=  1 ]0.447Average Recall(AR) @[ IoU=0.50:0.95area=   allmaxDets= 10 ]0.671Average Recall(AR) @[ IoU=0.50:0.95area=   allmaxDets=100 ]0.676Average Recall(AR) @[ IoU=0.50:0.95area= smallmaxDets=100 ]0.278Average Recall(AR) @[ IoU=0.50:0.95area=mediummaxDets=100 ]0.641Average Recall(AR) @[ IoU=0.50:0.95area= largemaxDets=100 ]0.890"
    },
    "table-transformer-structure-recognition": {
        "Model Overview": "Table Transformer (fine-tuned for Table Structure Recognition)Table Transformer (DETR) model trained on PubTables1M. It was introduced in the paperPubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documentsby Smock et al. and first released inthis repository.Disclaimer: The team releasing Table Transformer did not write a model card for this model so this model card has been written by the Hugging Face team.Model descriptionThe Table Transformer is equivalent toDETR, a Transformer-based object detection model. Note that the authors decided to use the \"normalize before\" setting of DETR, which means that layernorm is applied before self- and cross-attention.UsageYou can use the raw model for detecting the structure (like rows, columns) in tables. See thedocumentationfor more info."
    },
    "yolos-fashionpedia": {
        "Model Overview": "This is a fine-tunned object detection model for fashion.For more details of the implementation you can check the source codeherethe dataset used for its training is availableherethis model supports the following categories:CATS = ['shirt, blouse', 'top, t-shirt, sweatshirt', 'sweater', 'cardigan', 'jacket', 'vest', 'pants', 'shorts', 'skirt', 'coat', 'dress', 'jumpsuit', 'cape', 'glasses', 'hat', 'headband, head covering, hair accessory', 'tie', 'glove', 'watch', 'belt', 'leg warmer', 'tights, stockings', 'sock', 'shoe', 'bag, wallet', 'scarf', 'umbrella', 'hood', 'collar', 'lapel', 'epaulette', 'sleeve', 'pocket', 'neckline', 'buckle', 'zipper', 'applique', 'bead', 'bow', 'flower', 'fringe', 'ribbon', 'rivet', 'ruffle', 'sequin', 'tassel']"
    },
    "mmdet-yolox-tiny": {
        "Model Overview": "Model DescriptionYOLOX: Exceeding YOLO Series in 2021SAHI: Slicing Aided Hyper Inference and Fine-tuning for Small Object DetectionImproved anchor-free YOLO architecture for object detection task.DocumentsGitHub RepoPaper - YOLOX: Exceeding YOLO Series in 2021DatasetsThe YOLOX model was pre-trained onImageNet-1kand fine-tuned onCOCO 2017 object detection, a dataset consisting of 118k/5k annotated images for training/validation respectively.How to useInstallsahiandmmdet:pip install -U sahi\npip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.11.0/index.html\npip install mmdet==2.26.0Load model and perform prediction:fromsahiimportAutoDetectionModelfromsahi.utils.fileimportdownload_from_urlfromsahi.predictimportget_predictionfromsahi.cvimportread_image_as_pil\n\nMMDET_YOLOX_TINY_MODEL_URL =\"https://huggingface.co/fcakyon/mmdet-yolox-tiny/resolve/main/yolox_tiny_8x8_300e_coco_20211124_171234-b4047906.pth\"MMDET_YOLOX_TINY_MODEL_PATH =\"yolox.pt\"MMDET_YOLOX_TINY_CONFIG_URL =\"https://huggingface.co/fcakyon/mmdet-yolox-tiny/raw/main/yolox_tiny_8x8_300e_coco.py\"MMDET_YOLOX_TINY_CONFIG_PATH =\"config.py\"IMAGE_URL =\"https://user-images.githubusercontent.com/34196005/142730935-2ace3999-a47b-49bb-83e0-2bdd509f1c90.jpg\"# download weight and configdownload_from_url(\n  MMDET_YOLOX_TINY_MODEL_URL,\n  MMDET_YOLOX_TINY_MODEL_PATH,\n)\ndownload_from_url(\n  MMDET_YOLOX_TINY_CONFIG_URL,\n  MMDET_YOLOX_TINY_CONFIG_PATH,\n)# create modeldetection_model = AutoDetectionModel.from_pretrained(\n    model_type='mmdet',\n    model_path=MMDET_YOLOX_TINY_MODEL_PATH,\n    config_path=MMDET_YOLOX_TINY_CONFIG_PATH,\n    confidence_threshold=0.5,\n    device=\"cuda:0\",# or 'cpu')# prepare input imageimage = read_image_as_pil(IMAGE_URL)# perform predictionprediction_result = get_prediction(\n  image=image,\n  detection_model=detection_model\n)# visualize predictionsprediction_result.export_predictions(export_dir='results/')# get predictionsprediction_result.object_prediction_listMore info atdemo notebook.BibTeX Entry and Citation Info@article{akyon2022sahi,\n  title={Slicing Aided Hyper Inference and Fine-tuning for Small Object Detection},\n  author={Akyon, Fatih Cagatay and Altinuc, Sinan Onur and Temizel, Alptekin},\n  journal={2022 IEEE International Conference on Image Processing (ICIP)},\n  doi={10.1109/ICIP46576.2022.9897990},\n  pages={966-970},\n  year={2022}\n}@article{yolox2021,\n  title={{YOLOX}: Exceeding YOLO Series in 2021},\n  author={Ge, Zheng and Liu, Songtao and Wang, Feng and Li, Zeming and Sun, Jian},\n  journal={arXiv preprint arXiv:2107.08430},\n  year={2021}\n}"
    },
    "deta-resnet-50": {
        "Model Overview": "Detection Transformers with AssignmentByJeffrey Ouyang-Zhang,Jang Hyun Cho,Xingyi Zhou,Philipp Kr\u00e4henb\u00fchlFrom the paperNMS Strikes Back.TL; DR.DetectionTransformers withAssignment (DETA) re-introduce IoU assignment and NMS for transformer-based detectors. DETA trains and tests comparibly as fast as Deformable-DETR and converges much faster (50.2 mAP in 12 epochs on COCO)."
    },
    "yolov8s": {
        "Model Overview": "Supported Labels['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']How to useInstallultralyticsplus:pip install -U ultralyticsplus==0.0.14Load model and perform prediction:fromultralyticsplusimportYOLO, render_result# load modelmodel = YOLO('ultralyticsplus/yolov8s')# set model parametersmodel.overrides['conf'] =0.25# NMS confidence thresholdmodel.overrides['iou'] =0.45# NMS IoU thresholdmodel.overrides['agnostic_nms'] =False# NMS class-agnosticmodel.overrides['max_det'] =1000# maximum number of detections per image# set imageimage ='https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'# perform inferenceresults = model.predict(image)# observe resultsprint(results[0].boxes)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()"
    },
    "yolov8s-plane-detection": {
        "Model Overview": "Supported Labels['planes']How to useInstallultralyticsplus:pip install ultralyticsplus==0.0.23 ultralytics==8.0.21Load model and perform prediction:fromultralyticsplusimportYOLO, render_result# load modelmodel = YOLO('keremberke/yolov8s-plane-detection')# set model parametersmodel.overrides['conf'] =0.25# NMS confidence thresholdmodel.overrides['iou'] =0.45# NMS IoU thresholdmodel.overrides['agnostic_nms'] =False# NMS class-agnosticmodel.overrides['max_det'] =1000# maximum number of detections per image# set imageimage ='https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'# perform inferenceresults = model.predict(image)# observe resultsprint(results[0].boxes)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()More models available at:awesome-yolov8-models"
    },
    "yolov8m-nlf-head-detection": {
        "Model Overview": "Supported Labels['Helmet', 'Helmet-Blurred', 'Helmet-Difficult', 'Helmet-Partial', 'Helmet-Sideline']How to useInstallultralyticsplus:pip install ultralyticsplus==0.0.24 ultralytics==8.0.23Load model and perform prediction:fromultralyticsplusimportYOLO, render_result# load modelmodel = YOLO('keremberke/yolov8m-nlf-head-detection')# set model parametersmodel.overrides['conf'] =0.25# NMS confidence thresholdmodel.overrides['iou'] =0.45# NMS IoU thresholdmodel.overrides['agnostic_nms'] =False# NMS class-agnosticmodel.overrides['max_det'] =1000# maximum number of detections per image# set imageimage ='https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'# perform inferenceresults = model.predict(image)# observe resultsprint(results[0].boxes)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()More models available at:awesome-yolov8-models"
    },
    "deta-swin-large": {
        "Model Overview": "Detection Transformers with AssignmentByJeffrey Ouyang-Zhang,Jang Hyun Cho,Xingyi Zhou,Philipp Kr\u00e4henb\u00fchlFrom the paperNMS Strikes Back.TL; DR.DetectionTransformers withAssignment (DETA) re-introduce IoU assignment and NMS for transformer-based detectors. DETA trains and tests comparibly as fast as Deformable-DETR and converges much faster (50.2 mAP in 12 epochs on COCO)."
    },
    "fashionfail": {
        "Model Overview": "Facere*The models proposed in the paper\"FashionFail: Addressing Failure Cases in Fashion Object Detection and Segmentation\"[paper][project page]:facere_base.onnx: A pre-trained Mask R-CNN fine-tuned onFashionpedia-train.facere_plus.onnx:facere_basemodel further fine-tuned onFashionFail-train.* Facere (fa:chere) is a Latin word for 'to make', from which the word fashion is derived.[source]Usagefromtorchvision.ioimportread_imagefromtorchvision.models.detectionimportMaskRCNN_ResNet50_FPN_Weightsfromhuggingface_hubimporthf_hub_download\n\npath_onnx = hf_hub_download(\n    repo_id=\"rizavelioglu/fashionfail\",\n    filename=\"facere_base.onnx\",# or \"facere_plus.onnx\")# Load pre-trained model transformations.weights = MaskRCNN_ResNet50_FPN_Weights.DEFAULT\ntransforms = weights.transforms()# Load image and apply original transformation to the image.img = read_image(\"path/to/image\")\nimg_transformed = transforms(img)# Create an inference session.ort_session = onnxruntime.InferenceSession(\n    path_onnx, providers=[\"CUDAExecutionProvider\",\"CPUExecutionProvider\"]\n)# Run inference on the input.ort_inputs = {\n    ort_session.get_inputs()[0].name: img_transformed.unsqueeze(dim=0).numpy()\n}\nort_outs = ort_session.run(None, ort_inputs)# Parse the model output.boxes, labels, scores, masks = ort_outsCheck out the demo code onHuggingFace Spacesfor visualizing the output.Also, check outFashionFail's GitHub repositoryto get more information on\ntraining, inference, and evaluation.LicenseTL;DR: Not available for commercial use, unless the FULL source code is shared!This project is intended solely for academic research. No commercial benefits are derived from it.\nModels are licensed underServer Side Public License (SSPL)CitationIf you find this repository useful in your research, please consider giving a star \u2b50 and a citation:@inproceedings{velioglu2024fashionfail,\n  author    = {Velioglu, Riza and Chan, Robin and Hammer, Barbara},\n  title     = {FashionFail: Addressing Failure Cases in Fashion Object Detection and Segmentation},\n  journal   = {IJCNN},\n  eprint    = {2404.08582},\n  year      = {2024},\n}"
    },
    "screenlist-slicing": {
        "Model Overview": "N/A"
    },
    "yolov8x-visdrone": {
        "Model Overview": "Supported Labels['pedestrian', 'people', 'bicycle', 'car', 'van', 'truck', 'tricycle', 'awning-tricycle', 'bus', 'motor']How to useInstallultralyticsplus:pip install ultralyticsplus==0.0.28 ultralytics==8.0.43Load model and perform prediction:fromultralyticsplusimportYOLO, render_result# load modelmodel = YOLO('mshamrai/yolov8x-visdrone')# set model parametersmodel.overrides['conf'] =0.25# NMS confidence thresholdmodel.overrides['iou'] =0.45# NMS IoU thresholdmodel.overrides['agnostic_nms'] =False# NMS class-agnosticmodel.overrides['max_det'] =1000# maximum number of detections per image# set imageimage ='https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'# perform inferenceresults = model.predict(image)# observe resultsprint(results[0].boxes)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()"
    },
    "yolov7-lego": {
        "Model Overview": "OverviewThe model(s) in this repository are trained with thedreamfactor/biggest-lego-dataset-600-partsfrom Kaggle and theYolov7training script.LimitationsThezero-shot-1000-single-class.ptwas trained in thetraining-zero-shot-1000-single-class.ipynbnotebook with 1000 images and does not differentiate lego classes but only tries to predict Lego objects.\nThis can be easily reconfigured and retrained in the notebook, but the current implementation leads to many false positives on non-Lego objects and therefore can be improved\nupon. Also, it could be worth investigating if the metrics improve with a bigger training dataset, as currently only 1000 images are being used (approx. 0.6% of the full \ndataset)."
    },
    "table-detection-and-extraction": {
        "Model Overview": "Model Card for YOLOv8s Table DetectionModel SummaryThe YOLOv8s Table Detection model is an object detection model based on the YOLO (You Only Look Once) framework. It is designed to detect tables, whether they are bordered or borderless, in images. The model has been fine-tuned on a vast dataset and achieved high accuracy in detecting tables and distinguishing between bordered and borderless ones.Model DetailsModel DescriptionThe YOLOv8s Table Detection model serves as a versatile solution for precisely identifying tables within images, whether they exhibit a bordered or borderless design. Notably, this model's capabilities extend beyond mere detection \u2013 it plays a crucial role in addressing the complexities of unstructured documents. By employing advanced techniques such as bounding box delineation, the model enables users to isolate tables of interest within the visual content.What sets this model apart is its synergy with Optical Character Recognition (OCR) technology. This seamless integration empowers the model to not only locate tables but also to extract pertinent data contained within. The bounding box information guides the cropping of tables, which is then coupled with OCR to meticulously extract textual data, streamlining the process of information retrieval from unstructured documents.We invite you to explore the potential of this model and its data extraction capabilities. For those interested in harnessing its power or seeking further collaboration, we encourage you to reach out to us atinfo@foduu.com. Whether you require assistance, customization, or have innovative ideas, our collaborative approach is geared towards addressing your unique challenges. Additionally, you can actively engage with our vibrant community section for valuable insights and collective problem-solving. Your input drives our continuous improvement, as we collectively pave the way towards enhanced data extraction and document analysis.Developed by:FODUU AIModel type:Object DetectionTask:Table Detection (Bordered and Borderless)Furthermore, the YOLOv8s Table Detection model is not limited to table detection alone. It is a versatile tool that contributes to the processing of unstructured documents. By utilizing advanced bounding box techniques, the model empowers users to isolate tables within the document's visual content. What sets this model apart is its seamless integration with Optical Character Recognition (OCR) technology. The combination of bounding box information and OCR allows for precise data extraction from the tables. This comprehensive approach streamlines the process of information retrieval from complex documents.User collaboration is actively encouraged to enrich the model's capabilities. By contributing table images of different designs and types, users play a pivotal role in enhancing the model's ability to detect a diverse range of tables accurately. Community participation can be facilitated through our platform or by reaching out to us atinfo@foduu.com. We value collaborative efforts that drive continuous improvement and innovation in table detection and extraction.Supported Labels['bordered', 'borderless']UsesDirect UseThe YOLOv8s Table Detection model can be directly used for detecting tables in images, whether they are bordered or borderless. It is equipped with the ability to distinguish between these two categories.Downstream UseThe model can also be fine-tuned for specific table detection tasks or integrated into larger applications for furniture recognition, interior design, image-based data extraction, and other related fields.Out-of-Scope UseThe model is not designed for unrelated object detection tasks or scenarios outside the scope of table detection.Bias, Risks, and LimitationsThe YOLOv8s Table Detection model may have some limitations and biases:Performance may vary based on the quality, diversity, and representativeness of the training data.The model may face challenges in detecting tables with intricate designs or complex arrangements.Accuracy may be affected by variations in lighting conditions, image quality, and resolution.Detection of very small or distant tables might be less accurate.The model's ability to classify bordered and borderless tables may be influenced by variations in design.RecommendationsUsers should be informed about the model's limitations and potential biases. Further testing and validation are advised for specific use cases to evaluate its performance accurately.How to Get Started with the ModelTo begin using the YOLOv8s Table Detection model, follow these steps:pip install ultralyticsplus==0.0.28 ultralytics==8.0.43Load model and perform prediction:fromultralyticsplusimportYOLO, render_result# load modelmodel = YOLO('foduucom/table-detection-and-extraction')# set model parametersmodel.overrides['conf'] =0.25# NMS confidence thresholdmodel.overrides['iou'] =0.45# NMS IoU thresholdmodel.overrides['agnostic_nms'] =False# NMS class-agnosticmodel.overrides['max_det'] =1000# maximum number of detections per image# set imageimage ='/path/to/your/document/images'# perform inferenceresults = model.predict(image)# observe resultsprint(results[0].boxes)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()Training DetailsTraining DataThe model is trained on a diverse dataset containing images of tables from various sources. The dataset includes examples of both bordered and borderless tables, capturing different designs and styles.Training ProcedureThe training process involves extensive computation and is conducted over multiple epochs. The model's weights are adjusted to minimize detection loss and optimize performance.MetricsmAP@0.5(box):All: 0.962Bordered: 0.961Borderless: 0.963Model Architecture and ObjectiveThe YOLOv8s architecture employs a modified CSPDarknet53 as its backbone, along with self-attention mechanisms and feature pyramid networks. These components contribute to the model's ability to detect and classify tables accurately, considering variations in size, design, and style.Compute InfrastructureHardwareNVIDIA GeForce RTX 3060 cardSoftwareThe model was trained and fine-tuned using a Jupyter Notebook environment.Model Card ContactFor inquiries and contributions, please contact us atinfo@foduu.com.@ModelCard{\n  author    = {Nehul Agrawal and\n               Pranjal Singh Thakur},\n  title     = {YOLOv8s Table Detection},\n  year      = {2023}\n}"
    },
    "human-detector": {
        "Model Overview": "Human DetectorAge and gender recognition in the field is a challenging task: in addition to variable environmental conditions, pose complexity, and differences in image quality, there is also partial or complete occlusion of the face.MiVOLO (Multi-Input VOLO) is a simple approach to age and gender estimation utilizing the state-of-the-art Vision Transformer. The method integrates these two tasks into a unified two-input/output model that utilizes not only facial information but also person image data. This improves the generalization ability of the model, allowing it to provide satisfactory results even when faces are not visible in the image. To evaluate the model, experiments were conducted on four popular benchmark datasets and state-of-the-art performance was achieved while demonstrating the ability to process in real-time. In addition, a new benchmark dataset was introduced based on images from the Open Images dataset. The ground truth annotations of this benchmark dataset are carefully generated by human annotators and high accuracy is guaranteed by intelligently aggregating the voting results. In addition, the age recognition performance of the model is compared to human-level accuracy and demonstrated to significantly outperform humans in most age ranges. Finally, access to the model was provided to the public, along with the code used for validation and inference. Additional annotations are also provided for the datasets used, and new benchmark datasets are presented.MaintenanceGIT_LFS_SKIP_SMUDGE=1 gitclonegit@hf.co:monet-joe/human-detectorMirrorhttps://www.modelscope.cn/models/monetjoe/human-detectorReference[1]https://github.com/WildChlamydia/MiVOLO"
    }
}