{
    "detr-resnet-50": {
        "model_name": "detr-resnet-50",
        "developed_by": "Hugging Face Team (based on the original work by Nicolas Carion et al.)",
        "model_type": "End-to-End Object Detection",
        "licensing": "Unknown",
        "installation": {
            "python_version": "Python 3.x",
            "additional_libraries": "torch, transformers, PIL, requests",
            "installation_command": "pip install torch transformers pillow requests"
        },
        "usage": {
            "cli_example": "Not Provided",
            "python_example": "from transformers import DetrImageProcessor, DetrForObjectDetection\nimport torch\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\nprocessor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")\nmodel = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")\n\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\ntarget_sizes = torch.tensor([image.size[::-1]])\nresults = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\nfor score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n    box = [round(i, 2) for i in box.tolist()]\n    print(f\"Detected {model.config.id2label[label.item()]} with confidence {round(score.item(), 3)} at location {box}\")"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": "facebook/detr-resnet-50",
            "pretrained_datasets": "COCO 2017 object detection",
            "performance_metrics": {
                "model": "DETR",
                "size_pixels": "Images resized/rescaled such that the shortest side is at least 800 pixels and the largest side at most 1333 pixels",
                "map_val50_95": "42.0",
                "speed_cpu_onnx_ms": "Not Provided",
                "speed_a100_tensorrt_ms": "Not Provided",
                "params_m": "Not Provided",
                "flops_b": "Not Provided"
            }
        },
        "model_details": {
            "model_description": "DETR (End-to-End Object Detection) model with ResNet-50 backbone. The DETR model is an encoder-decoder transformer with a convolutional backbone. Two heads are added on top of the decoder outputs to perform object detection: a linear layer for the class labels and a MLP (multi-layer perceptron) for the bounding boxes. The model uses object queries to detect objects in an image. For COCO, the number of object queries is set to 100. The model is trained using a bipartite matching loss.",
            "supported_labels": "80 object classes from COCO dataset"
        },
        "limitations_and_biases": {
            "limitations": "The model may not perform well on datasets or tasks different from the COCO dataset.",
            "biases": "Potential biases inherent in the COCO dataset.",
            "risks": "Misidentification of objects, especially in images that are significantly different from the COCO dataset."
        },
        "recommendations": "Recommended to use for object detection tasks similar to COCO 2017 dataset.",
        "compute_infrastructure": {
            "hardware": "16 V100 GPUs",
            "software": "PyTorch"
        },
        "contact_information": {
            "model_card_contact": "Hugging Face Team"
        },
        "references": {
            "related_papers_and_resources": "Nicolas Carion et al., End-to-End Object Detection with Transformers, 2020, https://arxiv.org/abs/2005.12872"
        },
        "example_implementation": {
            "sample_code": "from transformers import DetrImageProcessor, DetrForObjectDetection\nimport torch\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\nprocessor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")\nmodel = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")\n\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\ntarget_sizes = torch.tensor([image.size[::-1]])\nresults = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\nfor score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n    box = [round(i, 2) for i in box.tolist()]\n    print(f\"Detected {model.config.id2label[label.item()]} with confidence {round(score.item(), 3)} at location {box}\")"
        }
    },
    "YOLOv8": {
        "model_name": "YOLOv8",
        "developed_by": "Ultralytics",
        "model_type": "Object Detection, Tracking, Instance Segmentation, Image Classification, Pose Estimation",
        "licensing": {
            "open_source": "AGPL-3.0 License",
            "enterprise": "Enterprise License"
        },
        "installation": {
            "python_version": ">=3.8",
            "additional_libraries": "PyTorch>=1.8",
            "installation_command": "pip install ultralytics"
        },
        "usage": {
            "cli_example": "yolo predict model=yolov8n.pt source='https://ultralytics.com/images/bus.jpg'",
            "python_example": "from ultralytics import YOLO\n# Load a model\nmodel = YOLO('yolov8n.yaml')\n# build a new model from scratch\nmodel = YOLO('yolov8n.pt')\n# load a pretrained model (recommended for training)\n# Use the model\nmodel.train(data='coco128.yaml', epochs=3)\n# train the model\nmetrics = model.val()\n# evaluate model performance on the validation set\nresults = model('https://ultralytics.com/images/bus.jpg')\n# predict on an image\npath = model.export(format='onnx')\n# export the model to ONNX format"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": [
                "YOLOv8n",
                "YOLOv8s",
                "YOLOv8m",
                "YOLOv8l",
                "YOLOv8x",
                "YOLOv8n-seg",
                "YOLOv8s-seg",
                "YOLOv8m-seg",
                "YOLOv8l-seg",
                "YOLOv8x-seg",
                "YOLOv8n-pose",
                "YOLOv8s-pose",
                "YOLOv8m-pose",
                "YOLOv8l-pose",
                "YOLOv8x-pose",
                "YOLOv8x-pose-p6",
                "YOLOv8n-obb",
                "YOLOv8s-obb",
                "YOLOv8m-obb",
                "YOLOv8l-obb",
                "YOLOv8x-obb",
                "YOLOv8n-cls",
                "YOLOv8s-cls",
                "YOLOv8m-cls",
                "YOLOv8l-cls",
                "YOLOv8x-cls"
            ],
            "pretrained_datasets": [
                "COCO",
                "Open Image V7",
                "COCO-Seg",
                "COCO-Pose",
                "DOTAv1",
                "ImageNet"
            ],
            "performance_metrics": {
                "detection_coco": [
                    {
                        "model": "YOLOv8n",
                        "size_pixels": 640,
                        "map_val50_95": 37.3,
                        "speed_cpu_onnx_ms": 80.4,
                        "speed_a100_tensorrt_ms": 0.99,
                        "params_m": 3.2,
                        "flops_b": 8.7
                    },
                    {
                        "model": "YOLOv8s",
                        "size_pixels": 640,
                        "map_val50_95": 44.9,
                        "speed_cpu_onnx_ms": 128.4,
                        "speed_a100_tensorrt_ms": 1.2,
                        "params_m": 11.2,
                        "flops_b": 28.6
                    },
                    {
                        "model": "YOLOv8m",
                        "size_pixels": 640,
                        "map_val50_95": 50.2,
                        "speed_cpu_onnx_ms": 234.7,
                        "speed_a100_tensorrt_ms": 1.83,
                        "params_m": 25.9,
                        "flops_b": 78.9
                    },
                    {
                        "model": "YOLOv8l",
                        "size_pixels": 640,
                        "map_val50_95": 52.9,
                        "speed_cpu_onnx_ms": 375.2,
                        "speed_a100_tensorrt_ms": 2.39,
                        "params_m": 43.7,
                        "flops_b": 165.2
                    },
                    {
                        "model": "YOLOv8x",
                        "size_pixels": 640,
                        "map_val50_95": 53.9,
                        "speed_cpu_onnx_ms": 479.1,
                        "speed_a100_tensorrt_ms": 3.53,
                        "params_m": 68.2,
                        "flops_b": 257.8
                    }
                ],
                "detection_open_image_v7": [
                    {
                        "model": "YOLOv8n",
                        "size_pixels": 640,
                        "map_val50_95": 18.4,
                        "speed_cpu_onnx_ms": 142.4,
                        "speed_a100_tensorrt_ms": 1.21,
                        "params_m": 3.5,
                        "flops_b": 10.5
                    },
                    {
                        "model": "YOLOv8s",
                        "size_pixels": 640,
                        "map_val50_95": 27.7,
                        "speed_cpu_onnx_ms": 183.1,
                        "speed_a100_tensorrt_ms": 1.4,
                        "params_m": 11.4,
                        "flops_b": 29.7
                    },
                    {
                        "model": "YOLOv8m",
                        "size_pixels": 640,
                        "map_val50_95": 33.6,
                        "speed_cpu_onnx_ms": 408.5,
                        "speed_a100_tensorrt_ms": 2.26,
                        "params_m": 26.2,
                        "flops_b": 80.6
                    },
                    {
                        "model": "YOLOv8l",
                        "size_pixels": 640,
                        "map_val50_95": 34.9,
                        "speed_cpu_onnx_ms": 596.9,
                        "speed_a100_tensorrt_ms": 2.43,
                        "params_m": 44.1,
                        "flops_b": 167.4
                    },
                    {
                        "model": "YOLOv8x",
                        "size_pixels": 640,
                        "map_val50_95": 36.3,
                        "speed_cpu_onnx_ms": 860.6,
                        "speed_a100_tensorrt_ms": 3.56,
                        "params_m": 68.7,
                        "flops_b": 260.6
                    }
                ],
                "segmentation_coco": [
                    {
                        "model": "YOLOv8n-seg",
                        "size_pixels": 640,
                        "map_box50_95": 36.7,
                        "map_mask50_95": 30.5,
                        "speed_cpu_onnx_ms": 96.1,
                        "speed_a100_tensorrt_ms": 1.21,
                        "params_m": 3.4,
                        "flops_b": 12.6
                    },
                    {
                        "model": "YOLOv8s-seg",
                        "size_pixels": 640,
                        "map_box50_95": 44.6,
                        "map_mask50_95": 36.8,
                        "speed_cpu_onnx_ms": 155.7,
                        "speed_a100_tensorrt_ms": 1.47,
                        "params_m": 11.8,
                        "flops_b": 42.6
                    },
                    {
                        "model": "YOLOv8m-seg",
                        "size_pixels": 640,
                        "map_box50_95": 49.9,
                        "map_mask50_95": 40.8,
                        "speed_cpu_onnx_ms": 317.0,
                        "speed_a100_tensorrt_ms": 2.18,
                        "params_m": 27.3,
                        "flops_b": 110.2
                    },
                    {
                        "model": "YOLOv8l-seg",
                        "size_pixels": 640,
                        "map_box50_95": 52.3,
                        "map_mask50_95": 42.6,
                        "speed_cpu_onnx_ms": 572.4,
                        "speed_a100_tensorrt_ms": 2.79,
                        "params_m": 46.0,
                        "flops_b": 220.5
                    },
                    {
                        "model": "YOLOv8x-seg",
                        "size_pixels": 640,
                        "map_box50_95": 53.4,
                        "map_mask50_95": 43.4,
                        "speed_cpu_onnx_ms": 712.1,
                        "speed_a100_tensorrt_ms": 4.02,
                        "params_m": 71.8,
                        "flops_b": 344.1
                    }
                ],
                "pose_coco": [
                    {
                        "model": "YOLOv8n-pose",
                        "size_pixels": 640,
                        "map_pose50_95": 50.4,
                        "map_pose50": 80.1,
                        "speed_cpu_onnx_ms": 131.8,
                        "speed_a100_tensorrt_ms": 1.18,
                        "params_m": 3.3,
                        "flops_b": 9.2
                    },
                    {
                        "model": "YOLOv8s-pose",
                        "size_pixels": 640,
                        "map_pose50_95": 60.0,
                        "map_pose50": 86.2,
                        "speed_cpu_onnx_ms": 233.2,
                        "speed_a100_tensorrt_ms": 1.42,
                        "params_m": 11.6,
                        "flops_b": 30.2
                    },
                    {
                        "model": "YOLOv8m-pose",
                        "size_pixels": 640,
                        "map_pose50_95": 65.0,
                        "map_pose50": 88.8,
                        "speed_cpu_onnx_ms": 456.3,
                        "speed_a100_tensorrt_ms": 2.0,
                        "params_m": 26.4,
                        "flops_b": 81.0
                    },
                    {
                        "model": "YOLOv8l-pose",
                        "size_pixels": 640,
                        "map_pose50_95": 67.6,
                        "map_pose50": 90.0,
                        "speed_cpu_onnx_ms": 784.5,
                        "speed_a100_tensorrt_ms": 2.59,
                        "params_m": 44.4,
                        "flops_b": 168.6
                    },
                    {
                        "model": "YOLOv8x-pose",
                        "size_pixels": 640,
                        "map_pose50_95": 69.2,
                        "map_pose50": 90.2,
                        "speed_cpu_onnx_ms": 1607.1,
                        "speed_a100_tensorrt_ms": 3.73,
                        "params_m": 69.4,
                        "flops_b": 263.2
                    },
                    {
                        "model": "YOLOv8x-pose-p6",
                        "size_pixels": 1280,
                        "map_pose50_95": 71.6,
                        "map_pose50": 91.2,
                        "speed_cpu_onnx_ms": 4088.7,
                        "speed_a100_tensorrt_ms": 10.0,
                        "params_m": 499.1,
                        "flops_b": 1066.4
                    }
                ],
                "obb_dota_v1": [
                    {
                        "model": "YOLOv8n-obb",
                        "size_pixels": 1024,
                        "map_test50": 78.0,
                        "speed_cpu_onnx_ms": 204.7,
                        "speed_a100_tensorrt_ms": 3.57,
                        "params_m": 3.1,
                        "flops_b": 23.3
                    },
                    {
                        "model": "YOLOv8s-obb",
                        "size_pixels": 1024,
                        "map_test50": 79.5,
                        "speed_cpu_onnx_ms": 424.8,
                        "speed_a100_tensorrt_ms": 4.07,
                        "params_m": 11.4,
                        "flops_b": 76.3
                    },
                    {
                        "model": "YOLOv8m-obb",
                        "size_pixels": 1024,
                        "map_test50": 80.5,
                        "speed_cpu_onnx_ms": 763.4,
                        "speed_a100_tensorrt_ms": 7.61,
                        "params_m": 26.4,
                        "flops_b": 208.6
                    },
                    {
                        "model": "YOLOv8l-obb",
                        "size_pixels": 1024,
                        "map_test50": 80.7,
                        "speed_cpu_onnx_ms": 1278.4,
                        "speed_a100_tensorrt_ms": 11.83,
                        "params_m": 44.5,
                        "flops_b": 433.8
                    },
                    {
                        "model": "YOLOv8x-obb",
                        "size_pixels": 1024,
                        "map_test50": 81.3,
                        "speed_cpu_onnx_ms": 1759.1,
                        "speed_a100_tensorrt_ms": 13.23,
                        "params_m": 69.5,
                        "flops_b": 676.7
                    }
                ],
                "classification_imagenet": [
                    {
                        "model": "YOLOv8n-cls",
                        "size_pixels": 224,
                        "acc_top1": 69.0,
                        "acc_top5": 88.3,
                        "speed_cpu_onnx_ms": 12.9,
                        "speed_a100_tensorrt_ms": 0.31,
                        "params_m": 2.7,
                        "flops_b": 4.3
                    },
                    {
                        "model": "YOLOv8s-cls",
                        "size_pixels": 224,
                        "acc_top1": 73.8,
                        "acc_top5": 91.7,
                        "speed_cpu_onnx_ms": 23.4,
                        "speed_a100_tensorrt_ms": 0.35,
                        "params_m": 6.4,
                        "flops_b": 13.5
                    },
                    {
                        "model": "YOLOv8m-cls",
                        "size_pixels": 224,
                        "acc_top1": 76.8,
                        "acc_top5": 93.5,
                        "speed_cpu_onnx_ms": 85.4,
                        "speed_a100_tensorrt_ms": 0.62,
                        "params_m": 17.0,
                        "flops_b": 42.7
                    },
                    {
                        "model": "YOLOv8l-cls",
                        "size_pixels": 224,
                        "acc_top1": 76.8,
                        "acc_top5": 93.5,
                        "speed_cpu_onnx_ms": 163.0,
                        "speed_a100_tensorrt_ms": 0.87,
                        "params_m": 37.5,
                        "flops_b": 99.7
                    },
                    {
                        "model": "YOLOv8x-cls",
                        "size_pixels": 224,
                        "acc_top1": 79.0,
                        "acc_top5": 94.6,
                        "speed_cpu_onnx_ms": 232.0,
                        "speed_a100_tensorrt_ms": 1.01,
                        "params_m": 57.4,
                        "flops_b": 154.8
                    }
                ]
            }
        },
        "model_details": {
            "model_description": "Ultralytics YOLOv8 is a cutting-edge, state-of-the-art (SOTA) model that builds upon the success of previous YOLO versions and introduces new features and improvements to further boost performance and flexibility. YOLOv8 is designed to be fast, accurate, and easy to use, making it an excellent choice for a wide range of object detection and tracking, instance segmentation, image classification, and pose estimation tasks.",
            "supported_labels": "80 classes for COCO, 600 classes for Open Image V7, 1 class (person) for COCO-Pose, 15 classes for DOTAv1, 1000 classes for ImageNet"
        },
        "limitations_and_biases": {
            "limitations": "Might not perform well on datasets that differ significantly from the pretrained datasets.",
            "biases": "Biases present in the pretrained datasets (e.g., COCO, Open Image V7) might be reflected in the model's predictions.",
            "risks": "Inaccurate predictions can lead to incorrect decisions in critical applications."
        },
        "recommendations": "Thoroughly evaluate the model's performance on your specific dataset and task. Fine-tuning on a custom dataset is recommended for best results.",
        "compute_infrastructure": {
            "hardware": "Amazon EC2 P4d instance, A100 GPUs",
            "software": "Python, PyTorch"
        },
        "contact_information": {
            "model_card_contact": "GitHub Issues, Ultralytics Discord Community"
        },
        "references": {
            "related_papers_and_resources": "YOLOv8 Docs, YOLOv8 CLI Docs, YOLOv8 Python Docs, YOLOv8 Segmentation Docs, YOLOv8 Pose Docs, YOLOv8 OBB Docs, YOLOv8 Classification Docs"
        },
        "example_implementation": {
            "sample_code": "Ultralytics provides interactive notebooks for YOLOv8, covering training, validation, tracking, and more. Each notebook is paired with a YouTube tutorial, making it easy to learn and implement advanced YOLOv8 features."
        }
    },
    "yolos-tiny": {
        "model_name": "yolos-tiny",
        "developed_by": "Hugging Face team (original model by Fang et al.)",
        "model_type": "Vision Transformer (ViT) for Object Detection",
        "licensing": "Not specified",
        "installation": {
            "python_version": "Not specified",
            "additional_libraries": "transformers, PIL, torch, requests",
            "installation_command": "pip install transformers pillow torch requests"
        },
        "usage": {
            "cli_example": "Not provided",
            "python_example": "from transformers import YolosImageProcessor, YolosForObjectDetection\nfrom PIL import Image\nimport torch\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\nimage_processor = YolosImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n\ninputs = image_processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\n# model predicts bounding boxes and corresponding COCO classes\nlogits = outputs.logits\nbboxes = outputs.pred_boxes\n# print results\ntarget_sizes = torch.tensor([image.size[::-1]])\nresults = image_processor.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)[0]\nfor score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n    box = [round(i, 2) for i in box.tolist()]\n    print(f\"Detected {model.config.id2label[label.item()]} with confidence {round(score.item(), 3)} at location {box}\")"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": "YOLOS models",
            "pretrained_datasets": "ImageNet-1k, COCO 2017",
            "performance_metrics": {
                "model": "yolos-tiny",
                "size_pixels": "Not specified",
                "map_val50_95": "28.7",
                "speed_cpu_onnx_ms": "Not specified",
                "speed_a100_tensorrt_ms": "Not specified",
                "params_m": "Not specified",
                "flops_b": "Not specified"
            }
        },
        "model_details": {
            "model_description": "YOLOS (tiny-sized) model YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper 'You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection' by Fang et al. YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN). The model is trained using a \"bipartite matching loss\": one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N. The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.",
            "supported_labels": "COCO classes"
        },
        "limitations_and_biases": {
            "limitations": "Not specified",
            "biases": "Not specified",
            "risks": "Not specified"
        },
        "recommendations": "You can use the raw model for object detection.",
        "compute_infrastructure": {
            "hardware": "Not specified",
            "software": "PyTorch"
        },
        "contact_information": {
            "model_card_contact": "Hugging Face team"
        },
        "references": {
            "related_papers_and_resources": "Fang et al., 'You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection', 2021. URL: https://arxiv.org/abs/2106.00666"
        },
        "example_implementation": {
            "sample_code": "from transformers import YolosImageProcessor, YolosForObjectDetection\nfrom PIL import Image\nimport torch\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\nimage_processor = YolosImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n\ninputs = image_processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\n# model predicts bounding boxes and corresponding COCO classes\nlogits = outputs.logits\nbboxes = outputs.pred_boxes\n# print results\ntarget_sizes = torch.tensor([image.size[::-1]])\nresults = image_processor.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)[0]\nfor score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n    box = [round(i, 2) for i in box.tolist()]\n    print(f\"Detected {model.config.id2label[label.item()]} with confidence {round(score.item(), 3)} at location {box}\")"
        }
    },
    "stockmarket-future-prediction": {
        "model_name": "YOLOv8s Stock Market future prediction on Live Trading Video Data",
        "developed_by": "FODUU AI",
        "model_type": "Object Detection",
        "licensing": "N/A",
        "installation": {
            "python_version": "N/A",
            "additional_libraries": "ultralyticsplus==0.0.28, ultralytics==8.0.43",
            "installation_command": "pip install ultralyticsplus==0.0.28 ultralytics==8.0.43"
        },
        "usage": {
            "cli_example": "N/A",
            "python_example": "from ultralyticsplus import YOLO, render_result\nimport cv2\n\n# load model\nmodel = YOLO('foduucom/stockmarket-future-prediction')\n\n# set model parameters\nmodel.overrides['conf'] = 0.25  # NMS confidence threshold\nmodel.overrides['iou'] = 0.45  # NMS IoU threshold\nmodel.overrides['agnostic_nms'] = False  # NMS class-agnostic\nmodel.overrides['max_det'] = 1000  # maximum number of detections per image\n\n# set image\nimage = '/path/to/your/document/images'\n\n# perform inference\nresults = model.predict(image)\n\n# observe results\nprint(results[0].boxes)\n\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": "N/A",
            "pretrained_datasets": "Stock market chart images with various chart patterns",
            "performance_metrics": {
                "model": "YOLOv8s",
                "size_pixels": "N/A",
                "map_val50_95": "N/A",
                "speed_cpu_onnx_ms": "N/A",
                "speed_a100_tensorrt_ms": "N/A",
                "params_m": "N/A",
                "flops_b": "N/A"
            }
        },
        "model_details": {
            "model_description": "The YOLOv8s Stock Market future trends prediction model offers a transformative solution for traders and investors by enabling real-time detection of crucial chart patterns within live trading video data. As stock markets evolve rapidly, this model's capabilities empower users with timely insights, allowing them to make informed decisions with speed and accuracy. The model seamlessly integrates into live trading systems, providing instant trends prediction and classification. By leveraging advanced bounding box techniques and pattern-specific feature extraction, the model excels in identifying patterns such as 'Down', 'Up'. This enables traders to optimize their strategies, automate trading decisions, and respond to market trends in real-time.",
            "supported_labels": "['Down', 'Up']"
        },
        "limitations_and_biases": {
            "limitations": [
                "Performance may be affected by variations in video quality, lighting conditions, and pattern complexity within live trading data.",
                "Rapid market fluctuations and noise in video data may impact the model's accuracy and responsiveness.",
                "Market-specific patterns or anomalies not well-represented in the training data may pose challenges for detection."
            ],
            "biases": "N/A",
            "risks": "N/A"
        },
        "recommendations": "Users should be aware of the model's limitations and potential biases. Thorough testing and validation within live trading simulations are advised before deploying the model in real trading environments.",
        "compute_infrastructure": {
            "hardware": "NVIDIA GeForce RTX 3080 card",
            "software": "Jupyter Notebook environment"
        },
        "contact_information": {
            "model_card_contact": "info@foduu.com"
        },
        "references": {
            "related_papers_and_resources": "N/A"
        },
        "example_implementation": {
            "sample_code": "from ultralyticsplus import YOLO, render_result\nimport cv2\n\n# load model\nmodel = YOLO('foduucom/stockmarket-future-prediction')\n\n# set model parameters\nmodel.overrides['conf'] = 0.25  # NMS confidence threshold\nmodel.overrides['iou'] = 0.45  # NMS IoU threshold\nmodel.overrides['agnostic_nms'] = False  # NMS class-agnostic\nmodel.overrides['max_det'] = 1000  # maximum number of detections per image\n\n# set image\nimage = '/path/to/your/document/images'\n\n# perform inference\nresults = model.predict(image)\n\n# observe results\nprint(results[0].boxes)\n\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()"
        }
    },
    "table-transformer-detection": "Sure, let's transform the given JSON into the unified schema.\n\nOriginal JSON:\n```json\n{\n    \"table-transformer-detection\": {\n        \"Model Overview\": \"Table Transformer (fine-tuned for Table Detection)Table Transformer (DETR) model trained on PubTables1M. It was introduced in the paperPubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documentsby Smock et al. and first released inthis repository.Disclaimer: The team releasing Table Transformer did not write a model card for this model so this model card has been written by the Hugging Face team.Model descriptionThe Table Transformer is equivalent toDETR, a Transformer-based object detection model. Note that the authors decided to use the \\\"normalize before\\\" setting of DETR, which means that layernorm is applied before self- and cross-attention.UsageYou can use the raw model for detecting tables in documents. See thedocumentationfor more info.\"\n    }\n}\n```\n\nTransformed JSON:\n```json\n{\n    \"model_name\": \"Table Transformer (fine-tuned for Table Detection)\",\n    \"developed_by\": \"Smock et al.\",\n    \"model_type\": \"Transformer-based object detection model\",\n    \"licensing\": \"Not specified\",\n    \"installation\": {\n        \"python_version\": \"Not specified\",\n        \"additional_libraries\": \"Not specified\",\n        \"installation_command\": \"Not specified\"\n    },\n    \"usage\": {\n        \"cli_example\": \"Not specified\",\n        \"python_example\": \"Not specified\"\n    },\n    \"pretrained_models_and_performance_metrics\": {\n        \"available_models\": \"Table Transformer (DETR) model trained on PubTables1M\",\n        \"pretrained_datasets\": \"PubTables1M\",\n        \"performance_metrics\": {\n            \"model\": \"Not specified\",\n            \"size_pixels\": \"Not specified\",\n            \"map_val50_95\": \"Not specified\",\n            \"speed_cpu_onnx_ms\": \"Not specified\",\n            \"speed_a100_tensorrt_ms\": \"Not specified\",\n            \"params_m\": \"Not specified\",\n            \"flops_b\": \"Not specified\"\n        }\n    },\n    \"model_details\": {\n        \"model_description\": \"The Table Transformer is equivalent to DETR, a Transformer-based object detection model. Note that the authors decided to use the \\\"normalize before\\\" setting of DETR, which means that layernorm is applied before self- and cross-attention.\",\n        \"supported_labels\": \"Tables in documents\"\n    },\n    \"limitations_and_biases\": {\n        \"limitations\": \"Not specified\",\n        \"biases\": \"Not specified\",\n        \"risks\": \"Not specified\"\n    },\n    \"recommendations\": \"Not specified\",\n    \"compute_infrastructure\": {\n        \"hardware\": \"Not specified\",\n        \"software\": \"Not specified\"\n    },\n    \"contact_information\": {\n        \"model_card_contact\": \"Hugging Face team\"\n    },\n    \"references\": {\n        \"related_papers_and_resources\": \"PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents\"\n    },\n    \"example_implementation\": {\n        \"sample_code\": \"Not specified\"\n    }\n}\n```\n\nThis transformed JSON fits the unified schema you provided, filling in specific fields with \"Not specified\" where information was missing from the original JSON.",
    "web-form-ui-field-detection": {
        "model_name": "web-form-ui-field-detection",
        "developed_by": "Nehul Agrawal and Rahul Parihar",
        "model_type": "yolov8 object detection",
        "licensing": "Not specified",
        "installation": {
            "python_version": "Not specified",
            "additional_libraries": "ultralyticsplus==0.0.28, ultralytics==8.0.43",
            "installation_command": "pip install ultralyticsplus==0.0.28 ultralytics==8.0.43"
        },
        "usage": {
            "cli_example": "Not specified",
            "python_example": "from ultralyticsplus import YOLO, render_result\n# load model\nmodel = YOLO('foduucom/web-form-ui-field-detection')\n# set model parameters\nmodel.overrides['conf'] = 0.25  # NMS confidence threshold\nmodel.overrides['iou'] = 0.45  # NMS IoU threshold\nmodel.overrides['agnostic_nms'] = False  # NMS class-agnostic\nmodel.overrides['max_det'] = 1000  # maximum number of detections per image\n# set image\nimage = '/path/to/your/document/images'\n# perform inference\nresults = model.predict(image)\n# observe results\nprint(results[0].boxes)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": "Not specified",
            "pretrained_datasets": "Diverse dataset containing images of web UI form data from different sources, resolutions, and lighting conditions",
            "performance_metrics": {
                "model": "YOLOv8s web-form UI fields detection",
                "size_pixels": "Not specified",
                "map_val50_95": "0.51",
                "speed_cpu_onnx_ms": "Not specified",
                "speed_a100_tensorrt_ms": "Not specified",
                "params_m": "Not specified",
                "flops_b": "Not specified"
            }
        },
        "model_details": {
            "model_description": "The web-form-Detect model is a YOLOv8 object detection model trained to detect and locate UI form fields in images. It is built upon the ultralytics library and fine-tuned using a dataset of annotated UI form images.",
            "supported_labels": "Name, number, email, password, button, radio bullet, and other fields"
        },
        "limitations_and_biases": {
            "limitations": "The model's performance is subject to variations in image quality, lighting conditions, and image resolutions. The model may struggle with detecting web UI forms in cases of extreme occlusion. The model may not generalize well to non-standard UI form formats or variations.",
            "biases": "Not specified",
            "risks": "Not specified"
        },
        "recommendations": "Not specified",
        "compute_infrastructure": {
            "hardware": "Single NVIDIA GeForce RTX 3090 GPU",
            "software": "Jupyter Notebook environment"
        },
        "contact_information": {
            "model_card_contact": "info@foduu.com"
        },
        "references": {
            "related_papers_and_resources": "Not specified"
        },
        "example_implementation": {
            "sample_code": "from ultralyticsplus import YOLO, render_result\n# load model\nmodel = YOLO('foduucom/web-form-ui-field-detection')\n# set model parameters\nmodel.overrides['conf'] = 0.25  # NMS confidence threshold\nmodel.overrides['iou'] = 0.45  # NMS IoU threshold\nmodel.overrides['agnostic_nms'] = False  # NMS class-agnostic\nmodel.overrides['max_det'] = 1000  # maximum number of detections per image\n# set image\nimage = '/path/to/your/document/images'\n# perform inference\nresults = model.predict(image)\n# observe results\nprint(results[0].boxes)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()"
        }
    },
    "rtdetr_r50vd_coco_o365": {
        "model_name": "rtdetr_r50vd_coco_o365",
        "developed_by": "Yian Zhao and Sangbum Choi",
        "model_type": "RT-DETR",
        "licensing": "Apache-2.0",
        "installation": {
            "python_version": "Python 3.7+",
            "additional_libraries": "torch, requests, PIL, transformers",
            "installation_command": "pip install torch requests pillow transformers"
        },
        "usage": {
            "cli_example": "",
            "python_example": "import torch\nimport requests\nfrom PIL import Image\nfrom transformers import RTDetrForObjectDetection, RTDetrImageProcessor\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nimage_processor = RTDetrImageProcessor.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\")\nmodel = RTDetrForObjectDetection.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\")\n\ninputs = image_processor(images=image, return_tensors=\"pt\")\nwith torch.no_grad():\n    outputs = model(**inputs)\n\nresults = image_processor.post_process_object_detection(outputs, target_sizes=torch.tensor([image.size[::-1]]), threshold=0.3)\nfor result in results:\n    for score, label_id, box in zip(result[\"scores\"], result[\"labels\"], result[\"boxes\"]):\n        score, label = score.item(), label_id.item()\n        box = [round(i, 2) for i in box.tolist()]\n        print(f\"{model.config.id2label[label]}: {score:.2f} {box}\")"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": [
                "RT-DETR-R18",
                "RT-DETR-R34",
                "RT-DETR-R50",
                "RT-DETR-R50 (Objects 365 pretrained)",
                "RT-DETR-R101",
                "RT-DETR-R101 (Objects 365 pretrained)"
            ],
            "pretrained_datasets": "COCO 2017, Objects365",
            "performance_metrics": [
                {
                    "model": "RT-DETR-R18",
                    "size_pixels": "640x640",
                    "map_val50_95": "50.4%",
                    "speed_cpu_onnx_ms": "",
                    "speed_a100_tensorrt_ms": "",
                    "params_m": "60.7",
                    "flops_b": "174",
                    "FPS_bs=1": "63.8"
                },
                {
                    "model": "RT-DETR-R34",
                    "size_pixels": "640x640",
                    "map_val50_95": "52.3%",
                    "speed_cpu_onnx_ms": "",
                    "speed_a100_tensorrt_ms": "",
                    "params_m": "91.0",
                    "flops_b": "172",
                    "FPS_bs=1": "66.2"
                },
                {
                    "model": "RT-DETR-R50",
                    "size_pixels": "640x640",
                    "map_val50_95": "53.1%",
                    "speed_cpu_onnx_ms": "",
                    "speed_a100_tensorrt_ms": "",
                    "params_m": "136",
                    "flops_b": "",
                    "FPS_bs=1": "108"
                },
                {
                    "model": "RT-DETR-R50 (Objects 365 pretrained)",
                    "size_pixels": "640x640",
                    "map_val50_95": "55.3%",
                    "speed_cpu_onnx_ms": "",
                    "speed_a100_tensorrt_ms": "",
                    "params_m": "136",
                    "flops_b": "",
                    "FPS_bs=1": "108"
                },
                {
                    "model": "RT-DETR-R101",
                    "size_pixels": "640x640",
                    "map_val50_95": "54.3%",
                    "speed_cpu_onnx_ms": "",
                    "speed_a100_tensorrt_ms": "",
                    "params_m": "259",
                    "flops_b": "",
                    "FPS_bs=1": "74"
                },
                {
                    "model": "RT-DETR-R101 (Objects 365 pretrained)",
                    "size_pixels": "640x640",
                    "map_val50_95": "56.2%",
                    "speed_cpu_onnx_ms": "",
                    "speed_a100_tensorrt_ms": "",
                    "params_m": "259",
                    "flops_b": "",
                    "FPS_bs=1": "74"
                }
            ]
        },
        "model_details": {
            "model_description": "The YOLO series has become the most popular framework for real-time object detection due to its reasonable trade-off between speed and accuracy. However, the speed and accuracy of YOLOs are negatively affected by the NMS. Recently, end-to-end Transformer-based detectors (DETRs) have provided an alternative to eliminating NMS. Nevertheless, the high computational cost limits their practicality and hinders them from fully exploiting the advantage of excluding NMS. In this paper, we propose the Real-Time DEtection TRansformer (RT-DETR), the first real-time end-to-end object detector to our best knowledge that addresses the above dilemma. We build RT-DETR in two steps, drawing on the advanced DETR: first we focus on maintaining accuracy while improving speed, followed by maintaining speed while improving accuracy. Specifically, we design an efficient hybrid encoder to expeditiously process multi-scale features by decoupling intra-scale interaction and cross-scale fusion to improve speed. Then, we propose the uncertainty-minimal query selection to provide high-quality initial queries to the decoder, thereby improving accuracy. In addition, RT-DETR supports flexible speed tuning by adjusting the number of decoder layers to adapt to various scenarios without retraining. Our RT-DETR-R50 / R101 achieves 53.1% / 54.3% AP on COCO and 108 / 74 FPS on T4 GPU, outperforming previously advanced YOLOs in both speed and accuracy. We also develop scaled RT-DETRs that outperform the lighter YOLO detectors (S and M models). Furthermore, RT-DETR-R50 outperforms DINO-R50 by 2.2% AP in accuracy and about 21 times in FPS. After pre-training with Objects365, RT-DETR-R50 / R101 achieves 55.3% / 56.2% AP.",
            "supported_labels": "sofa, cat, remote"
        },
        "limitations_and_biases": {
            "limitations": "High computational cost compared to traditional YOLO models.",
            "biases": "Potential biases from the training datasets (COCO 2017, Objects365).",
            "risks": "Misidentification and false positives in object detection."
        },
        "recommendations": "Use the model with the provided preprocessing and post-processing steps for optimal performance.",
        "compute_infrastructure": {
            "hardware": "T4 GPU, A100 GPU",
            "software": "PyTorch, Transformers"
        },
        "contact_information": {
            "model_card_contact": "Sangbum Choi"
        },
        "references": {
            "related_papers_and_resources": "https://arxiv.org/abs/2304.08069"
        },
        "example_implementation": {
            "sample_code": "import torch\nimport requests\nfrom PIL import Image\nfrom transformers import RTDetrForObjectDetection, RTDetrImageProcessor\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nimage_processor = RTDetrImageProcessor.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\")\nmodel = RTDetrForObjectDetection.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\")\n\ninputs = image_processor(images=image, return_tensors=\"pt\")\nwith torch.no_grad():\n    outputs = model(**inputs)\n\nresults = image_processor.post_process_object_detection(outputs, target_sizes=torch.tensor([image.size[::-1]]), threshold=0.3)\nfor result in results:\n    for score, label_id, box in zip(result[\"scores\"], result[\"labels\"], result[\"boxes\"]):\n        score, label = score.item(), label_id.item()\n        box = [round(i, 2) for i in box.tolist()]\n        print(f\"{model.config.id2label[label]}: {score:.2f} {box}\")"
        }
    },
    "yolov10x": {
        "model_name": "YOLOv10x",
        "developed_by": "Wang, Ao; Chen, Hui; Liu, Lihao; Chen, Kai; Lin, Zijia; Han, Jungong; Ding, Guiguang",
        "model_type": "Object Detection",
        "licensing": "Unknown",
        "installation": {
            "python_version": "Unknown",
            "additional_libraries": "ultralytics",
            "installation_command": "pip install git+https://github.com/THU-MIG/yolov10.git"
        },
        "usage": {
            "cli_example": "Unknown",
            "python_example": "from ultralytics import YOLOv10\n\nmodel = YOLOv10.from_pretrained('jameslahm/yolov10x')\n\n# Training\nmodel.train(...)\n\n# after training, one can push to the hub\nmodel.push_to_hub('your-hf-username/yolov10-finetuned')\n\n# Validation\nmodel.val(...)\n\n# Inference\nsource = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nmodel.predict(source=source, save=True)"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": "jameslahm/yolov10x",
            "pretrained_datasets": "Unknown",
            "performance_metrics": {
                "model": "Unknown",
                "size_pixels": "Unknown",
                "map_val50_95": "Unknown",
                "speed_cpu_onnx_ms": "Unknown",
                "speed_a100_tensorrt_ms": "Unknown",
                "params_m": "Unknown",
                "flops_b": "Unknown"
            }
        },
        "model_details": {
            "model_description": "YOLOv10: Real-Time End-to-End Object Detection",
            "supported_labels": "Unknown"
        },
        "limitations_and_biases": {
            "limitations": "Unknown",
            "biases": "Unknown",
            "risks": "Unknown"
        },
        "recommendations": "Unknown",
        "compute_infrastructure": {
            "hardware": "Unknown",
            "software": "Unknown"
        },
        "contact_information": {
            "model_card_contact": "Unknown"
        },
        "references": {
            "related_papers_and_resources": "arXiv: https://arxiv.org/abs/2405.14458v1\ngithub: https://github.com/THU-MIG/yolov10"
        },
        "example_implementation": {
            "sample_code": "from ultralytics import YOLOv10\n\nmodel = YOLOv10.from_pretrained('jameslahm/yolov10x')\n\n# Inference example\nsource = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nmodel.predict(source=source, save=True)"
        }
    },
    "YOLOv10-Document-Layout-Analysis": {
        "model_name": "YOLOv10-Document-Layout-Analysis",
        "developed_by": "Omar Moured",
        "model_type": "Document Layout Analysis",
        "licensing": "Unknown",
        "installation": {
            "python_version": "Unknown",
            "additional_libraries": "Unknown",
            "installation_command": "Unknown"
        },
        "usage": {
            "cli_example": "Unknown",
            "python_example": "Unknown"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": [
                {
                    "model": "YOLOv10-x",
                    "map_val50": 0.924,
                    "map_val50_95": 0.74,
                    "download_link": "Download"
                },
                {
                    "model": "YOLOv10-b",
                    "map_val50": 0.922,
                    "map_val50_95": 0.732,
                    "download_link": "Download"
                },
                {
                    "model": "YOLOv10-l",
                    "map_val50": 0.921,
                    "map_val50_95": 0.732,
                    "download_link": "Download"
                },
                {
                    "model": "YOLOv10-m",
                    "map_val50": 0.917,
                    "map_val50_95": 0.737,
                    "download_link": "Download"
                },
                {
                    "model": "YOLOv10-s",
                    "map_val50": 0.905,
                    "map_val50_95": 0.713,
                    "download_link": "Download"
                },
                {
                    "model": "YOLOv10-n",
                    "map_val50": 0.892,
                    "map_val50_95": 0.685,
                    "download_link": "Download"
                }
            ],
            "pretrained_datasets": "Doclaynet-base",
            "performance_metrics": {
                "model": "Unknown",
                "size_pixels": "Unknown",
                "map_val50_95": "Unknown",
                "speed_cpu_onnx_ms": "Unknown",
                "speed_a100_tensorrt_ms": "Unknown",
                "params_m": "Unknown",
                "flops_b": "Unknown"
            }
        },
        "model_details": {
            "model_description": "The models were fine-tuned using 4xA100 GPUs on the Doclaynet-base dataset, which consists of 6910 training images, 648 validation images, and 499 test images.",
            "supported_labels": "Unknown"
        },
        "limitations_and_biases": {
            "limitations": "Unknown",
            "biases": "Unknown",
            "risks": "Unknown"
        },
        "recommendations": "Unknown",
        "compute_infrastructure": {
            "hardware": "4xA100 GPUs",
            "software": "Unknown"
        },
        "contact_information": {
            "model_card_contact": "LinkedIn:https://www.linkedin.com/in/omar-moured/"
        },
        "references": {
            "related_papers_and_resources": [
                {
                    "title": "YOLOv10: Real-Time End-to-End Object Detection",
                    "author": "Wang, Ao and Chen, Hui and Liu, Lihao and Chen, Kai and Lin, Zijia and Han, Jungong and Ding, Guiguang",
                    "journal": "arXiv preprint arXiv:2405.14458",
                    "year": 2024
                },
                {
                    "title": "DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis",
                    "author": "Pfitzmann, Birgit and Auer, Christoph and Dolfi, Michele and Nassar, Ahmed S and Staar, Peter W J",
                    "journal": "arXiv preprint arXiv:2206.01062",
                    "year": 2022,
                    "doi": "10.1145/3534678.353904",
                    "url": "https://arxiv.org/abs/2206.01062"
                }
            ]
        },
        "example_implementation": {
            "sample_code": "Check out our Github repo for inference codes: https://github.com/moured/YOLOv10-Document-Layout-Analysis"
        }
    },
    "Object-Detection-RetinaNet": "Certainly! Let's transform the given JSON for \"Object-Detection-RetinaNet\" into the unified schema provided.\n\nOriginal JSON:\n```json\n{\n    \"Object-Detection-RetinaNet\": {\n        \"Model Overview\": \"Model descriptionImplementing RetinaNet: Focal Loss for Dense Object Detection.This repo contains the model for the notebookObject Detection with RetinaNetHere the model is tasked with localizing the objects present in an image, and at the same time, classifying them into different categories. In this, RetinaNet has been implemented, a popularsingle-stage detector, which is accurate and runs fast. RetinaNet uses afeature pyramid networkto efficiently detect objects at multiple scales and introduces a new loss, theFocal loss function, to alleviate the problem of the extreme foreground-background class imbalance.Full credits go toSrihari HumbarwadiReferencesRetinaNet PaperFeature Pyramid Network PaperTraining and evaluation dataThe dataset used here is aCOCO2017 datasetTraining procedureTraining hyperparametersThe following hyperparameters were used during training:namelearning_ratedecaymomentumnesterovtraining_precisionSGD{'class_name': 'PiecewiseConstantDecay', 'config': {'boundaries': [125, 250, 500, 240000, 360000], 'values': [2.5e-06, 0.000625, 0.00125, 0.0025, 0.00025, 2.5e-05], 'name': None}}0.00.8999999761581421Falsefloat32Model PlotView Model PlotModel Reproduced ByKavya Bisht\"\n    }\n}\n```\n\nTransformed JSON:\n```json\n{\n    \"model_name\": \"Object-Detection-RetinaNet\",\n    \"developed_by\": \"Srihari Humbarwadi\",\n    \"model_type\": \"Object Detection\",\n    \"licensing\": \"Not specified\",\n    \"installation\": {\n        \"python_version\": \"Not specified\",\n        \"additional_libraries\": \"Not specified\",\n        \"installation_command\": \"Not specified\"\n    },\n    \"usage\": {\n        \"cli_example\": \"Not specified\",\n        \"python_example\": \"Not specified\"\n    },\n    \"pretrained_models_and_performance_metrics\": {\n        \"available_models\": \"Not specified\",\n        \"pretrained_datasets\": \"COCO2017\",\n        \"performance_metrics\": {\n            \"model\": \"RetinaNet\",\n            \"size_pixels\": \"Not specified\",\n            \"map_val50_95\": \"Not specified\",\n            \"speed_cpu_onnx_ms\": \"Not specified\",\n            \"speed_a100_tensorrt_ms\": \"Not specified\",\n            \"params_m\": \"Not specified\",\n            \"flops_b\": \"Not specified\"\n        }\n    },\n    \"model_details\": {\n        \"model_description\": \"Implementing RetinaNet: Focal Loss for Dense Object Detection. This repo contains the model for the notebook Object Detection with RetinaNet. Here the model is tasked with localizing the objects present in an image, and at the same time, classifying them into different categories. In this, RetinaNet has been implemented, a popular single-stage detector, which is accurate and runs fast. RetinaNet uses a feature pyramid network to efficiently detect objects at multiple scales and introduces a new loss, the Focal loss function, to alleviate the problem of the extreme foreground-background class imbalance.\",\n        \"supported_labels\": \"Not specified\"\n    },\n    \"limitations_and_biases\": {\n        \"limitations\": \"Not specified\",\n        \"biases\": \"Not specified\",\n        \"risks\": \"Not specified\"\n    },\n    \"recommendations\": \"Not specified\",\n    \"compute_infrastructure\": {\n        \"hardware\": \"Not specified\",\n        \"software\": \"Not specified\"\n    },\n    \"contact_information\": {\n        \"model_card_contact\": \"Kavya Bisht\"\n    },\n    \"references\": {\n        \"related_papers_and_resources\": \"RetinaNet Paper, Feature Pyramid Network Paper\"\n    },\n    \"example_implementation\": {\n        \"sample_code\": \"Not specified\"\n    }\n}\n```\n\nThis transformed JSON aligns with the unified schema provided while filling in the details extracted from the original JSON. Some fields are marked as \"Not specified\" due to missing information from the original JSON.",
    "table-transformer-structure-recognition": {
        "model_name": "Table Transformer (fine-tuned for Table Structure Recognition)",
        "developed_by": "Smock et al., Hugging Face team",
        "model_type": "Transformer-based object detection model (DETR)",
        "licensing": "Not specified",
        "installation": {
            "python_version": "Not specified",
            "additional_libraries": "Not specified",
            "installation_command": "Not specified"
        },
        "usage": {
            "cli_example": "Not specified",
            "python_example": "Not specified"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": "Table Transformer (fine-tuned for Table Structure Recognition)",
            "pretrained_datasets": "PubTables1M",
            "performance_metrics": {
                "model": "Not specified",
                "size_pixels": "Not specified",
                "map_val50_95": "Not specified",
                "speed_cpu_onnx_ms": "Not specified",
                "speed_a100_tensorrt_ms": "Not specified",
                "params_m": "Not specified",
                "flops_b": "Not specified"
            }
        },
        "model_details": {
            "model_description": "The Table Transformer is equivalent to DETR, a Transformer-based object detection model. Note that the authors decided to use the 'normalize before' setting of DETR, which means that layernorm is applied before self- and cross-attention.",
            "supported_labels": "Rows, columns in tables"
        },
        "limitations_and_biases": {
            "limitations": "Not specified",
            "biases": "Not specified",
            "risks": "Not specified"
        },
        "recommendations": "Not specified",
        "compute_infrastructure": {
            "hardware": "Not specified",
            "software": "Not specified"
        },
        "contact_information": {
            "model_card_contact": "Hugging Face team"
        },
        "references": {
            "related_papers_and_resources": [
                "PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents by Smock et al.",
                "Table Transformer (DETR) model repository"
            ]
        },
        "example_implementation": {
            "sample_code": "Not specified"
        }
    },
    "yolos-fashionpedia": {
        "model_name": "yolos-fashionpedia",
        "developed_by": "Unknown",
        "model_type": "Object Detection",
        "licensing": "Unknown",
        "installation": {
            "python_version": "Unknown",
            "additional_libraries": "Unknown",
            "installation_command": "Unknown"
        },
        "usage": {
            "cli_example": "Unknown",
            "python_example": "Unknown"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": "Unknown",
            "pretrained_datasets": "Fashionpedia",
            "performance_metrics": {
                "model": "Unknown",
                "size_pixels": "Unknown",
                "map_val50_95": "Unknown",
                "speed_cpu_onnx_ms": "Unknown",
                "speed_a100_tensorrt_ms": "Unknown",
                "params_m": "Unknown",
                "flops_b": "Unknown"
            }
        },
        "model_details": {
            "model_description": "This is a fine-tuned object detection model for fashion. For more details of the implementation you can check the source code here. The dataset used for its training is available here.",
            "supported_labels": [
                "shirt, blouse",
                "top, t-shirt, sweatshirt",
                "sweater",
                "cardigan",
                "jacket",
                "vest",
                "pants",
                "shorts",
                "skirt",
                "coat",
                "dress",
                "jumpsuit",
                "cape",
                "glasses",
                "hat",
                "headband, head covering, hair accessory",
                "tie",
                "glove",
                "watch",
                "belt",
                "leg warmer",
                "tights, stockings",
                "sock",
                "shoe",
                "bag, wallet",
                "scarf",
                "umbrella",
                "hood",
                "collar",
                "lapel",
                "epaulette",
                "sleeve",
                "pocket",
                "neckline",
                "buckle",
                "zipper",
                "applique",
                "bead",
                "bow",
                "flower",
                "fringe",
                "ribbon",
                "rivet",
                "ruffle",
                "sequin",
                "tassel"
            ]
        },
        "limitations_and_biases": {
            "limitations": "Unknown",
            "biases": "Unknown",
            "risks": "Unknown"
        },
        "recommendations": "Unknown",
        "compute_infrastructure": {
            "hardware": "Unknown",
            "software": "Unknown"
        },
        "contact_information": {
            "model_card_contact": "Unknown"
        },
        "references": {
            "related_papers_and_resources": "Unknown"
        },
        "example_implementation": {
            "sample_code": "Unknown"
        }
    },
    "mmdet-yolox-tiny": {
        "model_name": "mmdet-yolox-tiny",
        "developed_by": "Fatih Cagatay Akyon, Sinan Onur Altinuc, Alptekin Temizel",
        "model_type": "Object Detection",
        "licensing": "Not specified",
        "installation": {
            "python_version": "Not specified",
            "additional_libraries": "sahi, mmdet, mmcv-full",
            "installation_command": "pip install -U sahi\npip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.11.0/index.html\npip install mmdet==2.26.0"
        },
        "usage": {
            "cli_example": "Not specified",
            "python_example": "from sahi import AutoDetectionModel\nfrom sahi.utils.file import download_from_url\nfrom sahi.predict import get_prediction\nfrom sahi.cv import read_image_as_pil\n\nMMDET_YOLOX_TINY_MODEL_URL = \"https://huggingface.co/fcakyon/mmdet-yolox-tiny/resolve/main/yolox_tiny_8x8_300e_coco_20211124_171234-b4047906.pth\"\nMMDET_YOLOX_TINY_MODEL_PATH = \"yolox.pt\"\nMMDET_YOLOX_TINY_CONFIG_URL = \"https://huggingface.co/fcakyon/mmdet-yolox-tiny/raw/main/yolox_tiny_8x8_300e_coco.py\"\nMMDET_YOLOX_TINY_CONFIG_PATH = \"config.py\"\nIMAGE_URL = \"https://user-images.githubusercontent.com/34196005/142730935-2ace3999-a47b-49bb-83e0-2bdd509f1c90.jpg\"\n\n# Download weight and config\ndownload_from_url(MMDET_YOLOX_TINY_MODEL_URL, MMDET_YOLOX_TINY_MODEL_PATH)\ndownload_from_url(MMDET_YOLOX_TINY_CONFIG_URL, MMDET_YOLOX_TINY_CONFIG_PATH)\n\n# Create model\ndetection_model = AutoDetectionModel.from_pretrained(model_type='mmdet', model_path=MMDET_YOLOX_TINY_MODEL_PATH, config_path=MMDET_YOLOX_TINY_CONFIG_PATH, confidence_threshold=0.5, device=\"cuda:0\")\n\n# Prepare input image\nimage = read_image_as_pil(IMAGE_URL)\n\n# Perform prediction\nprediction_result = get_prediction(image=image, detection_model=detection_model)\n\n# Visualize predictions\nprediction_result.export_predictions(export_dir='results/')\n\n# Get predictions\nprediction_result.object_prediction_list"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": "Not specified",
            "pretrained_datasets": "ImageNet-1k, COCO 2017 object detection",
            "performance_metrics": {
                "model": "Not specified",
                "size_pixels": "Not specified",
                "map_val50_95": "Not specified",
                "speed_cpu_onnx_ms": "Not specified",
                "speed_a100_tensorrt_ms": "Not specified",
                "params_m": "Not specified",
                "flops_b": "Not specified"
            }
        },
        "model_details": {
            "model_description": "YOLOX: Exceeding YOLO Series in 2021\nSAHI: Slicing Aided Hyper Inference and Fine-tuning for Small Object Detection\nImproved anchor-free YOLO architecture for object detection task.",
            "supported_labels": "Not specified"
        },
        "limitations_and_biases": {
            "limitations": "Not specified",
            "biases": "Not specified",
            "risks": "Not specified"
        },
        "recommendations": "Not specified",
        "compute_infrastructure": {
            "hardware": "Not specified",
            "software": "Not specified"
        },
        "contact_information": {
            "model_card_contact": "Not specified"
        },
        "references": {
            "related_papers_and_resources": [
                {
                    "title": "Slicing Aided Hyper Inference and Fine-tuning for Small Object Detection",
                    "author": "Fatih Cagatay Akyon, Sinan Onur Altinuc, Alptekin Temizel",
                    "journal": "2022 IEEE International Conference on Image Processing (ICIP)",
                    "doi": "10.1109/ICIP46576.2022.9897990",
                    "year": 2022
                },
                {
                    "title": "YOLOX: Exceeding YOLO Series in 2021",
                    "author": "Ge, Zheng, Liu, Songtao, Wang, Feng, Li, Zeming, Sun, Jian",
                    "journal": "arXiv preprint arXiv:2107.08430",
                    "year": 2021
                }
            ]
        },
        "example_implementation": {
            "sample_code": "from sahi import AutoDetectionModel\nfrom sahi.utils.file import download_from_url\nfrom sahi.predict import get_prediction\nfrom sahi.cv import read_image_as_pil\n\nMMDET_YOLOX_TINY_MODEL_URL = \"https://huggingface.co/fcakyon/mmdet-yolox-tiny/resolve/main/yolox_tiny_8x8_300e_coco_20211124_171234-b4047906.pth\"\nMMDET_YOLOX_TINY_MODEL_PATH = \"yolox.pt\"\nMMDET_YOLOX_TINY_CONFIG_URL = \"https://huggingface.co/fcakyon/mmdet-yolox-tiny/raw/main/yolox_tiny_8x8_300e_coco.py\"\nMMDET_YOLOX_TINY_CONFIG_PATH = \"config.py\"\nIMAGE_URL = \"https://user-images.githubusercontent.com/34196005/142730935-2ace3999-a47b-49bb-83e0-2bdd509f1c90.jpg\"\n\n# Download weight and config\ndownload_from_url(MMDET_YOLOX_TINY_MODEL_URL, MMDET_YOLOX_TINY_MODEL_PATH)\ndownload_from_url(MMDET_YOLOX_TINY_CONFIG_URL, MMDET_YOLOX_TINY_CONFIG_PATH)\n\n# Create model\ndetection_model = AutoDetectionModel.from_pretrained(model_type='mmdet', model_path=MMDET_YOLOX_TINY_MODEL_PATH, config_path=MMDET_YOLOX_TINY_CONFIG_PATH, confidence_threshold=0.5, device=\"cuda:0\")\n\n# Prepare input image\nimage = read_image_as_pil(IMAGE_URL)\n\n# Perform prediction\nprediction_result = get_prediction(image=image, detection_model=detection_model)\n\n# Visualize predictions\nprediction_result.export_predictions(export_dir='results/')\n\n# Get predictions\nprediction_result.object_prediction_list"
        }
    },
    "deta-resnet-50": "Sure, let's transform the given JSON into the provided unified schema:\n\n### Original JSON\n```json\n{\n    \"deta-resnet-50\": {\n        \"Model Overview\": \"Detection Transformers with AssignmentByJeffrey Ouyang-Zhang,Jang Hyun Cho,Xingyi Zhou,Philipp Kr\\u00e4henb\\u00fchlFrom the paperNMS Strikes Back.TL; DR.DetectionTransformers withAssignment (DETA) re-introduce IoU assignment and NMS for transformer-based detectors. DETA trains and tests comparibly as fast as Deformable-DETR and converges much faster (50.2 mAP in 12 epochs on COCO).\"\n    }\n}\n```\n\n### Transformed JSON\n```json\n{\n    \"model_name\": \"deta-resnet-50\",\n    \"developed_by\": \"Jeffrey Ouyang-Zhang, Jang Hyun Cho, Xingyi Zhou, Philipp Kr\u00e4henb\u00fchl\",\n    \"model_type\": \"Detection Transformers with Assignment\",\n    \"licensing\": \"\",\n    \"installation\": {\n        \"python_version\": \"\",\n        \"additional_libraries\": \"\",\n        \"installation_command\": \"\"\n    },\n    \"usage\": {\n        \"cli_example\": \"\",\n        \"python_example\": \"\"\n    },\n    \"pretrained_models_and_performance_metrics\": {\n        \"available_models\": \"\",\n        \"pretrained_datasets\": \"COCO\",\n        \"performance_metrics\": {\n            \"model\": \"deta-resnet-50\",\n            \"size_pixels\": \"\",\n            \"map_val50_95\": \"50.2\",\n            \"speed_cpu_onnx_ms\": \"\",\n            \"speed_a100_tensorrt_ms\": \"\",\n            \"params_m\": \"\",\n            \"flops_b\": \"\"\n        }\n    },\n    \"model_details\": {\n        \"model_description\": \"Detection Transformers with Assignment (DETA) re-introduce IoU assignment and NMS for transformer-based detectors. DETA trains and tests comparably as fast as Deformable-DETR and converges much faster (50.2 mAP in 12 epochs on COCO).\",\n        \"supported_labels\": \"\"\n    },\n    \"limitations_and_biases\": {\n        \"limitations\": \"\",\n        \"biases\": \"\",\n        \"risks\": \"\"\n    },\n    \"recommendations\": \"\",\n    \"compute_infrastructure\": {\n        \"hardware\": \"\",\n        \"software\": \"\"\n    },\n    \"contact_information\": {\n        \"model_card_contact\": \"\"\n    },\n    \"references\": {\n        \"related_papers_and_resources\": \"NMS Strikes Back\"\n    },\n    \"example_implementation\": {\n        \"sample_code\": \"\"\n    }\n}\n```\n\n### Explanation\n\n- **model_name**: Extracted from the key of the original JSON.\n- **developed_by**: Extracted from the \"Model Overview\" text.\n- **model_type**: Extracted from the \"Model Overview\" text.\n- **pretrained_datasets**: Mentioned as \"COCO\" in the original text.\n- **performance_metrics**: Extracted the mAP value from the text.\n- **model_description**: The main description text from \"Model Overview\".\n- **related_papers_and_resources**: Paper name \"NMS Strikes Back\" is extracted from the text.\n- Other fields are left blank as there is no information provided in the original JSON for those fields.",
    "yolov8s": {
        "model_name": "yolov8s",
        "developed_by": "Ultralytics",
        "model_type": "Object Detection",
        "licensing": "Apache 2.0",
        "installation": {
            "python_version": ">=3.6",
            "additional_libraries": "ultralyticsplus",
            "installation_command": "pip install -U ultralyticsplus==0.0.14"
        },
        "usage": {
            "cli_example": "Not provided",
            "python_example": "from ultralyticsplus import YOLO, render_result\n# load model\nmodel = YOLO('ultralyticsplus/yolov8s')\n# set model parameters\nmodel.overrides['conf'] = 0.25  # NMS confidence threshold\nmodel.overrides['iou'] = 0.45  # NMS IoU threshold\nmodel.overrides['agnostic_nms'] = False  # NMS class-agnostic\nmodel.overrides['max_det'] = 1000  # maximum number of detections per image\n# set image\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\n# perform inference\nresults = model.predict(image)\n# observe results\nprint(results[0].boxes)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": "Not provided",
            "pretrained_datasets": "COCO",
            "performance_metrics": {
                "model": "Not provided",
                "size_pixels": "Not provided",
                "map_val50_95": "Not provided",
                "speed_cpu_onnx_ms": "Not provided",
                "speed_a100_tensorrt_ms": "Not provided",
                "params_m": "Not provided",
                "flops_b": "Not provided"
            }
        },
        "model_details": {
            "model_description": "YOLOv8s is a small-sized object detection model developed by Ultralytics. It supports a wide range of labels and is designed for high-speed inference.",
            "supported_labels": [
                "person",
                "bicycle",
                "car",
                "motorcycle",
                "airplane",
                "bus",
                "train",
                "truck",
                "boat",
                "traffic light",
                "fire hydrant",
                "stop sign",
                "parking meter",
                "bench",
                "bird",
                "cat",
                "dog",
                "horse",
                "sheep",
                "cow",
                "elephant",
                "bear",
                "zebra",
                "giraffe",
                "backpack",
                "umbrella",
                "handbag",
                "tie",
                "suitcase",
                "frisbee",
                "skis",
                "snowboard",
                "sports ball",
                "kite",
                "baseball bat",
                "baseball glove",
                "skateboard",
                "surfboard",
                "tennis racket",
                "bottle",
                "wine glass",
                "cup",
                "fork",
                "knife",
                "spoon",
                "bowl",
                "banana",
                "apple",
                "sandwich",
                "orange",
                "broccoli",
                "carrot",
                "hot dog",
                "pizza",
                "donut",
                "cake",
                "chair",
                "couch",
                "potted plant",
                "bed",
                "dining table",
                "toilet",
                "tv",
                "laptop",
                "mouse",
                "remote",
                "keyboard",
                "cell phone",
                "microwave",
                "oven",
                "toaster",
                "sink",
                "refrigerator",
                "book",
                "clock",
                "vase",
                "scissors",
                "teddy bear",
                "hair drier",
                "toothbrush"
            ]
        },
        "limitations_and_biases": {
            "limitations": "Not provided",
            "biases": "Not provided",
            "risks": "Not provided"
        },
        "recommendations": "Not provided",
        "compute_infrastructure": {
            "hardware": "Not provided",
            "software": "Not provided"
        },
        "contact_information": {
            "model_card_contact": "Not provided"
        },
        "references": {
            "related_papers_and_resources": "Not provided"
        },
        "example_implementation": {
            "sample_code": "from ultralyticsplus import YOLO, render_result\n# load model\nmodel = YOLO('ultralyticsplus/yolov8s')\n# set model parameters\nmodel.overrides['conf'] = 0.25  # NMS confidence threshold\nmodel.overrides['iou'] = 0.45  # NMS IoU threshold\nmodel.overrides['agnostic_nms'] = False  # NMS class-agnostic\nmodel.overrides['max_det'] = 1000  # maximum number of detections per image\n# set image\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\n# perform inference\nresults = model.predict(image)\n# observe results\nprint(results[0].boxes)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()"
        }
    },
    "deta-swin-large": "To transform the provided JSON into the unified schema, we'll map the given information into the appropriate fields in the new schema. Here's the transformed JSON:\n\nOriginal JSON:\n```json\n{\n    \"deta-swin-large\": {\n        \"Model Overview\": \"Detection Transformers with AssignmentByJeffrey Ouyang-Zhang,Jang Hyun Cho,Xingyi Zhou,Philipp Kr\\u00e4henb\\u00fchlFrom the paperNMS Strikes Back.TL; DR.DetectionTransformers withAssignment (DETA) re-introduce IoU assignment and NMS for transformer-based detectors. DETA trains and tests comparably as fast as Deformable-DETR and converges much faster (50.2 mAP in 12 epochs on COCO).\"\n    }\n}\n```\n\nTransformed JSON:\n```json\n{\n    \"model_name\": \"deta-swin-large\",\n    \"developed_by\": \"Jeffrey Ouyang-Zhang, Jang Hyun Cho, Xingyi Zhou, Philipp Kr\u00e4henb\u00fchl\",\n    \"model_type\": \"Detection Transformers with Assignment\",\n    \"licensing\": \"Unknown\",\n    \"installation\": {\n        \"python_version\": \"Unknown\",\n        \"additional_libraries\": \"Unknown\",\n        \"installation_command\": \"Unknown\"\n    },\n    \"usage\": {\n        \"cli_example\": \"Unknown\",\n        \"python_example\": \"Unknown\"\n    },\n    \"pretrained_models_and_performance_metrics\": {\n        \"available_models\": \"Unknown\",\n        \"pretrained_datasets\": \"COCO\",\n        \"performance_metrics\": {\n            \"model\": \"deta-swin-large\",\n            \"size_pixels\": \"Unknown\",\n            \"map_val50_95\": \"50.2\",\n            \"speed_cpu_onnx_ms\": \"Unknown\",\n            \"speed_a100_tensorrt_ms\": \"Unknown\",\n            \"params_m\": \"Unknown\",\n            \"flops_b\": \"Unknown\"\n        }\n    },\n    \"model_details\": {\n        \"model_description\": \"Detection Transformers with Assignment (DETA) re-introduce IoU assignment and NMS for transformer-based detectors. DETA trains and tests comparably as fast as Deformable-DETR and converges much faster.\",\n        \"supported_labels\": \"Unknown\"\n    },\n    \"limitations_and_biases\": {\n        \"limitations\": \"Unknown\",\n        \"biases\": \"Unknown\",\n        \"risks\": \"Unknown\"\n    },\n    \"recommendations\": \"Unknown\",\n    \"compute_infrastructure\": {\n        \"hardware\": \"Unknown\",\n        \"software\": \"Unknown\"\n    },\n    \"contact_information\": {\n        \"model_card_contact\": \"Unknown\"\n    },\n    \"references\": {\n        \"related_papers_and_resources\": \"NMS Strikes Back\"\n    },\n    \"example_implementation\": {\n        \"sample_code\": \"Unknown\"\n    }\n}\n```\n\n### Explanation:\n1. **Model Name**: Retrieved from the key \"deta-swin-large\".\n2. **Developed By**: Extracted names from the \"Model Overview\" text.\n3. **Model Type**: Described as \"Detection Transformers with Assignment\".\n4. **Licensing**: Not provided in the original JSON, so marked as \"Unknown\".\n5. **Installation**: Details are not provided, marked as \"Unknown\".\n6. **Usage**: CLI and Python examples are not provided, marked as \"Unknown\".\n7. **Pretrained Models and Performance Metrics**:\n    - **Available Models**: Not provided, marked as \"Unknown\".\n    - **Pretrained Datasets**: Inferred as \"COCO\".\n    - **Performance Metrics**: Extracted \"50.2 mAP in 12 epochs on COCO\", mapped to `map_val50_95`.\n8. **Model Details**: \n    - **Model Description**: Retrieved and cleaned from the \"Model Overview\".\n    - **Supported Labels**: Not provided, marked as \"Unknown\".\n9. **Limitations and Biases**: Not provided, marked as \"Unknown\".\n10. **Recommendations**: Not provided, marked as \"Unknown\".\n11. **Compute Infrastructure**: Not provided, marked as \"Unknown\".\n12. **Contact Information**: Not provided, marked as \"Unknown\".\n13. **References**: Extracted from the \"Model Overview\".\n14. **Example Implementation**: Not provided, marked as \"Unknown\".\n\nThis transformation ensures that all relevant information from the original JSON is captured and placed into the new unified schema.",
    "yolov8x-visdrone": {
        "model_name": "yolov8x-visdrone",
        "developed_by": "Ultralytics",
        "model_type": "Object Detection",
        "licensing": "Unknown",
        "installation": {
            "python_version": ">=3.7",
            "additional_libraries": "ultralyticsplus, ultralytics",
            "installation_command": "pip install ultralyticsplus==0.0.28 ultralytics==8.0.43"
        },
        "usage": {
            "cli_example": "Not provided",
            "python_example": "from ultralyticsplus import YOLO, render_result\n# load model\nmodel = YOLO('mshamrai/yolov8x-visdrone')\n# set model parameters\nmodel.overrides['conf'] = 0.25  # NMS confidence threshold\nmodel.overrides['iou'] = 0.45  # NMS IoU threshold\nmodel.overrides['agnostic_nms'] = False  # NMS class-agnostic\nmodel.overrides['max_det'] = 1000  # maximum number of detections per image\n# set image\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\n# perform inference\nresults = model.predict(image)\n# observe results\nprint(results[0].boxes)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": "Not provided",
            "pretrained_datasets": "VisDrone",
            "performance_metrics": {
                "model": "yolov8x-visdrone",
                "size_pixels": "Unknown",
                "map_val50_95": "Unknown",
                "speed_cpu_onnx_ms": "Unknown",
                "speed_a100_tensorrt_ms": "Unknown",
                "params_m": "Unknown",
                "flops_b": "Unknown"
            }
        },
        "model_details": {
            "model_description": "YOLOv8x model fine-tuned on the VisDrone dataset for object detection.",
            "supported_labels": [
                "pedestrian",
                "people",
                "bicycle",
                "car",
                "van",
                "truck",
                "tricycle",
                "awning-tricycle",
                "bus",
                "motor"
            ]
        },
        "limitations_and_biases": {
            "limitations": "Not provided",
            "biases": "Not provided",
            "risks": "Not provided"
        },
        "recommendations": "Not provided",
        "compute_infrastructure": {
            "hardware": "Unknown",
            "software": "Unknown"
        },
        "contact_information": {
            "model_card_contact": "Not provided"
        },
        "references": {
            "related_papers_and_resources": "Not provided"
        },
        "example_implementation": {
            "sample_code": "from ultralyticsplus import YOLO, render_result\n# load model\nmodel = YOLO('mshamrai/yolov8x-visdrone')\n# set model parameters\nmodel.overrides['conf'] = 0.25  # NMS confidence threshold\nmodel.overrides['iou'] = 0.45  # NMS IoU threshold\nmodel.overrides['agnostic_nms'] = False  # NMS class-agnostic\nmodel.overrides['max_det'] = 1000  # maximum number of detections per image\n# set image\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\n# perform inference\nresults = model.predict(image)\n# observe results\nprint(results[0].boxes)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()"
        }
    },
    "table-detection-and-extraction": {
        "model_name": "YOLOv8s Table Detection",
        "developed_by": "FODUU AI",
        "model_type": "Object Detection",
        "licensing": "Not Specified",
        "installation": {
            "python_version": "Not Specified",
            "additional_libraries": "ultralyticsplus==0.0.28, ultralytics==8.0.43",
            "installation_command": "pip install ultralyticsplus==0.0.28 ultralytics==8.0.43"
        },
        "usage": {
            "cli_example": "Not Specified",
            "python_example": "from ultralyticsplus import YOLO, render_result\n# load model\nmodel = YOLO('foduucom/table-detection-and-extraction')\n# set model parameters\nmodel.overrides['conf'] = 0.25  # NMS confidence threshold\nmodel.overrides['iou'] = 0.45  # NMS IoU threshold\nmodel.overrides['agnostic_nms'] = False  # NMS class-agnostic\nmodel.overrides['max_det'] = 1000  # maximum number of detections per image\n# set image\nimage = '/path/to/your/document/images'\n# perform inference\nresults = model.predict(image)\n# observe results\nprint(results[0].boxes)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": "Not Specified",
            "pretrained_datasets": "Diverse dataset containing images of tables from various sources, including both bordered and borderless tables.",
            "performance_metrics": {
                "model": "YOLOv8s Table Detection",
                "size_pixels": "Not Specified",
                "map_val50_95": "Not Specified",
                "speed_cpu_onnx_ms": "Not Specified",
                "speed_a100_tensorrt_ms": "Not Specified",
                "params_m": "Not Specified",
                "flops_b": "Not Specified"
            }
        },
        "model_details": {
            "model_description": "The YOLOv8s Table Detection model is an object detection model based on the YOLO (You Only Look Once) framework. It is designed to detect tables, whether they are bordered or borderless, in images. The model has been fine-tuned on a vast dataset and achieved high accuracy in detecting tables and distinguishing between bordered and borderless ones. It integrates seamlessly with Optical Character Recognition (OCR) technology to extract pertinent data contained within tables.",
            "supported_labels": "['bordered', 'borderless']"
        },
        "limitations_and_biases": {
            "limitations": "Performance may vary based on the quality, diversity, and representativeness of the training data. The model may face challenges in detecting tables with intricate designs or complex arrangements. Accuracy may be affected by variations in lighting conditions, image quality, and resolution. Detection of very small or distant tables might be less accurate.",
            "biases": "The model's ability to classify bordered and borderless tables may be influenced by variations in design.",
            "risks": "Not Specified"
        },
        "recommendations": "Users should be informed about the model's limitations and potential biases. Further testing and validation are advised for specific use cases to evaluate its performance accurately.",
        "compute_infrastructure": {
            "hardware": "NVIDIA GeForce RTX 3060 card",
            "software": "The model was trained and fine-tuned using a Jupyter Notebook environment."
        },
        "contact_information": {
            "model_card_contact": "info@foduu.com"
        },
        "references": {
            "related_papers_and_resources": "Not Specified"
        },
        "example_implementation": {
            "sample_code": "from ultralyticsplus import YOLO, render_result\n# load model\nmodel = YOLO('foduucom/table-detection-and-extraction')\n# set model parameters\nmodel.overrides['conf'] = 0.25  # NMS confidence threshold\nmodel.overrides['iou'] = 0.45  # NMS IoU threshold\nmodel.overrides['agnostic_nms'] = False  # NMS class-agnostic\nmodel.overrides['max_det'] = 1000  # maximum number of detections per image\n# set image\nimage = '/path/to/your/document/images'\n# perform inference\nresults = model.predict(image)\n# observe results\nprint(results[0].boxes)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()"
        }
    },
    "human-detector": {
        "model_name": "human-detector",
        "developed_by": "monet-joe",
        "model_type": "Human Detector",
        "licensing": "Unknown",
        "installation": {
            "python_version": "Unknown",
            "additional_libraries": "Git LFS",
            "installation_command": "GIT_LFS_SKIP_SMUDGE=1 git clone git@hf.co:monet-joe/human-detector"
        },
        "usage": {
            "cli_example": "Unknown",
            "python_example": "Unknown"
        },
        "pretrained_models_and_performance_metrics": {
            "available_models": "Unknown",
            "pretrained_datasets": "Open Images dataset",
            "performance_metrics": {
                "model": "Unknown",
                "size_pixels": "Unknown",
                "map_val50_95": "Unknown",
                "speed_cpu_onnx_ms": "Unknown",
                "speed_a100_tensorrt_ms": "Unknown",
                "params_m": "Unknown",
                "flops_b": "Unknown"
            }
        },
        "model_details": {
            "model_description": "Human DetectorAge and gender recognition in the field is a challenging task: in addition to variable environmental conditions, pose complexity, and differences in image quality, there is also partial or complete occlusion of the face. MiVOLO (Multi-Input VOLO) is a simple approach to age and gender estimation utilizing the state-of-the-art Vision Transformer. The method integrates these two tasks into a unified two-input/output model that utilizes not only facial information but also person image data. This improves the generalization ability of the model, allowing it to provide satisfactory results even when faces are not visible in the image. To evaluate the model, experiments were conducted on four popular benchmark datasets and state-of-the-art performance was achieved while demonstrating the ability to process in real-time. In addition, a new benchmark dataset was introduced based on images from the Open Images dataset. The ground truth annotations of this benchmark dataset are carefully generated by human annotators and high accuracy is guaranteed by intelligently aggregating the voting results. In addition, the age recognition performance of the model is compared to human-level accuracy and demonstrated to significantly outperform humans in most age ranges. Finally, access to the model was provided to the public, along with the code used for validation and inference. Additional annotations are also provided for the datasets used, and new benchmark datasets are presented.",
            "supported_labels": "age, gender"
        },
        "limitations_and_biases": {
            "limitations": "Unknown",
            "biases": "Unknown",
            "risks": "Unknown"
        },
        "recommendations": "Unknown",
        "compute_infrastructure": {
            "hardware": "Unknown",
            "software": "Unknown"
        },
        "contact_information": {
            "model_card_contact": "Unknown"
        },
        "references": {
            "related_papers_and_resources": [
                "https://github.com/WildChlamydia/MiVOLO",
                "https://www.modelscope.cn/models/monetjoe/human-detector"
            ]
        },
        "example_implementation": {
            "sample_code": "Unknown"
        }
    }
}