{
    "YOLOv8": {
        "Model Overview": "\n\n\n\n\n\n\u4e2d\u6587 | \ud55c\uad6d\uc5b4 | \u65e5\u672c\u8a9e | \u0420\u0443\u0441\u0441\u043a\u0438\u0439 | Deutsch | Fran\u00e7ais | Espa\u00f1ol | Portugu\u00eas | \u0939\u093f\u0928\u094d\u0926\u0940 | \u0627\u0644\u0639\u0631\u0628\u064a\u0629 \n\nUltralytics YOLOv8 is a cutting-edge, state-of-the-art (SOTA) model that builds upon the success of previous YOLO versions and introduces new features and improvements to further boost performance and flexibility. YOLOv8 is designed to be fast, accurate, and easy to use, making it an excellent choice for a wide range of object detection and tracking, instance segmentation, image classification and pose estimation tasks.\nWe hope that the resources here will help you get the most out of YOLOv8. Please browse the YOLOv8 Docs for details, raise an issue on GitHub for support, and join our Discord community for questions and discussions!\nTo request an Enterprise License please complete the form at Ultralytics Licensing.\n\n\n\n\n\n\n\nDocumentation\n\n\nSee below for a quickstart installation and usage example, and see the YOLOv8 Docs for full documentation on training, validation, prediction and deployment.\n\nInstall\nPip install the ultralytics package including all requirements in a Python>=3.8 environment with PyTorch>=1.8.\n \npip install ultralytics\n\nFor alternative installation methods including Conda, Docker, and Git, please refer to the Quickstart Guide.\n\n\nUsage\n\n\n\n\n\n\t\tCLI\n\t\n\nYOLOv8 may be used directly in the Command Line Interface (CLI) with a yolo command:\nyolo predict model=yolov8n.pt source='https://ultralytics.com/images/bus.jpg'\n\nyolo can be used for a variety of tasks and modes and accepts additional arguments, i.e. imgsz=640. See the YOLOv8 CLI Docs for examples.\n\n\n\n\n\n\t\tPython\n\t\n\nYOLOv8 may also be used directly in a Python environment, and accepts the same arguments as in the CLI example above:\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolov8n.yaml\")  # build a new model from scratch\nmodel = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n\n# Use the model\nmodel.train(data=\"coco128.yaml\", epochs=3)  # train the model\nmetrics = model.val()  # evaluate model performance on the validation set\nresults = model(\"https://ultralytics.com/images/bus.jpg\")  # predict on an image\npath = model.export(format=\"onnx\")  # export the model to ONNX format\n\nSee YOLOv8 Python Docs for more examples.\n\n\n\n\n\n\n\t\tNotebooks\n\t\n\nUltralytics provides interactive notebooks for YOLOv8, covering training, validation, tracking, and more. Each notebook is paired with a YouTube tutorial, making it easy to learn and implement advanced YOLOv8 features.\n\n\n\nDocs\nNotebook\nYouTube\n\n\nYOLOv8 Train, Val, Predict and Export Modes\n\n\n\n\nUltralytics HUB QuickStart\n\n\n\n\nYOLOv8 Multi-Object Tracking in Videos\n\n\n\n\nYOLOv8 Object Counting in Videos\n\n\n\n\nYOLOv8 Heatmaps in Videos\n\n\n\n\nUltralytics Datasets Explorer with SQL and OpenAI Integration \ud83d\ude80 New\n\n\n\n\n\n\n\n\n\n\n\nModels\n\n\nYOLOv8 Detect, Segment and Pose models pretrained on the COCO dataset are available here, as well as YOLOv8 Classify models pretrained on the ImageNet dataset. Track mode is available for all Detect, Segment and Pose models.\n\nAll Models download automatically from the latest Ultralytics release on first use.\nDetection (COCO)\nSee Detection Docs for usage examples with these models trained on COCO, which include 80 pre-trained classes.\n\n\n\nModel\nsize(pixels)\nmAPval50-95\nSpeedCPU ONNX(ms)\nSpeedA100 TensorRT(ms)\nparams(M)\nFLOPs(B)\n\n\nYOLOv8n\n640\n37.3\n80.4\n0.99\n3.2\n8.7\n\n\nYOLOv8s\n640\n44.9\n128.4\n1.20\n11.2\n28.6\n\n\nYOLOv8m\n640\n50.2\n234.7\n1.83\n25.9\n78.9\n\n\nYOLOv8l\n640\n52.9\n375.2\n2.39\n43.7\n165.2\n\n\nYOLOv8x\n640\n53.9\n479.1\n3.53\n68.2\n257.8\n\n\n\n\n\nmAPval values are for single-model single-scale on COCO val2017 dataset. Reproduce by yolo val detect data=coco.yaml device=0\nSpeed averaged over COCO val images using an Amazon EC2 P4d instance. Reproduce by yolo val detect data=coco.yaml batch=1 device=0|cpu\n\n\nDetection (Open Image V7)\nSee Detection Docs for usage examples with these models trained on Open Image V7, which include 600 pre-trained classes.\n\n\n\nModel\nsize(pixels)\nmAPval50-95\nSpeedCPU ONNX(ms)\nSpeedA100 TensorRT(ms)\nparams(M)\nFLOPs(B)\n\n\nYOLOv8n\n640\n18.4\n142.4\n1.21\n3.5\n10.5\n\n\nYOLOv8s\n640\n27.7\n183.1\n1.40\n11.4\n29.7\n\n\nYOLOv8m\n640\n33.6\n408.5\n2.26\n26.2\n80.6\n\n\nYOLOv8l\n640\n34.9\n596.9\n2.43\n44.1\n167.4\n\n\nYOLOv8x\n640\n36.3\n860.6\n3.56\n68.7\n260.6\n\n\n\n\n\nmAPval values are for single-model single-scale on Open Image V7 dataset. Reproduce by yolo val detect data=open-images-v7.yaml device=0\nSpeed averaged over Open Image V7 val images using an Amazon EC2 P4d instance. Reproduce by yolo val detect data=open-images-v7.yaml batch=1 device=0|cpu\n\n\nSegmentation (COCO)\nSee Segmentation Docs for usage examples with these models trained on COCO-Seg, which include 80 pre-trained classes.\n\n\n\nModel\nsize(pixels)\nmAPbox50-95\nmAPmask50-95\nSpeedCPU ONNX(ms)\nSpeedA100 TensorRT(ms)\nparams(M)\nFLOPs(B)\n\n\nYOLOv8n-seg\n640\n36.7\n30.5\n96.1\n1.21\n3.4\n12.6\n\n\nYOLOv8s-seg\n640\n44.6\n36.8\n155.7\n1.47\n11.8\n42.6\n\n\nYOLOv8m-seg\n640\n49.9\n40.8\n317.0\n2.18\n27.3\n110.2\n\n\nYOLOv8l-seg\n640\n52.3\n42.6\n572.4\n2.79\n46.0\n220.5\n\n\nYOLOv8x-seg\n640\n53.4\n43.4\n712.1\n4.02\n71.8\n344.1\n\n\n\n\n\nmAPval values are for single-model single-scale on COCO val2017 dataset. Reproduce by yolo val segment data=coco-seg.yaml device=0\nSpeed averaged over COCO val images using an Amazon EC2 P4d instance. Reproduce by yolo val segment data=coco-seg.yaml batch=1 device=0|cpu\n\n\nPose (COCO)\nSee Pose Docs for usage examples with these models trained on COCO-Pose, which include 1 pre-trained class, person.\n\n\n\nModel\nsize(pixels)\nmAPpose50-95\nmAPpose50\nSpeedCPU ONNX(ms)\nSpeedA100 TensorRT(ms)\nparams(M)\nFLOPs(B)\n\n\nYOLOv8n-pose\n640\n50.4\n80.1\n131.8\n1.18\n3.3\n9.2\n\n\nYOLOv8s-pose\n640\n60.0\n86.2\n233.2\n1.42\n11.6\n30.2\n\n\nYOLOv8m-pose\n640\n65.0\n88.8\n456.3\n2.00\n26.4\n81.0\n\n\nYOLOv8l-pose\n640\n67.6\n90.0\n784.5\n2.59\n44.4\n168.6\n\n\nYOLOv8x-pose\n640\n69.2\n90.2\n1607.1\n3.73\n69.4\n263.2\n\n\nYOLOv8x-pose-p6\n1280\n71.6\n91.2\n4088.7\n10.04\n99.1\n1066.4\n\n\n\n\n\nmAPval values are for single-model single-scale on COCO Keypoints val2017 dataset. Reproduce by yolo val pose data=coco-pose.yaml device=0\nSpeed averaged over COCO val images using an Amazon EC2 P4d instance. Reproduce by yolo val pose data=coco-pose.yaml batch=1 device=0|cpu\n\n\nOBB (DOTAv1)\nSee OBB Docs for usage examples with these models trained on DOTAv1, which include 15 pre-trained classes.\n\n\n\nModel\nsize(pixels)\nmAPtest50\nSpeedCPU ONNX(ms)\nSpeedA100 TensorRT(ms)\nparams(M)\nFLOPs(B)\n\n\nYOLOv8n-obb\n1024\n78.0\n204.77\n3.57\n3.1\n23.3\n\n\nYOLOv8s-obb\n1024\n79.5\n424.88\n4.07\n11.4\n76.3\n\n\nYOLOv8m-obb\n1024\n80.5\n763.48\n7.61\n26.4\n208.6\n\n\nYOLOv8l-obb\n1024\n80.7\n1278.42\n11.83\n44.5\n433.8\n\n\nYOLOv8x-obb\n1024\n81.36\n1759.10\n13.23\n69.5\n676.7\n\n\n\n\n\nmAPtest values are for single-model multi-scale on DOTAv1 dataset. Reproduce by yolo val obb data=DOTAv1.yaml device=0 split=test and submit merged results to DOTA evaluation.\nSpeed averaged over DOTAv1 val images using an Amazon EC2 P4d instance. Reproduce by yolo val obb data=DOTAv1.yaml batch=1 device=0|cpu\n\n\nClassification (ImageNet)\nSee Classification Docs for usage examples with these models trained on ImageNet, which include 1000 pretrained classes.\n\n\n\nModel\nsize(pixels)\nacctop1\nacctop5\nSpeedCPU ONNX(ms)\nSpeedA100 TensorRT(ms)\nparams(M)\nFLOPs(B) at 640\n\n\nYOLOv8n-cls\n224\n69.0\n88.3\n12.9\n0.31\n2.7\n4.3\n\n\nYOLOv8s-cls\n224\n73.8\n91.7\n23.4\n0.35\n6.4\n13.5\n\n\nYOLOv8m-cls\n224\n76.8\n93.5\n85.4\n0.62\n17.0\n42.7\n\n\nYOLOv8l-cls\n224\n76.8\n93.5\n163.0\n0.87\n37.5\n99.7\n\n\nYOLOv8x-cls\n224\n79.0\n94.6\n232.0\n1.01\n57.4\n154.8\n\n\n\n\n\nacc values are model accuracies on the ImageNet dataset validation set. Reproduce by yolo val classify data=path/to/ImageNet device=0\nSpeed averaged over ImageNet val images using an Amazon EC2 P4d instance. Reproduce by yolo val classify data=path/to/ImageNet batch=1 device=0|cpu\n\n\n\n\n\n\n\nIntegrations\n\n\nOur key integrations with leading AI platforms extend the functionality of Ultralytics' offerings, enhancing tasks like dataset labeling, training, visualization, and model management. Discover how Ultralytics, in collaboration with Roboflow, ClearML, Comet, Neural Magic and OpenVINO, can optimize your AI workflow.\n\n\n\n\n\n\n\n\nRoboflow\nClearML \u2b50 NEW\nComet \u2b50 NEW\nNeural Magic \u2b50 NEW\n\n\nLabel and export your custom datasets directly to YOLOv8 for training with Roboflow\nAutomatically track, visualize and even remotely train YOLOv8 using ClearML (open-source!)\nFree forever, Comet lets you save YOLOv8 models, resume training, and interactively visualize and debug predictions\nRun YOLOv8 inference up to 6x faster with Neural Magic DeepSparse\n\n\n\n\n\n\n\n\n\nUltralytics HUB\n\n\nExperience seamless AI with Ultralytics HUB \u2b50, the all-in-one solution for data visualization, YOLOv5 and YOLOv8 \ud83d\ude80 model training and deployment, without any coding. Transform images into actionable insights and bring your AI visions to life with ease using our cutting-edge platform and user-friendly Ultralytics App. Start your journey for Free now!\n\n\n\n\n\n\n\nContribute\n\n\nWe love your input! YOLOv5 and YOLOv8 would not be possible without help from our community. Please see our Contributing Guide to get started, and fill out our Survey to send us feedback on your experience. Thank you \ud83d\ude4f to all our contributors!\n\n\n\n\n\n\n\nLicense\n\n\nUltralytics offers two licensing options to accommodate diverse use cases:\n\nAGPL-3.0 License: This OSI-approved open-source license is ideal for students and enthusiasts, promoting open collaboration and knowledge sharing. See the LICENSE file for more details.\nEnterprise License: Designed for commercial use, this license permits seamless integration of Ultralytics software and AI models into commercial goods and services, bypassing the open-source requirements of AGPL-3.0. If your scenario involves embedding our solutions into a commercial offering, reach out through Ultralytics Licensing.\n\n\n\n\n\n\nContact\n\n\nFor Ultralytics bug reports and feature requests please visit GitHub Issues, and join our Discord community for questions and discussions!\n\n"
    },
    "thermal-image-object-detection": {
        "Model Overview": "\n\n\n\n\n\n\n\n\n\t\tModel Card for YOLOv8 object detection in thermal image\n\t\n\n\n\n\n\n\n\t\tNote\n\t\n\nThis model is specially design for thermal image object detection \n\n\n\n\n\n\t\tModel Summary\n\t\n\nThe YOLOv8  object Detection model is an obYOLOv8s Table Detectionject detection model based on the YOLO (You Only Look Once) framework. It is designed to thermal image detect object, whether they are thermal object detect, in images. The model has been fine-tuned on a vast dataset and achieved high accuracy in detecting tables and distinguishing between thermal object detect ones.\n\n\n\n\n\n\t\tModel Details\n\t\n\n\n\n\n\n\n\t\tModel Description\n\t\n\nThe YOLOv8 Object Detection model serves as a versatile solution for precisely identifying thermal object detect within images, whether they exhibit a object detect. Notably, this model's capabilities extend beyond mere detection \u2013 it plays a crucial role for object detection. By employing advanced techniques such as object detection.\nWe invite you to explore the potential of this model and its object detection capabilities. For those interested in harnessing its power or seeking further collaboration, we encourage you to reach out to us at info@foduu.com. Whether you require assistance, customization, or have innovative ideas, our collaborative approach is geared towards addressing your unique challenges. Additionally, you can actively engage with our vibrant community section for valuable insights and collective problem-solving. Your input drives our continuous improvement, as we collectively pave the way towards enhanced object detection.\n\nDeveloped by: FODUU AI\nModel type: Object Detection\nTask: Thermal Object Detection (object detection)\n\nFurthermore, the YOLOv8  Detection model is limited to object detectionsetup environment python setup.py\nyolov8 object detection model alone. It is a versatile tool that contributes to the processing of structured image data. By utilizing advanced box techniques, the model empowers users to isolate object within the thermal image data. What sets this model apart is its seamless integration with object detection technology. The combination of box information allows for precise object detection from the thermal image data. This comprehensive approach streamlines the process of information retrieval from thermal image data.\nUser collaboration is actively encouraged to enrich the model's capabilities. By contributing table images of different designs and types, users play a pivotal role in enhancing the model's ability to detect a diverse range of object accurately. Community participation can be facilitated through our platform or by reaching out to us at info@foduu.com. We value collaborative efforts that drive continuous improvement and innovation in object detection.\n\n\n\n\n\n\t\tSupported Labels\n\t\n\n YOLOv8\n['box', 'object detect']\n\n\n\n\n\n\n\t\tUses\n\t\n\n\n\n\n\n\n\t\tDirect Use\n\t\n\nThe YOLOv8 for object Detection  model can be directly used for detecting object from thermal images, whether they are bordered box.\n\n\n\n\n\n\t\tDownstream Use\n\t\n\nThe model can also be fine-tuned for specific object detection tasks or integrated into larger applications for distance measure, image-based object detection, and other related fields.\n\n\n\n\n\n\t\tLimitations:\n\t\n\nPerformance Dependence on Training Data: The model's performance heavily relies on the quality, quantity, and diversity of the training data. Inaccuracies in object detection and distance estimation may arise when encountering object types, lighting conditions, or environments that significantly differ from the training data.\nComplex Object Arrangements: The model's accuracy may decrease when detecting objects within cluttered or complex scenes. It might struggle to accurately estimate distances for objects that are partially occluded or located behind other objects.\n\n\n\n\n\n\t\tBiases:\n\t\n\nTraining Data Bias: Biases present in the training data, such as object type distribution, camera viewpoints, and lighting conditions, could lead to differential performance across various scenarios. For instance, the model might exhibit better accuracy for object types more heavily represented in the training data.\n\n\n\n\n\n\t\tRisks:\n\t\n\nPrivacy Concerns: The model processes images, potentially capturing sensitive or private information. Deploying the model in contexts where privacy is a concern may inadvertently expose sensitive data, raising ethical and legal issues.\nSafety Considerations: Users should exercise caution when relying solely on the model's outputs for critical decision-making. The model may not account for all safety hazards, obstacles, or dynamic environmental changes that could impact real-time object detect.\n\n\n\n\n\n\t\tRecommendations\n\t\n\nUsers should be informed about the model's limitations and potential biases. Further testing and validation are advised for specific use cases to evaluate its performance accurately.\n Load model and perform prediction:\n\n\n\n\n\n\t\tHow to Get Started with the Model\n\t\n\nTo get started with the YOLOv8s object Detection and Classification model, follow these steps:\npip install ultralyticsplus==0.0.28 ultralytics==8.0.43\n\n\nLoad model and perform prediction:\n\n\nfrom ultralyticsplus import YOLO, render_result\n\n# load model\nmodel = YOLO('foduucom/thermal-image-object-detection')\n\n# set model parameters\nmodel.overrides['conf'] = 0.25  # NMS confidence threshold\nmodel.overrides['iou'] = 0.45  # NMS IoU threshold\nmodel.overrides['agnostic_nms'] = False  # NMS class-agnostic\nmodel.overrides['max_det'] = 1000  # maximum number of detections per image\n\n# set image\nimage = '/path/to/your/document/images'\n\n# perform inference\nresults = model.predict(image)\n\n# observe results\nprint(results[0].boxes)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()\n\n\n\n\n\n\n\t\tCompute Infrastructure\n\t\n\n\n\n\n\n\n\t\tHardware\n\t\n\nNVIDIA GeForce RTX 3060 card\n\n\n\n\n\n\t\tSoftware\n\t\n\nThe model was trained and fine-tuned using a Jupyter Notebook environment.\n\n\n\n\n\n\t\tModel Card Contact\n\t\n\nFor inquiries and contributions, please contact us at info@foduu.com.\n@ModelCard{\n  author    = {Nehul Agrawal and\n               Rahul parihar},\n  title     = {YOLOv8s thermal image Detection},\n  year      = {2023}\n}\n\n\n"
    },
    "Object-Detection-RetinaNet": {
        "Model Overview": "\n\n\n\n\n\n\t\tModel description\n\t\n\nImplementing RetinaNet: Focal Loss for Dense Object Detection.\nThis repo contains the model for the notebook Object Detection with RetinaNet\nHere the model is tasked with localizing the objects present in an image, and at the same time, classifying them into different categories. In this, RetinaNet has been implemented, a popular single-stage detector, which is accurate and runs fast. RetinaNet uses a feature pyramid network to efficiently detect objects at multiple scales and introduces a new loss, the Focal loss function, to alleviate the problem of the extreme foreground-background class imbalance.\nFull credits go to Srihari Humbarwadi\n\n\n\n\n\n\t\tReferences\n\t\n\n\nRetinaNet Paper\nFeature Pyramid Network Paper\n\n\n\n\n\n\n\t\tTraining and evaluation data\n\t\n\nThe dataset used here is a COCO2017 dataset \n\n\n\n\n\n\t\tTraining procedure\n\t\n\n\n\n\n\n\n\t\tTraining hyperparameters\n\t\n\nThe following hyperparameters were used during training:\n\n\n\nname\nlearning_rate\ndecay\nmomentum\nnesterov\ntraining_precision\n\n\nSGD\n{'class_name': 'PiecewiseConstantDecay', 'config': {'boundaries': [125, 250, 500, 240000, 360000], 'values': [2.5e-06, 0.000625, 0.00125, 0.0025, 0.00025, 2.5e-05], 'name': None}}\n0.0\n0.8999999761581421\nFalse\nfloat32\n\n\n\n\n\n\n\n\n\n\t\tModel Plot\n\t\n\n\nView Model Plot\n\n\n\nModel Reproduced By Kavya Bisht\n"
    }
}