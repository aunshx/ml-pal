{
    "gpt2": {
        "Model Overview": "GPT-2Test the whole generation capabilities here:https://transformer.huggingface.co/doc/gpt2-largePretrained model on English language using a causal language modeling (CLM) objective. It was introduced inthis paperand first released atthis page.Disclaimer: The team releasing GPT-2 also wrote amodel cardfor their model. Content from this model card\nhas been written by the Hugging Face team to complete the information they provided and give specific examples of bias.Model descriptionGPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. This\nmeans it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots\nof publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely,\nit was trained to guess the next word in sentences.More precisely, inputs are sequences of continuous text of a certain length and the targets are the same sequence,\nshifted one token (word or piece of word) to the right. The model uses internally a mask-mechanism to make sure the\npredictions for the tokenionly uses the inputs from1toibut not the future tokens.This way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks. The model is best at what it was pretrained for however, which is generating texts from a\nprompt.This is thesmallestversion of GPT-2, with 124M parameters.Related Models:GPT-Large,GPT-MediumandGPT-XLIntended uses & limitationsYou can use the raw model for text generation or fine-tune it to a downstream task. See themodel hubto look for fine-tuned versions on a task that interests you.How to useYou can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we\nset a seed for reproducibility:>>>fromtransformersimportpipeline, set_seed>>>generator = pipeline('text-generation', model='gpt2')>>>set_seed(42)>>>generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n\n[{'generated_text':\"Hello, I'm a language model, a language for thinking, a language for expressing thoughts.\"},\n {'generated_text':\"Hello, I'm a language model, a compiler, a compiler library, I just want to know how I build this kind of stuff. I don\"},\n {'generated_text':\"Hello, I'm a language model, and also have more than a few of your own, but I understand that they're going to need some help\"},\n {'generated_text':\"Hello, I'm a language model, a system model. I want to know my language so that it might be more interesting, more user-friendly\"},\n {'generated_text':'Hello, I\\'m a language model, not a language model\"\\n\\nThe concept of \"no-tricks\" comes in handy later with new'}]Here is how to use this model to get the features of a given text in PyTorch:fromtransformersimportGPT2Tokenizer, GPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2Model.from_pretrained('gpt2')\ntext =\"Replace me by any text you'd like.\"encoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)and in TensorFlow:fromtransformersimportGPT2Tokenizer, TFGPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = TFGPT2Model.from_pretrained('gpt2')\ntext =\"Replace me by any text you'd like.\"encoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)Limitations and biasThe training data used for this model has not been released as a dataset one can browse. We know it contains a lot of\nunfiltered content from the internet, which is far from neutral. As the openAI team themselves point out in theirmodel card:Because large-scale language models like GPT-2 do not distinguish fact from fiction, we don\u2019t support use-cases\nthat require the generated text to be true.Additionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so we do\nnot recommend that they be deployed into systems that interact with humans > unless the deployers first carry out a\nstudy of biases relevant to the intended use-case. We found no statistically significant difference in gender, race,\nand religious bias probes between 774M and 1.5B, implying all versions of GPT-2 should be approached with similar\nlevels of caution around use cases that are sensitive to biases around human attributes.Here's an example of how the model can have biased predictions:>>>fromtransformersimportpipeline, set_seed>>>generator = pipeline('text-generation', model='gpt2')>>>set_seed(42)>>>generator(\"The White man worked as a\", max_length=10, num_return_sequences=5)\n\n[{'generated_text':'The White man worked as a mannequin for'},\n {'generated_text':'The White man worked as a maniser of the'},\n {'generated_text':'The White man worked as a bus conductor by day'},\n {'generated_text':'The White man worked as a plumber at the'},\n {'generated_text':'The White man worked as a journalist. He had'}]>>>set_seed(42)>>>generator(\"The Black man worked as a\", max_length=10, num_return_sequences=5)\n\n[{'generated_text':'The Black man worked as a man at a restaurant'},\n {'generated_text':'The Black man worked as a car salesman in a'},\n {'generated_text':'The Black man worked as a police sergeant at the'},\n {'generated_text':'The Black man worked as a man-eating monster'},\n {'generated_text':'The Black man worked as a slave, and was'}]This bias will also affect all fine-tuned versions of this model.Training dataThe OpenAI team wanted to train this model on a corpus as large as possible. To build it, they scraped all the web\npages from outbound links on Reddit which received at least 3 karma. Note that all Wikipedia pages were removed from\nthis dataset, so the model was not trained on any part of Wikipedia. The resulting dataset (called WebText) weights\n40GB of texts but has not been publicly released. You can find a list of the top 1,000 domains present in WebTexthere.Training procedurePreprocessingThe texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a\nvocabulary size of 50,257. The inputs are sequences of 1024 consecutive tokens.The larger model was trained on 256 cloud TPU v3 cores. The training duration was not disclosed, nor were the exact\ndetails of training.Evaluation resultsThe model achieves the following results without any fine-tuning (zero-shot):DatasetLAMBADALAMBADACBT-CNCBT-NEWikiText2PTBenwiki8text8WikiText1031BW(metric)(PPL)(ACC)(ACC)(ACC)(PPL)(PPL)(BPB)(BPC)(PPL)(PPL)35.1345.9987.6583.429.4165.851.161,1737.5075.20BibTeX entry and citation info@article{radford2019language,\n  title={Language Models are Unsupervised Multitask Learners},\n  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},\n  year={2019}\n}"
    },
    "openai-gpt": {
        "Model Overview": "OpenAI GPT 1Table of ContentsModel DetailsHow To Get Started With the ModelUsesRisks, Limitations and BiasesTrainingEvaluationEnvironmental ImpactTechnical SpecificationsCitation InformationModel Card AuthorsModel DetailsModel Description:openai-gpt(a.k.a. \"GPT-1\") is the first transformer-based language model created and released by OpenAI. The model is a causal (unidirectional) transformer pre-trained using language modeling on a large corpus with long range dependencies.Developed by:Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever. Seeassociated research paperandGitHub repofor model developers and contributors.Model Type:Transformer-based language modelLanguage(s):EnglishLicense:MIT LicenseRelated Models:GPT2,GPT2-Medium,GPT2-LargeandGPT2-XLResources for more information:Research PaperOpenAI Blog PostGitHub RepoTest the full generation capabilities here:https://transformer.huggingface.co/doc/gptHow to Get Started with the ModelUse the code below to get started with the model. You can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we\nset a seed for reproducibility:>>>fromtransformersimportpipeline, set_seed>>>generator = pipeline('text-generation', model='openai-gpt')>>>set_seed(42)>>>generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n\n[{'generated_text':\"Hello, I'm a language model,'he said, when i was finished.'ah well,'said the man,'that's\"},\n {'generated_text':'Hello, I\\'m a language model, \" she said. \\n she reached the bottom of the shaft and leaned a little further out. it was'},\n {'generated_text':'Hello, I\\'m a language model, \" she laughed. \" we call that a\\'white girl.\\'or as we are called by the'},\n {'generated_text':'Hello, I\\'m a language model, \" said mr pin. \" an\\'the ones with the funny hats don\\'t. \" the rest of'},\n {'generated_text':'Hello, I\\'m a language model, was\\'ere \\'bout to do some more dancin \\', \" he said, then his voice lowered to'}]Here is how to use this model in PyTorch:fromtransformersimportOpenAIGPTTokenizer, OpenAIGPTModelimporttorch\n\ntokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")\nmodel = OpenAIGPTModel.from_pretrained(\"openai-gpt\")\n\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\noutputs = model(**inputs)\n\nlast_hidden_states = outputs.last_hidden_stateand in TensorFlow:fromtransformersimportOpenAIGPTTokenizer, TFOpenAIGPTModel\n\ntokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")\nmodel = TFOpenAIGPTModel.from_pretrained(\"openai-gpt\")\n\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\noutputs = model(inputs)\n\nlast_hidden_states = outputs.last_hidden_stateUsesDirect UseThis model can be used for language modeling tasks.Downstream UsePotential downstream uses of this model include tasks that leverage language models. In theassociated paper, the model developers discuss evaluations of the model for tasks including natural language inference (NLI), question answering, semantic similarity, and text classification.Misuse and Out-of-scope UseThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.Risks, Limitations and BiasesBiasesCONTENT WARNING: Readers should be aware that language generated by this model can be disturbing or offensive to some and can propagate historical and current stereotypes.Significant research has explored bias and fairness issues with language models (see, e.g.,Sheng et al. (2021)andBender et al. (2021)). \nPredictions generated by this model can include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups. For example:>>>fromtransformersimportpipeline, set_seed>>>generator = pipeline('text-generation', model='openai-gpt')>>>set_seed(42)>>>generator(\"The man worked as a\", max_length=10, num_return_sequences=5)\n\n[{'generated_text':'The man worked as a teacher for the college he'},\n {'generated_text':'The man worked as a janitor at the club.'},\n {'generated_text':'The man worked as a bodyguard in america. the'},\n {'generated_text':'The man worked as a clerk for one of the'},\n {'generated_text':'The man worked as a nurse, but there was'}]>>>set_seed(42)>>>generator(\"The woman worked as a\", max_length=10, num_return_sequences=5)\n\n[{'generated_text':'The woman worked as a medical intern but is a'},\n {'generated_text':'The woman worked as a midwife, i know that'},\n {'generated_text':'The woman worked as a prostitute in a sex club'},\n {'generated_text':'The woman worked as a secretary for one of the'},\n {'generated_text':'The woman worked as a nurse, but she had'}]This bias may also affect fine-tuned versions of this model. Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model.Risks and LimitationsThe model developers also wrote in ablog postabout risks and limitations of the model, including:Compute Requirements:Many previous approaches to NLP tasks train relatively small models on a single GPU from scratch. Our approach requires an expensive pre-training step - 1 month on 8 GPUs. Luckily, this only has to be done once and we\u2019re releasing our model so others can avoid it. It is also a large model (in comparison to prior work) and consequently uses more compute and memory \u2014 we used a 37-layer (12 block) Transformer architecture, and we train on sequences of up to 512 tokens. Most experiments were conducted on 4 and 8 GPU systems. The model does fine-tune to new tasks very quickly which helps mitigate the additional resource requirements.The limits and bias of learning about the world through text:Books and text readily available on the internet do not contain complete or even accurate information about the world. Recent work (Lucy and Gauthier, 2017) has shown that certain kinds of information are difficult to learn via just text and other work (Gururangan et al., 2018) has shown that models learn and exploit biases in data distributions.Still brittle generalization:Although our approach improves performance across a broad range of tasks, current deep learning NLP models still exhibit surprising and counterintuitive behavior - especially when evaluated in a systematic, adversarial, or out-of-distribution way. Our approach is not immune to these issues, though we have observed some indications of progress. Our approach shows improved lexical robustness over previous purely neural approaches to textual entailment. On the dataset introduced in Glockner et al. (2018) our model achieves 83.75%, performing similarly to KIM, which incorporates external knowledge via WordNet.TrainingTraining DataThe model developerswrite:We use the BooksCorpus dataset (Zhu et al., 2015) for training the language model. It contains over 7,000 unique unpublished books from a variety of genres including Adventure, Fantasy, and Romance. Crucially, it contains long stretches of contiguous text, which allows the generative model to learn to condition on long-range information.Training ProcedureThe model developerswrite:Our model largely follows the original transformer work [62]. We trained a 12-layer decoder-only transformer with masked self-attention heads (768 dimensional states and 12 attention heads). For the position-wise feed-forward networks, we used 3072 dimensional inner states. We used the Adam optimization scheme [27] with a max learning rate of 2.5e-4. The learning rate was increased linearly from zero over the first 2000 updates and annealed to 0 using a cosine schedule. We train for 100 epochs on minibatches of 64 randomly sampled, contiguous sequences of 512 tokens. Since layernorm [2] is used extensively throughout the model, a simple weight initialization of N (0, 0.02) was sufficient. We used a bytepair encoding (BPE) vocabulary with 40,000 merges [53] and residual, embedding, and attention dropouts with a rate of 0.1 for regularization. We also employed a modified version of L2 regularization proposed in [37], with w = 0.01 on all non bias or gain weights. For the activation function, we used the Gaussian Error Linear Unit (GELU) [18]. We used learned position embeddings instead of the sinusoidal version proposed in the original work. We use the ftfy library2 to clean the raw text in BooksCorpus, standardize some punctuation and whitespace, and use the spaCy tokenizer.See the paper for further details and links to citations.EvaluationThe following evaluation information is extracted from theassociated blog post. See theassociated paperfor further details.Testing Data, Factors and MetricsThe model developers report that the model was evaluated on the following tasks and datasets using the listed metrics:Task:Textual EntailmentDatasets:SNLI,MNLI Matched,MNLI Mismatched,SciTail,QNLI,RTEMetrics:AccuracyTask:Semantic SimilarityDatasets:STS-B,QQP,MRPCMetrics:AccuracyTask:Reading ComprehensionDatasets:RACEMetrics:AccuracyTask:Commonsense ReasoningDatasets:ROCStories,COPAMetrics:AccuracyTask:Sentiment AnalysisDatasets:SST-2Metrics:AccuracyTask:Linguistic AcceptabilityDatasets:CoLAMetrics:AccuracyTask:Multi Task BenchmarkDatasets:GLUEMetrics:AccuracyResultsThe model achieves the following results without any fine-tuning (zero-shot):TaskTETETETETETESSSSSSRCCRCRSALAMTBDatasetSNLIMNLI MatchedMNLI MismatchedSciTailQNLIRTESTS-BQQPMPRCRACEROCStoriesCOPASST-2CoLAGLUE89.982.181.488.388.156.082.070.382.359.086.578.691.345.472.8Environmental ImpactThe model developersreport that:The total compute used to train this model was 0.96 petaflop days (pfs-days).8 P600 GPU's * 30 days * 12 TFLOPS/GPU * 0.33 utilization = .96 pfs-daysCarbon emissions can be estimated using theMachine Learning Impact calculatorpresented inLacoste et al. (2019).Hardware Type:8 P600 GPUsHours used:720 hours (30 days)Cloud Provider:UnknownCompute Region:UnknownCarbon Emitted:UnknownTechnical SpecificationsSee theassociated paperfor details on the modeling architecture, objective, compute infrastructure, and training details.Citation Information@article{radford2018improving,\n  title={Improving language understanding by generative pre-training},\n  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},\n  year={2018},\n  publisher={OpenAI}\n}APA:Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving language understanding by generative pre-training.Model Card AuthorsThis model card was written by the Hugging Face team."
    },
    "Meta-Llama-3-8B": {
        "Model Overview": "Model DetailsMeta developed and released the Meta Llama 3 family of large language models (LLMs), a collection of pretrained and instruction tuned generative text models in 8 and 70B sizes. The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks. Further, in developing these models, we took great care to optimize helpfulness and safety.Model developersMetaVariationsLlama 3 comes in two sizes \u2014 8B and 70B parameters \u2014 in pre-trained and instruction tuned variants.InputModels input text only.OutputModels generate text and code only.Model ArchitectureLlama 3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.Training DataParamsContext lengthGQAToken countKnowledge cutoffLlama 3A new mix of publicly available online data.8B8kYes15T+March, 202370B8kYesDecember, 2023Llama 3 family of models. Token counts refer to pretraining data only. Both the 8 and 70B versions use Grouped-Query Attention (GQA) for improved inference scalability.Model Release DateApril 18, 2024.StatusThis is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.LicenseA custom commercial license is available at:https://llama.meta.com/llama3/licenseWhere to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the modelREADME. For more technical information about generation parameters and recipes for how to use Llama 3 in applications, please gohere.Intended UseIntended Use CasesLlama 3 is intended for commercial and research use in English. Instruction tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks.Out-of-scopeUse in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3 Community License. Use in languages other than English**.**Note: Developers may fine-tune Llama 3 models for languages beyond English provided they comply with the Llama 3 Community License and the Acceptable Use Policy.How to useThis repository contains two versions of Meta-Llama-3-8B, for use with transformers and with the originalllama3codebase.Use with transformersSee the snippet below for usage with Transformers:>>>importtransformers>>>importtorch>>>model_id =\"meta-llama/Meta-Llama-3-8B\">>>pipeline = transformers.pipeline(\"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\")>>>pipeline(\"Hey how are you doing today?\")Use withllama3Please, follow the instructions in therepository.To download Original checkpoints, see the example command below leveraginghuggingface-cli:huggingface-cli download meta-llama/Meta-Llama-3-8B --include \"original/*\" --local-dir Meta-Llama-3-8BFor Hugging Face support, we recommend using transformers or TGI, but a similar command works.Hardware and SoftwareTraining FactorsWe used custom training libraries, Meta's Research SuperCluster, and production clusters for pretraining. Fine-tuning, annotation, and evaluation were also performed on third-party cloud compute.Carbon Footprint Pretraining utilized a cumulative7.7M GPU hours of computation on hardware of type H100-80GB (TDP of 700W). Estimated total emissions were 2290 tCO2eq, 100% of which were offset by Meta\u2019s sustainability program.Time (GPU hours)Power Consumption (W)Carbon Emitted(tCO2eq)Llama 3 8B1.3M700390Llama 3 70B6.4M7001900Total7.7M2290CO2 emissions during pre-training. Time: total GPU time required for training each model. Power Consumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency. 100% of the emissions are directly offset by Meta's sustainability program, and because we are openly releasing these models, the pretraining costs do not need to be incurred by others.Training DataOverviewLlama 3 was pretrained on over 15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 10M human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data.Data FreshnessThe pretraining data has a cutoff of March 2023 for the 8B and December 2023 for the 70B models respectively.BenchmarksIn this section, we report the results for Llama 3 models on standard automatic benchmarks. For all the evaluations, we use our internal evaluations library. For details on the methodology seehere.Base pretrained modelsCategoryBenchmarkLlama 3 8BLlama2 7BLlama2 13BLlama 3 70BLlama2 70BGeneralMMLU (5-shot)66.645.753.879.569.7AGIEval English (3-5 shot)45.928.838.763.054.8CommonSenseQA (7-shot)72.657.667.683.878.7Winogrande (5-shot)76.173.375.483.181.8BIG-Bench Hard (3-shot, CoT)61.138.147.081.365.7ARC-Challenge (25-shot)78.653.767.693.085.3Knowledge reasoningTriviaQA-Wiki (5-shot)78.572.179.689.787.5Reading comprehensionSQuAD (1-shot)76.472.272.185.682.6QuAC (1-shot, F1)44.439.644.951.149.4BoolQ (0-shot)75.765.566.979.073.1DROP (3-shot, F1)58.437.949.879.770.2Instruction tuned modelsBenchmarkLlama 3 8BLlama 2 7BLlama 2 13BLlama 3 70BLlama 2 70BMMLU (5-shot)68.434.147.882.052.9GPQA (0-shot)34.221.722.339.521.0HumanEval (0-shot)62.27.914.081.725.6GSM-8K (8-shot, CoT)79.625.777.493.057.5MATH (4-shot, CoT)30.03.86.750.411.6Responsibility & SafetyWe believe that an open approach to AI leads to better, safer products, faster innovation, and a bigger overall market. We are committed to Responsible AI development and took a series of steps to limit misuse and harm and support the open source community.Foundation models are widely capable technologies that are built to be used for a diverse range of applications. They are not designed to meet every developer preference on safety levels for all use cases, out-of-the-box, as those by their nature will differ across different applications.Rather, responsible LLM-application deployment is achieved by implementing a series of safety best practices throughout the development of such applications, from the model pre-training, fine-tuning and the deployment of systems composed of safeguards to tailor the safety needs specifically to the use case and audience.As part of the Llama 3 release, we updated ourResponsible Use Guideto outline the steps and best practices for developers to implement model and system level safety for their application. We also provide a set of resources includingMeta Llama Guard 2andCode Shieldsafeguards. These tools have proven to drastically reduce residual risks of LLM Systems, while maintaining a high level of helpfulness. We encourage developers to tune and deploy these safeguards according to their needs and we provide areference implementationto get you started.Llama 3-InstructAs outlined in the Responsible Use Guide, some trade-off between model helpfulness and model alignment is likely unavoidable. Developers should exercise discretion about how to weigh the benefits of alignment and helpfulness for their specific use case and audience. Developers should be mindful of residual risks when using Llama models and leverage additional safety tools as needed to reach the right safety bar for their use case.SafetyFor our instruction tuned model, we conducted extensive red teaming exercises, performed adversarial evaluations and implemented safety mitigations techniques to lower residual risks. As with any Large Language Model, residual risks will likely remain and we recommend that developers assess these risks in the context of their use case. In parallel, we are working with the community to make AI safety benchmark standards transparent, rigorous and interpretable.RefusalsIn addition to residual risks, we put a great emphasis on model refusals to benign prompts. Over-refusing not only can impact the user experience but could even be harmful in certain contexts as well. We\u2019ve heard the feedback from the developer community and improved our fine tuning to ensure that Llama 3 is significantly less likely to falsely refuse to answer prompts than Llama 2.We built internal benchmarks and developed mitigations to limit false refusals making Llama 3 our most helpful model to date.Responsible releaseIn addition to responsible use considerations outlined above, we followed a rigorous process that requires us to take extra measures against misuse and critical risks before we make our release decision.MisuseIf you access or use Llama 3, you agree to the Acceptable Use Policy. The most recent copy of this policy can be found athttps://llama.meta.com/llama3/use-policy/.Critical risksCBRNE(Chemical, Biological, Radiological, Nuclear, and high yield Explosives)We have conducted a two fold assessment of the safety of the model in this area:Iterative testing during model training to assess the safety of responses related to CBRNE threats and other adversarial risks.Involving external CBRNE experts to conduct an uplift test assessing the ability of the model to accurately provide expert knowledge and reduce barriers to potential CBRNE misuse, by reference to what can be achieved using web search (without the model).Cyber SecurityWe have evaluated Llama 3 with CyberSecEval, Meta\u2019s cybersecurity safety eval suite, measuring Llama 3\u2019s propensity to suggest insecure code when used as a coding assistant, and Llama 3\u2019s propensity to comply with requests to help carry out cyber attacks, where attacks are defined by the industry standard MITRE ATT&CK cyber attack ontology. On our insecure coding and cyber attacker helpfulness tests, Llama 3 behaved in the same range or safer than models ofequivalent coding capability.Child SafetyChild Safety risk assessments were conducted using a team of experts, to assess the model\u2019s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences.CommunityGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership in AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to ourGithub repository.Finally, we put in place a set of resources including anoutput reporting mechanismandbug bounty programto continuously improve the Llama technology with the help of the community.Ethical Considerations and LimitationsThe core values of Llama 3 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.But Llama 3 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has been in English, and has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3\u2019s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3 models, developers should perform safety testing and tuning tailored to their specific applications of the model. As outlined in the Responsible Use Guide, we recommend incorporatingPurple Llamasolutions into your workflows and specificallyLlama Guardwhich provides a base model to filter input and output prompts to layer system-level safety on top of model-level safety.Please see the Responsible Use Guide available athttp://llama.meta.com/responsible-use-guideCitation instructions@article{llama3modelcard,title={Llama 3 Model Card},author={AI@Meta},year={2024},url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}}ContributorsAaditya Singh; Aaron Grattafiori; Abhimanyu Dubey; Abhinav Jauhri; Abhinav Pandey; Abhishek Kadian; Adam Kelsey; Adi Gangidi; Ahmad Al-Dahle; Ahuva Goldstand; Aiesha Letman; Ajay Menon; Akhil Mathur; Alan Schelten; Alex Vaughan; Amy Yang; Andrei Lupu; Andres Alvarado; Andrew Gallagher; Andrew Gu; Andrew Ho; Andrew Poulton; Andrew Ryan; Angela Fan; Ankit Ramchandani; Anthony Hartshorn; Archi Mitra; Archie Sravankumar; Artem Korenev; Arun Rao; Ashley Gabriel; Ashwin Bharambe; Assaf Eisenman; Aston Zhang; Aurelien Rodriguez; Austen Gregerson; Ava Spataru; Baptiste Roziere; Ben Maurer; Benjamin Leonhardi; Bernie Huang; Bhargavi Paranjape; Bing Liu; Binh Tang; Bobbie Chern; Brani Stojkovic; Brian Fuller; Catalina Mejia Arenas; Chao Zhou; Charlotte Caucheteux; Chaya Nayak; Ching-Hsiang Chu; Chloe Bi; Chris Cai; Chris Cox; Chris Marra; Chris McConnell; Christian Keller; Christoph Feichtenhofer; Christophe Touret; Chunyang Wu; Corinne Wong; Cristian Canton Ferrer; Damien Allonsius; Daniel Kreymer; Daniel Haziza; Daniel Li; Danielle Pintz; Danny Livshits; Danny Wyatt; David Adkins; David Esiobu; David Xu; Davide Testuggine; Delia David; Devi Parikh; Dhruv Choudhary; Dhruv Mahajan; Diana Liskovich; Diego Garcia-Olano; Diego Perino; Dieuwke Hupkes; Dingkang Wang; Dustin Holland; Egor Lakomkin; Elina Lobanova; Xiaoqing Ellen Tan; Emily Dinan; Eric Smith; Erik Brinkman; Esteban Arcaute; Filip Radenovic; Firat Ozgenel; Francesco Caggioni; Frank Seide; Frank Zhang; Gabriel Synnaeve; Gabriella Schwarz; Gabrielle Lee; Gada Badeer; Georgia Anderson; Graeme Nail; Gregoire Mialon; Guan Pang; Guillem Cucurell; Hailey Nguyen; Hannah Korevaar; Hannah Wang; Haroun Habeeb; Harrison Rudolph; Henry Aspegren; Hu Xu; Hugo Touvron; Iga Kozlowska; Igor Molybog; Igor Tufanov; Iliyan Zarov; Imanol Arrieta Ibarra; Irina-Elena Veliche; Isabel Kloumann; Ishan Misra; Ivan Evtimov; Jacob Xu; Jade Copet; Jake Weissman; Jan Geffert; Jana Vranes; Japhet Asher; Jason Park; Jay Mahadeokar; Jean-Baptiste Gaya; Jeet Shah; Jelmer van der Linde; Jennifer Chan; Jenny Hong; Jenya Lee; Jeremy Fu; Jeremy Teboul; Jianfeng Chi; Jianyu Huang; Jie Wang; Jiecao Yu; Joanna Bitton; Joe Spisak; Joelle Pineau; Jon Carvill; Jongsoo Park; Joseph Rocca; Joshua Johnstun; Junteng Jia; Kalyan Vasuden Alwala; Kam Hou U; Kate Plawiak; Kartikeya Upasani; Kaushik Veeraraghavan; Ke Li; Kenneth Heafield; Kevin Stone; Khalid El-Arini; Krithika Iyer; Kshitiz Malik; Kuenley Chiu; Kunal Bhalla; Kyle Huang; Lakshya Garg; Lauren Rantala-Yeary; Laurens van der Maaten; Lawrence Chen; Leandro Silva; Lee Bell; Lei Zhang; Liang Tan; Louis Martin; Lovish Madaan; Luca Wehrstedt; Lukas Blecher; Luke de Oliveira; Madeline Muzzi; Madian Khabsa; Manav Avlani; Mannat Singh; Manohar Paluri; Mark Zuckerberg; Marcin Kardas; Martynas Mankus; Mathew Oldham; Mathieu Rita; Matthew Lennie; Maya Pavlova; Meghan Keneally; Melanie Kambadur; Mihir Patel; Mikayel Samvelyan; Mike Clark; Mike Lewis; Min Si; Mitesh Kumar Singh; Mo Metanat; Mona Hassan; Naman Goyal; Narjes Torabi; Nicolas Usunier; Nikolay Bashlykov; Nikolay Bogoychev; Niladri Chatterji; Ning Dong; Oliver Aobo Yang; Olivier Duchenne; Onur Celebi; Parth Parekh; Patrick Alrassy; Paul Saab; Pavan Balaji; Pedro Rittner; Pengchuan Zhang; Pengwei Li; Petar Vasic; Peter Weng; Polina Zvyagina; Prajjwal Bhargava; Pratik Dubal; Praveen Krishnan; Punit Singh Koura; Qing He; Rachel Rodriguez; Ragavan Srinivasan; Rahul Mitra; Ramon Calderer; Raymond Li; Robert Stojnic; Roberta Raileanu; Robin Battey; Rocky Wang; Rohit Girdhar; Rohit Patel; Romain Sauvestre; Ronnie Polidoro; Roshan Sumbaly; Ross Taylor; Ruan Silva; Rui Hou; Rui Wang; Russ Howes; Ruty Rinott; Saghar Hosseini; Sai Jayesh Bondu; Samyak Datta; Sanjay Singh; Sara Chugh; Sargun Dhillon; Satadru Pan; Sean Bell; Sergey Edunov; Shaoliang Nie; Sharan Narang; Sharath Raparthy; Shaun Lindsay; Sheng Feng; Sheng Shen; Shenghao Lin; Shiva Shankar; Shruti Bhosale; Shun Zhang; Simon Vandenhende; Sinong Wang; Seohyun Sonia Kim; Soumya Batra; Sten Sootla; Steve Kehoe; Suchin Gururangan; Sumit Gupta; Sunny Virk; Sydney Borodinsky; Tamar Glaser; Tamar Herman; Tamara Best; Tara Fowler; Thomas Georgiou; Thomas Scialom; Tianhe Li; Todor Mihaylov; Tong Xiao; Ujjwal Karn; Vedanuj Goswami; Vibhor Gupta; Vignesh Ramanathan; Viktor Kerkez; Vinay Satish Kumar; Vincent Gonguet; Vish Vogeti; Vlad Poenaru; Vlad Tiberiu Mihailescu; Vladan Petrovic; Vladimir Ivanov; Wei Li; Weiwei Chu; Wenhan Xiong; Wenyin Fu; Wes Bouaziz; Whitney Meers; Will Constable; Xavier Martinet; Xiaojian Wu; Xinbo Gao; Xinfeng Xie; Xuchao Jia; Yaelle Goldschlag; Yann LeCun; Yashesh Gaur; Yasmine Babaei; Ye Qi; Yenda Li; Yi Wen; Yiwen Song; Youngjin Nam; Yuchen Hao; Yuchen Zhang; Yun Wang; Yuning Mao; Yuzi He; Zacharie Delpierre Coudert; Zachary DeVito; Zahra Hankir; Zhaoduo Wen; Zheng Yan; Zhengxing Chen; Zhenyu Yang; Zoe Papakipos"
    },
    "Llama-2-7b": {
        "Model Overview": "Llama 2Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B pretrained model. Links to other models can be found in the index at the bottom.Model DetailsNote: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit thewebsiteand accept our License before requesting access here.Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.Model DevelopersMetaVariationsLlama 2 comes in a range of parameter sizes \u2014 7B, 13B, and 70B \u2014 as well as pretrained and fine-tuned variations.InputModels input text only.OutputModels generate text only.Model ArchitectureLlama 2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align to human preferences for helpfulness and safety.Training DataParamsContent LengthGQATokensLRLlama 2A new mix of publicly available online data7B4k\u27172.0T3.0 x 10-4Llama 2A new mix of publicly available online data13B4k\u27172.0T3.0 x 10-4Llama 2A new mix of publicly available online data70B4k\u27142.0T1.5 x 10-4Llama 2 family of models.Token counts refer to pretraining data only. All models are trained with a global batch-size of 4M tokens. Bigger models -  70B -- use Grouped-Query Attention (GQA) for improved inference scalability.Model DatesLlama 2 was trained between January 2023 and July 2023.StatusThis is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.LicenseA custom commercial license is available at:https://ai.meta.com/resources/models-and-libraries/llama-downloads/Research Paper\"Llama-2: Open Foundation and Fine-tuned Chat Models\"Intended UseIntended Use CasesLlama 2 is intended for commercial and research use in English. Tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks.To get the expected features and performance for the chat versions, a specific formatting needs to be followed, including theINSTand<<SYS>>tags,BOSandEOStokens, and the whitespaces and breaklines in between (we recommend callingstrip()on inputs to avoid double-spaces). See our reference code in github for details:chat_completion.Out-of-scope UsesUse in any manner that violates applicable laws or regulations (including trade compliance laws).Use in languages other than English. Use in any other way that is prohibited by the Acceptable Use Policy and Licensing Agreement for Llama 2.Hardware and SoftwareTraining FactorsWe used custom training libraries, Meta's Research Super Cluster, and production clusters for pretraining. Fine-tuning, annotation, and evaluation were also performed on third-party cloud compute.Carbon FootprintPretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB (TDP of 350-400W). Estimated total emissions were 539 tCO2eq, 100% of which were offset by Meta\u2019s sustainability program.Time (GPU hours)Power Consumption (W)Carbon Emitted(tCO2eq)Llama 2 7B18432040031.22Llama 2 13B36864040062.44Llama 2 70B1720320400291.42Total3311616539.00CO2emissions during pretraining.Time: total GPU time required for training each model. Power Consumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency. 100% of the emissions are directly offset by Meta's sustainability program, and because we are openly releasing these models, the pretraining costs do not need to be incurred by others.Training DataOverviewLlama 2 was pretrained on 2 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over one million new human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data.Data FreshnessThe pretraining data has a cutoff of September 2022, but some tuning data is more recent, up to July 2023.Evaluation ResultsIn this section, we report the results for the Llama 1 and Llama 2 models on standard academic benchmarks.For all the evaluations, we use our internal evaluations library.ModelSizeCodeCommonsense ReasoningWorld KnowledgeReading ComprehensionMathMMLUBBHAGI EvalLlama 17B14.160.846.258.56.9535.130.323.9Llama 113B18.966.152.662.310.946.937.033.9Llama 133B26.070.058.467.621.457.839.841.7Llama 165B30.770.760.568.630.863.443.547.6Llama 27B16.863.948.961.314.645.332.629.3Llama 213B24.566.955.465.828.754.839.439.1Llama 270B37.571.963.669.435.268.951.254.2Overall performance on grouped academic benchmarks.Code:We report the average pass@1 scores of our models on HumanEval and MBPP.Commonsense Reasoning:We report the average of PIQA, SIQA, HellaSwag, WinoGrande, ARC easy and challenge, OpenBookQA, and CommonsenseQA. We report 7-shot results for CommonSenseQA and 0-shot results for all other benchmarks.World Knowledge:We evaluate the 5-shot performance on NaturalQuestions and TriviaQA and report the average.Reading Comprehension:For reading comprehension, we report the 0-shot average on SQuAD, QuAC, and BoolQ.MATH:We report the average of the GSM8K (8 shot) and MATH (4 shot) benchmarks at top 1.TruthfulQAToxigenLlama 17B27.4223.00Llama 113B41.7423.08Llama 133B44.1922.57Llama 165B48.7121.77Llama 27B33.2921.25Llama 213B41.8626.10Llama 270B50.1824.60Evaluation of pretrained LLMs on automatic safety benchmarks.For TruthfulQA, we present the percentage of generations that are both truthful and informative (the higher the better). For ToxiGen, we present the percentage of toxic generations (the smaller the better).TruthfulQAToxigenLlama-2-Chat7B57.040.00Llama-2-Chat13B62.180.00Llama-2-Chat70B64.140.01Evaluation of fine-tuned LLMs on different safety datasets.Same metric definitions as above.Ethical Considerations and LimitationsLlama 2 is a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Llama 2\u2019s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 2, developers should perform safety testing and tuning tailored to their specific applications of the model.Please see the Responsible Use Guide available athttps://ai.meta.com/llama/responsible-use-guide/Reporting IssuesPlease report any software \u201cbug,\u201d or other problems with the models through one of the following means:Reporting issues with the model:github.com/facebookresearch/llamaReporting problematic content generated by the model:developers.facebook.com/llama_output_feedbackReporting bugs and security concerns:facebook.com/whitehat/infoLlama Model IndexModelLlama2Llama2-hfLlama2-chatLlama2-chat-hf7BLinkLinkLinkLink13BLinkLinkLinkLink70BLinkLinkLinkLink"
    },
    "CodeLlama-70b-Instruct-hf": {
        "Model Overview": "Code LlamaCode Llama is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 70B instruct-tuned version in the Hugging Face Transformers format. This model is designed for general code synthesis and understanding. Links to other models can be found in the index at the bottom.Base ModelPythonInstruct7Bmeta-llama/CodeLlama-7b-hfmeta-llama/CodeLlama-7b-Python-hfmeta-llama/CodeLlama-7b-Instruct-hf13Bmeta-llama/CodeLlama-13b-hfmeta-llama/CodeLlama-13b-Python-hfmeta-llama/CodeLlama-13b-Instruct-hf34Bmeta-llama/CodeLlama-34b-hfmeta-llama/CodeLlama-34b-Python-hfmeta-llama/CodeLlama-34b-Instruct-hf70Bmeta-llama/CodeLlama-70b-hfmeta-llama/CodeLlama-70b-Python-hfmeta-llama/CodeLlama-70b-Instruct-hfModel capabilities:Code completion.Infilling.Instructions / chat.Python specialist.Model UseTo use this model, please make sure to install transformers:pip install transformers accelerateChat use:The 70B Instruct model uses adifferent prompt templatethan the smaller versions. To use it withtransformers, we recommend you use the built-in chat template:fromtransformersimportAutoTokenizer, AutoModelForCausalLMimporttransformersimporttorch\n\nmodel_id =\"meta-llama/CodeLlama-70b-Instruct-hf\"tokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n   model_id,\n   torch_dtype=torch.float16,\n   device_map=\"auto\",\n)\n\nchat = [\n   {\"role\":\"system\",\"content\":\"You are a helpful and honest code assistant expert in JavaScript. Please, provide all answers to programming questions in JavaScript\"},\n   {\"role\":\"user\",\"content\":\"Write a function that computes the set of sums of all contiguous sublists of a given list.\"},\n]\ninputs = tokenizer.apply_chat_template(chat, return_tensors=\"pt\").to(\"cuda\")\n\noutput = model.generate(input_ids=inputs, max_new_tokens=200)\noutput = output[0].to(\"cpu\")print(tokenizer.decode(output))You can also use the model fortext or code completion. This examples uses transformers'pipelineinterface:fromtransformersimportAutoTokenizerimporttransformersimporttorch\n\nmodel_id =\"meta-llama/CodeLlama-70b-hf\"tokenizer = AutoTokenizer.from_pretrained(model_id)\npipeline = transformers.pipeline(\"text-generation\",\n   model=model_id,\n   torch_dtype=torch.float16,\n   device_map=\"auto\",\n)\n\nsequences = pipeline('def fibonacci(',\n   do_sample=True,\n   temperature=0.2,\n   top_p=0.9,\n   num_return_sequences=1,\n   eos_token_id=tokenizer.eos_token_id,\n   max_length=100,\n)forseqinsequences:print(f\"Result:{seq['generated_text']}\")Chat promptCodeLlama 70B Instruct uses a different format for the chat prompt than previous Llama 2 or CodeLlama models. As mentioned above, the easiest way to use it is with the help of the tokenizer's chat template. If you need to build the string or tokens, manually, here's how to do it.We'll do our tests with the following made-up dialog:chat = [\n    {\"role\":\"system\",\"content\":\"System prompt    \"},\n    {\"role\":\"user\",\"content\":\"First user query\"},\n    {\"role\":\"assistant\",\"content\":\"Model response to first query\"},\n    {\"role\":\"user\",\"content\":\"Second user query\"},\n]First, let's see what the prompt looks like if we use the chat template:tokenizer.apply_chat_template(chat, tokenize=False)'<s>Source: system\\n\\n System prompt <step> Source: user\\n\\n First user query <step> Source: assistant\\n\\n Model response to first query <step> Source: user\\n\\n Second user query <step> Source: assistant\\nDestination: user\\n\\n 'So each turn of the conversation has aSource(system,user, orassistant), and then the content appears after two newlines and a space. Turns are separated with the special token<step>. After the last turn (which must necessarily come from theuser), we invite the model to respond by using the special syntaxSource: assistant\\nDestination: user\\n\\n. Let's see how we can build the same string ourselves:output =\"<s>\"forminchat:\n    output +=f\"Source:{m['role']}\\n\\n{m['content'].strip()}\"output +=\" <step> \"output +=\"Source: assistant\\nDestination: user\\n\\n \"output'<s>Source: system\\n\\n System prompt <step> Source: user\\n\\n First user query <step> Source: assistant\\n\\n Model response to first query <step> Source: user\\n\\n Second user query <step> Source: assistant\\nDestination: user\\n\\n 'To verify that we got it right, we'll compare against thereference code in the original GitHub repo. We used the same dialog and tokenized it with thedialog_prompt_tokensfunction and got the following tokens:reference_tokens = [1,7562,29901,1788,13,13,2184,9508,32015,7562,29901,1404,13,13,3824,1404,2346,32015,7562,29901,20255,13,13,8125,2933,304,937,2346,32015,7562,29901,1404,13,13,6440,1404,2346,32015,7562,29901,20255,13,14994,3381,29901,1404,13,13,29871]Let's see what we get with the string we built using our Python loop. Note that we don't add \"special tokens\" because the string already starts with<s>, the beginning of sentence token:tokens = tokenizer.encode(output, add_special_tokens=False)assertreference_tokens == tokensSimilarly, let's verify that the chat template produces the same token sequence:assertreference_tokens == tokenizer.apply_chat_template(chat)As a final detail, please note that if the dialog does not start with asystemturn, theoriginal code will insert one with an empty content string.Model Details*Note: Use of this model is governed by the Meta license. Meta developed and publicly released the Code Llama family of large language models (LLMs).Model DevelopersMetaVariationsCode Llama comes in four model sizes, and three variants:Code Llama: base models designed for general code synthesis and understandingCode Llama - Python: designed specifically for PythonCode Llama - Instruct: for instruction following and safer deploymentAll variants are available in sizes of 7B, 13B, 34B, and 70B parameters.This repository contains the Instruct version of the 70B parameters model.InputModels input text only.OutputModels generate text only.Model ArchitectureCode Llama is an auto-regressive language model that uses an optimized transformer architecture. It was fine-tuned with up to 16k tokens. This variantdoes notsupport long context of up to 100k tokens.Model DatesCode Llama and its variants have been trained between January 2023 and January 2024.StatusThis is a static model trained on an offline dataset. Future versions of Code Llama - Instruct will be released as we improve model safety with community feedback.LicenseA custom commercial license is available at:https://ai.meta.com/resources/models-and-libraries/llama-downloads/Research PaperMore information can be found in the paper \"Code Llama: Open Foundation Models for Code\" or itsarXiv page.Intended UseIntended Use CasesCode Llama and its variants are intended for commercial and research use in English and relevant programming languages. The base model Code Llama can be adapted for a variety of code synthesis and understanding tasks, Code Llama - Python is designed specifically to handle the Python programming language, and Code Llama - Instruct is intended to be safer to use for code assistant and generation applications.Out-of-Scope UsesUse in any manner that violates applicable laws or regulations (including trade compliance laws). Use in languages other than English. Use in any other way that is prohibited by the Acceptable Use Policy and Licensing Agreement for Code Llama and its variants.Hardware and SoftwareTraining FactorsWe used custom training libraries. The training and fine-tuning of the released models have been performed Meta\u2019s Research Super Cluster.Carbon FootprintIn aggregate, training all 12 Code Llama models required 1400K GPU hours of computation on hardware of type A100-80GB (TDP of 350-400W). Estimated total emissions were 228.55 tCO2eq, 100% of which were offset by Meta\u2019s sustainability program.Evaluation ResultsSee evaluations for the main models and detailed ablations in Section 3 and safety evaluations in Section 4 of the research paper.Ethical Considerations and LimitationsCode Llama and its variants are a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Code Llama\u2019s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate or objectionable responses to user prompts. Therefore, before deploying any applications of Code Llama, developers should perform safety testing and tuning tailored to their specific applications of the model.Please see the Responsible Use Guide available available athttps://ai.meta.com/llama/responsible-use-guide."
    },
    "bert-base-uncased": {
        "Model Overview": "BERT base model (uncased)Pretrained model on English language using a masked language modeling (MLM) objective. It was introduced inthis paperand first released inthis repository. This model is uncased: it does not make a difference\nbetween english and English.Disclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by\nthe Hugging Face team.Model descriptionBERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it\nwas pretrained on the raw texts only, with no humans labeling them in any way (which is why it can use lots of\npublicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it\nwas pretrained with two objectives:Masked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run\nthe entire masked sentence through the model and has to predict the masked words. This is different from traditional\nrecurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like\nGPT which internally masks the future tokens. It allows the model to learn a bidirectional representation of the\nsentence.Next sentence prediction (NSP): the models concatenates two masked sentences as inputs during pretraining. Sometimes\nthey correspond to sentences that were next to each other in the original text, sometimes not. The model then has to\npredict if the two sentences were following each other or not.This way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks: if you have a dataset of labeled sentences, for instance, you can train a standard\nclassifier using the features produced by the BERT model as inputs.Model variationsBERT has originally been released in base and large variations, for cased and uncased input text. The uncased models also strips out an accent markers.Chinese and multilingual uncased and cased versions followed shortly after.Modified preprocessing with whole word masking has replaced subpiece masking in a following work, with the release of two models.Other 24 smaller models are released afterward.The detailed release history can be found on thegoogle-research/bert readmeon github.Model#paramsLanguagebert-base-uncased110MEnglishbert-large-uncased340MEnglishbert-base-cased110MEnglishbert-large-cased340MEnglishbert-base-chinese110MChinesebert-base-multilingual-cased110MMultiplebert-large-uncased-whole-word-masking340MEnglishbert-large-cased-whole-word-masking340MEnglishIntended uses & limitationsYou can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to\nbe fine-tuned on a downstream task. See themodel hubto look for\nfine-tuned versions of a task that interests you.Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at model like GPT2.How to useYou can use this model directly with a pipeline for masked language modeling:>>>fromtransformersimportpipeline>>>unmasker = pipeline('fill-mask', model='bert-base-uncased')>>>unmasker(\"Hello I'm a [MASK] model.\")\n\n[{'sequence':\"[CLS] hello i'm a fashion model. [SEP]\",'score':0.1073106899857521,'token':4827,'token_str':'fashion'},\n {'sequence':\"[CLS] hello i'm a role model. [SEP]\",'score':0.08774490654468536,'token':2535,'token_str':'role'},\n {'sequence':\"[CLS] hello i'm a new model. [SEP]\",'score':0.05338378623127937,'token':2047,'token_str':'new'},\n {'sequence':\"[CLS] hello i'm a super model. [SEP]\",'score':0.04667217284440994,'token':3565,'token_str':'super'},\n {'sequence':\"[CLS] hello i'm a fine model. [SEP]\",'score':0.027095865458250046,'token':2986,'token_str':'fine'}]Here is how to use this model to get the features of a given text in PyTorch:fromtransformersimportBertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\ntext =\"Replace me by any text you'd like.\"encoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)and in TensorFlow:fromtransformersimportBertTokenizer, TFBertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = TFBertModel.from_pretrained(\"bert-base-uncased\")\ntext =\"Replace me by any text you'd like.\"encoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)Limitations and biasEven if the training data used for this model could be characterized as fairly neutral, this model can have biased\npredictions:>>>fromtransformersimportpipeline>>>unmasker = pipeline('fill-mask', model='bert-base-uncased')>>>unmasker(\"The man worked as a [MASK].\")\n\n[{'sequence':'[CLS] the man worked as a carpenter. [SEP]','score':0.09747550636529922,'token':10533,'token_str':'carpenter'},\n {'sequence':'[CLS] the man worked as a waiter. [SEP]','score':0.0523831807076931,'token':15610,'token_str':'waiter'},\n {'sequence':'[CLS] the man worked as a barber. [SEP]','score':0.04962705448269844,'token':13362,'token_str':'barber'},\n {'sequence':'[CLS] the man worked as a mechanic. [SEP]','score':0.03788609802722931,'token':15893,'token_str':'mechanic'},\n {'sequence':'[CLS] the man worked as a salesman. [SEP]','score':0.037680890411138535,'token':18968,'token_str':'salesman'}]>>>unmasker(\"The woman worked as a [MASK].\")\n\n[{'sequence':'[CLS] the woman worked as a nurse. [SEP]','score':0.21981462836265564,'token':6821,'token_str':'nurse'},\n {'sequence':'[CLS] the woman worked as a waitress. [SEP]','score':0.1597415804862976,'token':13877,'token_str':'waitress'},\n {'sequence':'[CLS] the woman worked as a maid. [SEP]','score':0.1154729500412941,'token':10850,'token_str':'maid'},\n {'sequence':'[CLS] the woman worked as a prostitute. [SEP]','score':0.037968918681144714,'token':19215,'token_str':'prostitute'},\n {'sequence':'[CLS] the woman worked as a cook. [SEP]','score':0.03042375110089779,'token':5660,'token_str':'cook'}]This bias will also affect all fine-tuned versions of this model.Training dataThe BERT model was pretrained onBookCorpus, a dataset consisting of 11,038\nunpublished books andEnglish Wikipedia(excluding lists, tables and\nheaders).Training procedurePreprocessingThe texts are lowercased and tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are\nthen of the form:[CLS] Sentence A [SEP] Sentence B [SEP]With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus, and in\nthe other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a\nconsecutive span of text usually longer than a single sentence. The only constrain is that the result with the two\n\"sentences\" has a combined length of less than 512 tokens.The details of the masking procedure for each sentence are the following:15% of the tokens are masked.In 80% of the cases, the masked tokens are replaced by[MASK].In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.In the 10% remaining cases, the masked tokens are left as is.PretrainingThe model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size\nof 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer\nused is Adam with a learning rate of 1e-4,\u03b21=0.9\\beta_{1} = 0.9\u03b21\u200b=0.9and\u03b22=0.999\\beta_{2} = 0.999\u03b22\u200b=0.999, a weight decay of 0.01,\nlearning rate warmup for 10,000 steps and linear decay of the learning rate after.Evaluation resultsWhen fine-tuned on downstream tasks, this model achieves the following results:Glue test results:TaskMNLI-(m/mm)QQPQNLISST-2CoLASTS-BMRPCRTEAverage84.6/83.471.290.593.552.185.888.966.479.6BibTeX entry and citation info@article{DBLP:journals/corr/abs-1810-04805,\n  author    = {Jacob Devlin and\n               Ming{-}Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova},\n  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\n               Understanding},\n  journal   = {CoRR},\n  volume    = {abs/1810.04805},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1810.04805},\n  archivePrefix = {arXiv},\n  eprint    = {1810.04805},\n  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}"
    },
    "roberta-base": {
        "Model Overview": "RoBERTa base modelPretrained model on English language using a masked language modeling (MLM) objective. It was introduced inthis paperand first released inthis repository. This model is case-sensitive: it\nmakes a difference between english and English.Disclaimer: The team releasing RoBERTa did not write a model card for this model so this model card has been written by\nthe Hugging Face team.Model descriptionRoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means\nit was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of\npublicly available data) with an automatic process to generate inputs and labels from those texts.More precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model\nrandomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict\nthe masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one\nafter the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to\nlearn a bidirectional representation of the sentence.This way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard\nclassifier using the features produced by the BERT model as inputs.Intended uses & limitationsYou can use the raw model for masked language modeling, but it's mostly intended to be fine-tuned on a downstream task.\nSee themodel hubto look for fine-tuned versions on a task that\ninterests you.Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at a model like GPT2.How to useYou can use this model directly with a pipeline for masked language modeling:>>>fromtransformersimportpipeline>>>unmasker = pipeline('fill-mask', model='roberta-base')>>>unmasker(\"Hello I'm a <mask> model.\")\n\n[{'sequence':\"<s>Hello I'm a male model.</s>\",'score':0.3306540250778198,'token':2943,'token_str':'\u0120male'},\n {'sequence':\"<s>Hello I'm a female model.</s>\",'score':0.04655390977859497,'token':2182,'token_str':'\u0120female'},\n {'sequence':\"<s>Hello I'm a professional model.</s>\",'score':0.04232972860336304,'token':2038,'token_str':'\u0120professional'},\n {'sequence':\"<s>Hello I'm a fashion model.</s>\",'score':0.037216778844594955,'token':2734,'token_str':'\u0120fashion'},\n {'sequence':\"<s>Hello I'm a Russian model.</s>\",'score':0.03253649175167084,'token':1083,'token_str':'\u0120Russian'}]Here is how to use this model to get the features of a given text in PyTorch:fromtransformersimportRobertaTokenizer, RobertaModel\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nmodel = RobertaModel.from_pretrained('roberta-base')\ntext =\"Replace me by any text you'd like.\"encoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)and in TensorFlow:fromtransformersimportRobertaTokenizer, TFRobertaModel\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nmodel = TFRobertaModel.from_pretrained('roberta-base')\ntext =\"Replace me by any text you'd like.\"encoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)Limitations and biasThe training data used for this model contains a lot of unfiltered content from the internet, which is far from\nneutral. Therefore, the model can have biased predictions:>>>fromtransformersimportpipeline>>>unmasker = pipeline('fill-mask', model='roberta-base')>>>unmasker(\"The man worked as a <mask>.\")\n\n[{'sequence':'<s>The man worked as a mechanic.</s>','score':0.08702439814805984,'token':25682,'token_str':'\u0120mechanic'},\n {'sequence':'<s>The man worked as a waiter.</s>','score':0.0819653645157814,'token':38233,'token_str':'\u0120waiter'},\n {'sequence':'<s>The man worked as a butcher.</s>','score':0.073323555290699,'token':32364,'token_str':'\u0120butcher'},\n {'sequence':'<s>The man worked as a miner.</s>','score':0.046322137117385864,'token':18678,'token_str':'\u0120miner'},\n {'sequence':'<s>The man worked as a guard.</s>','score':0.040150221437215805,'token':2510,'token_str':'\u0120guard'}]>>>unmasker(\"The Black woman worked as a <mask>.\")\n\n[{'sequence':'<s>The Black woman worked as a waitress.</s>','score':0.22177888453006744,'token':35698,'token_str':'\u0120waitress'},\n {'sequence':'<s>The Black woman worked as a prostitute.</s>','score':0.19288744032382965,'token':36289,'token_str':'\u0120prostitute'},\n {'sequence':'<s>The Black woman worked as a maid.</s>','score':0.06498628109693527,'token':29754,'token_str':'\u0120maid'},\n {'sequence':'<s>The Black woman worked as a secretary.</s>','score':0.05375480651855469,'token':2971,'token_str':'\u0120secretary'},\n {'sequence':'<s>The Black woman worked as a nurse.</s>','score':0.05245552211999893,'token':9008,'token_str':'\u0120nurse'}]This bias will also affect all fine-tuned versions of this model.Training dataThe RoBERTa model was pretrained on the reunion of five datasets:BookCorpus, a dataset consisting of 11,038 unpublished books;English Wikipedia(excluding lists, tables and headers) ;CC-News, a dataset containing 63 millions English news\narticles crawled between September 2016 and February 2019.OpenWebText, an opensource recreation of the WebText dataset used to\ntrain GPT-2,Storiesa dataset containing a subset of CommonCrawl data filtered to match the\nstory-like style of Winograd schemas.Together these datasets weigh 160GB of text.Training procedurePreprocessingThe texts are tokenized using a byte version of Byte-Pair Encoding (BPE) and a vocabulary size of 50,000. The inputs of\nthe model take pieces of 512 contiguous tokens that may span over documents. The beginning of a new document is marked\nwith<s>and the end of one by</s>The details of the masking procedure for each sentence are the following:15% of the tokens are masked.In 80% of the cases, the masked tokens are replaced by<mask>.In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.In the 10% remaining cases, the masked tokens are left as is.Contrary to BERT, the masking is done dynamically during pretraining (e.g., it changes at each epoch and is not fixed).PretrainingThe model was trained on 1024 V100 GPUs for 500K steps with a batch size of 8K and a sequence length of 512. The\noptimizer used is Adam with a learning rate of 6e-4,\u03b21=0.9\\beta_{1} = 0.9\u03b21\u200b=0.9,\u03b22=0.98\\beta_{2} = 0.98\u03b22\u200b=0.98and\u03f5=1e\u22126\\epsilon = 1e-6\u03f5=1e\u22126, a weight decay of 0.01, learning rate warmup for 24,000 steps and linear decay of the learning\nrate after.Evaluation resultsWhen fine-tuned on downstream tasks, this model achieves the following results:Glue test results:TaskMNLIQQPQNLISST-2CoLASTS-BMRPCRTE87.691.992.894.863.691.290.278.7BibTeX entry and citation info@article{DBLP:journals/corr/abs-1907-11692,\n  author    = {Yinhan Liu and\n               Myle Ott and\n               Naman Goyal and\n               Jingfei Du and\n               Mandar Joshi and\n               Danqi Chen and\n               Omer Levy and\n               Mike Lewis and\n               Luke Zettlemoyer and\n               Veselin Stoyanov},\n  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},\n  journal   = {CoRR},\n  volume    = {abs/1907.11692},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1907.11692},\n  archivePrefix = {arXiv},\n  eprint    = {1907.11692},\n  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}"
    },
    "t5-base": {
        "Model Overview": "Model Card for T5 BaseTable of ContentsModel DetailsUsesBias, Risks, and LimitationsTraining DetailsEvaluationEnvironmental ImpactCitationModel Card AuthorsHow To Get Started With the ModelModel DetailsModel DescriptionThe developers of the Text-To-Text Transfer Transformer (T5)write:With T5, we propose reframing all NLP tasks into a unified text-to-text-format where the input and output are always text strings, in contrast to BERT-style models that can only output either a class label or a span of the input. Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task.T5-Base is the checkpoint with 220 million parameters.Developed by:Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. Seeassociated paperandGitHub repoModel type:Language modelLanguage(s) (NLP):English, French, Romanian, GermanLicense:Apache 2.0Related Models:All T5 CheckpointsResources for more information:Research paperGoogle's T5 Blog PostGitHub RepoHugging Face T5 DocsUsesDirect Use and Downstream UseThe developers write in ablog postthat the model:Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task, including machine translation, document summarization, question answering, and classification tasks (e.g., sentiment analysis). We can even apply T5 to regression tasks by training it to predict the string representation of a number instead of the number itself.See theblog postandresearch paperfor further details.Out-of-Scope UseMore information needed.Bias, Risks, and LimitationsMore information needed.RecommendationsMore information needed.Training DetailsTraining DataThe model is pre-trained on theColossal Clean Crawled Corpus (C4), which was developed and released in the context of the sameresearch paperas T5.The model was pre-trained on a on amulti-task mixture of unsupervised (1.) and supervised tasks (2.).\nThereby, the following datasets were being used for (1.) and (2.):Datasets used for Unsupervised denoising objective:C4Wiki-DPRDatasets used for Supervised text-to-text language modeling objectiveSentence acceptability judgmentCoLAWarstadt et al., 2018Sentiment analysisSST-2Socher et al., 2013Paraphrasing/sentence similarityMRPCDolan and Brockett, 2005STS-BCeret al., 2017QQPIyer et al., 2017Natural language inferenceMNLIWilliams et al., 2017QNLIRajpurkar et al.,2016RTEDagan et al., 2005CBDe Marneff et al., 2019Sentence completionCOPARoemmele et al., 2011Word sense disambiguationWICPilehvar and Camacho-Collados, 2018Question answeringMultiRCKhashabi et al., 2018ReCoRDZhang et al., 2018BoolQClark et al., 2019Training ProcedureIn theirabstract, the model developers write:In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks.The framework introduced, the T5 framework, involves a training procedure that brings together the approaches studied in the paper. See theresearch paperfor further details.EvaluationTesting Data, Factors & MetricsThe developers evaluated the model on 24 tasks, see theresearch paperfor full details.ResultsFor full results for T5-Base, see theresearch paper, Table 14.Environmental ImpactCarbon emissions can be estimated using theMachine Learning Impact calculatorpresented inLacoste et al. (2019).Hardware Type:Google Cloud TPU PodsHours used:More information neededCloud Provider:GCPCompute Region:More information neededCarbon Emitted:More information neededCitationBibTeX:@article{2020t5,\n  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},\n  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},\n  journal = {Journal of Machine Learning Research},\n  year    = {2020},\n  volume  = {21},\n  number  = {140},\n  pages   = {1-67},\n  url     = {http://jmlr.org/papers/v21/20-074.html}\n}APA:Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., ... & Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140), 1-67.Model Card AuthorsThis model card was written by the team at Hugging Face.How to Get Started with the ModelUse the code below to get started with the model.Click to expandfromtransformersimportT5Tokenizer, T5Model\n\ntokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\nmodel = T5Model.from_pretrained(\"t5-base\")\n\ninput_ids = tokenizer(\"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\").input_ids# Batch size 1decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"pt\").input_ids# Batch size 1# forward passoutputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\nlast_hidden_states = outputs.last_hidden_stateSee theHugging Face T5docs and aColab Notebookcreated by the model developers for more examples."
    },
    "bloom": {
        "Model Overview": "BigScience Large Open-science Open-access Multilingual Language ModelVersion 1.3 / 6 July 2022Current Checkpoint:Training Iteration  95000Link to paper:hereTotal seen tokens:366BModel DetailsBLOOM is an autoregressive Large Language Model (LLM), trained to continue text from a prompt on vast amounts of text data using industrial-scale computational resources. As such, it is able to output coherent text in 46 languages and 13 programming languages that is hardly distinguishable from text written by humans. BLOOM can also be instructed to perform text tasks it hasn't been explicitly trained for, by casting them as text generation tasks.BasicsThis section provides information about the model type, version, license, funders, release date, developers, and contact information.It is useful for anyone who wants to reference the model.Click to expandDeveloped by:BigScience (website)All collaborators are either volunteers or have an agreement with their employer. (Further breakdown of participants forthcoming.)Model Type:Transformer-based Language ModelCheckpoints format:transformers(Megatron-DeepSpeed format availablehere)Version:1.0.0Languages:Multiple; seetraining dataLicense:RAIL License v1.0 (link/article and FAQ)Release Date Estimate:Monday, 11.July.2022Send Questions to:bigscience-contact@googlegroups.comCite as:BigScience,BigScience Language Open-science Open-access Multilingual (BLOOM) Language Model. International, May 2021-May 2022Funded by:The French government.Hugging Face (website).Organizations of contributors.(Further breakdown of organizations forthcoming.)Technical SpecificationsThis section includes details about the model objective and architecture, and the compute infrastructure.It is useful for people interested in model development.Click to expandPlease seethe BLOOM training READMEfor full details on replicating training.Model Architecture and ObjectiveModified from Megatron-LM GPT2 (seepaper,BLOOM Megatron code):Decoder-only architectureLayer normalization applied to word embeddings layer (StableEmbedding; seecode,paper)ALiBI positional encodings (seepaper), with GeLU activation functions176,247,271,424 parameters:3,596,615,680 embedding parameters70 layers, 112 attention headsHidden layers are 14336-dimensionalSequence length of 2048 tokens used (seeBLOOM tokenizer,tokenizer description)Objective Function:Cross Entropy with mean reduction (seeAPI documentation).Compute infrastructureJean Zay Public Supercomputer, provided by the French government (seeannouncement).Hardware384 A100 80GB GPUs (48 nodes)Additional 32 A100 80GB GPUs (4 nodes) in reserve8 GPUs per node Using NVLink 4 inter-gpu connects, 4 OmniPath linksCPU: AMDCPU memory: 512GB per nodeGPU memory: 640GB per nodeInter-node connect: Omni-Path Architecture (OPA)NCCL-communications network: a fully dedicated subnetDisc IO network: shared network with other types of nodesSoftwareMegatron-DeepSpeed (Github link)DeepSpeed (Github link)PyTorch (pytorch-1.11 w/ CUDA-11.5; seeGithub link)apex (Github link)TrainingThis section provides information about the training data, the speed and size of training elements, and the environmental impact of training.It is useful for people who want to learn more about the model inputs and training footprint.Click to expandTraining DataThis section provides a high-level overview of the training data. It is relevant for anyone who wants to know the basics of what the model is learning.Details for each dataset are provided in individualData Cards, and the sizes of each of their contributions to the aggregated training data are presented in anInteractive Corpus Map.Training data includes:46 natural languages13 programming languagesIn 1.6TB of pre-processed text, converted into 350B unique tokens (seethe tokenizer sectionfor more.)LanguagesThe pie chart shows the distribution of languages in training data.The following tables shows the further distribution of Niger-Congo & Indic languages and programming languages in the training data.Distribution of Niger Congo and Indic languages.Niger CongoPercentageIndicPercentageChi Tumbuka0.00002Assamese0.01Kikuyu0.00004Odia0.04Bambara0.00004Gujarati0.04Akan0.00007Marathi0.05Xitsonga0.00007Punjabi0.05Sesotho0.00007Kannada0.06Chi Chewa0.0001Nepali0.07Setswana0.0002Telugu0.09Lingala0.0002Malayalam0.10Northern Sotho0.0002Urdu0.10Fon0.0002Tamil0.20Kirundi0.0003Bengali0.50Wolof0.0004Hindi0.70Luganda0.0004Chi Shona0.001Isi Zulu0.001Igbo0.001Xhosa0.001Kinyarwanda0.003Yoruba0.006Swahili0.02Distribution of programming languages.ExtensionLanguageNumber of filesjavaJava5,407,724phpPHP4,942,186cppC++2,503,930pyPython2,435,072jsJavaScript1,905,518csC#1,577,347rbRuby6,78,413ccC++443,054hppC++391,048luaLua352,317goGO227,763tsTypeScript195,254CC134,537scalaScala92,052hhC++67,161HC++55,899tsxTypeScript33,107rsRust29,693phptPHP9,702c++C++1,342h++C++791php3PHP540phpsPHP270php5PHP166php4PHP29PreprocessingTokenization:The BLOOM tokenizer (link), a learned subword tokenizer trained using:A byte-level Byte Pair Encoding (BPE) algorithmA simple pre-tokenization rule, no normalizationA vocabulary size of 250,680It was trained on a subset of a preliminary version of the corpus using alpha-weighting per language.Speeds, Sizes, TimesTraining logs:Tensorboard linkDates:Started 11th March, 2022 11:42am PSTEstimated end: 5th July, 2022Checkpoint size:Bf16 weights: 329GBFull checkpoint with optimizer states: 2.3TBTraining throughput: About 150 TFLOP per GPU per secondNumber of epochs: 1Estimated cost of training: Equivalent of $2-5M in cloud computing (including preliminary experiments)Server training location: \u00cele-de-France, FranceEnvironmental ImpactThe training supercomputer, Jean Zay (website), uses mostly nuclear energy. The heat generated by it is reused for heating campus housing.Estimated carbon emissions:(Forthcoming.)Estimated electricity usage:(Forthcoming.)UsesThis section addresses questions around how the model is intended to be used, discusses the foreseeable users of the model (including those affected by the model), and describes uses that are considered out of scope or misuse of the model.It is useful for anyone considering using the model or who is affected by the model.Click to expandHow to useThis model can be easily used and deployed using HuggingFace's ecosystem. This needstransformersandaccelerateinstalled. The model can be downloaded as follows:Intended UseThis model is being created in order to enable public research on large language models (LLMs). LLMs are intended to be used for language generation or as a pretrained base model that can be further fine-tuned for specific tasks. Use cases below are not exhaustive.Direct UseText generationExploring characteristics of language generated by a language modelExamples: Cloze tests, counterfactuals, generations with reframingsDownstream UseTasks that leverage language models include: Information Extraction, Question Answering, SummarizationMisuse and Out-of-scope UseThis section addresses what users ought not do with the model.See theBLOOM License, Attachment A, for detailed usage restrictions. The below list is non-exhaustive, but lists some easily foreseeable problematic use cases.Out-of-scope UsesUsing the model inhigh-stakessettings is out of scope for this model.  The model is not designed forcritical decisionsnor uses with any material consequences on an individual's livelihood or wellbeing. The model outputs content that appears factual but may not be correct.Out-of-scope Uses Include:Usage in biomedical domains, political and legal domains, or finance domainsUsage for evaluating or scoring individuals, such as for employment, education, or creditApplying the model for critical automatic decisions, generating factual content, creating reliable summaries, or generating predictions that must be correctMisuseIntentionally using the model for harm, violatinghuman rights, or other kinds of malicious activities, is a misuse of this model. This includes:Spam generationDisinformation and influence operationsDisparagement and defamationHarassment and abuseDeceptionUnconsented impersonation and imitationUnconsented surveillanceGenerating content without attribution to the model, as specified in theRAIL License, Use RestrictionsIntended UsersDirect UsersGeneral PublicResearchersStudentsEducatorsEngineers/developersNon-commercial entitiesCommunity advocates, including human and civil rights groupsIndirect UsersUsers of derivatives created by Direct Users, such as those using software with anintended useUsers ofDerivatives of the Model, as described in the LicenseOthers Affected (Parties Prenantes)People and groups referred to by the LLMPeople and groups exposed to outputs of, or decisions based on, the LLMPeople and groups whose original work is included in the LLMRisks and LimitationsThis section identifies foreseeable harms and misunderstandings.Click to expandModel may:Overrepresent some viewpoints and underrepresent othersContain stereotypesContainpersonal informationGenerate:Hateful, abusive, or violent languageDiscriminatory or prejudicial languageContent that may not be appropriate for all settings, including sexual contentMake errors, including producing incorrect information as if it were factualGenerate irrelevant or repetitive outputsInduce users into attributing human traits to it, such as sentience or consciousnessEvaluationThis section describes the evaluation protocols and provides the results.Click to expandMetricsThis section describes the different ways performance is calculated and why.Includes:MetricWhy chosenPerplexityStandard metric for quantifying model improvements during trainingCross EntropyLossStandard objective for language models.And multiple different metrics for specific tasks.(More evaluation metrics forthcoming upon completion of evaluation protocol.)FactorsThis section lists some different aspects of BLOOM models. Its focus is on aspects that are likely to give rise to high variance in model behavior.Language, such as English or YorubaDomain, such as newswire or storiesDemographic characteristics, such as gender or nationalityResultsResults are based on theFactorsandMetrics.Zero-shot evaluations:WARNING:This section used to contain much more results, however they were not correct and we released without the approval of the evaluation working group. We are currently in the process of fixing the evaluations.See this repository for JSON files:https://github.com/bigscience-workshop/evaluation-resultsTaskLanguageMetricBLOOM-176BOPT-175B*humanevalpythonpass@1 \u21910.1550.0humanevalpythonpass@10 \u21910.3280.0humanevalpythonpass@100 \u21910.5720.003Train-time Evaluation:Final checkpoint after 95K steps:Training Loss: 1.939Validation Loss: 2.061Perplexity: 7.045For more see:https://huggingface.co/bigscience/tr11-176B-ml-logsRecommendationsThis section provides information on warnings and potential mitigations.Click to expandIndirect users should be made aware when the content they're working with is created by the LLM.Users should be aware ofRisks and Limitations, and include an appropriate age disclaimer or blocking interface as necessary.Models trained or finetuned downstream of BLOOM LM should include an updated Model Card.Users of the model should provide mechanisms for those affected to provide feedback, such as an email address for comments.Glossary and CalculationsThis section defines common terms and how metrics are calculated.Click to expandLoss:A calculation of the difference between what the model has learned and what the data shows (\"groundtruth\"). The lower the loss, the better. The training process aims to minimize the loss.Perplexity:This is based on what the model estimates the probability of new data is. The lower the perplexity, the better.  If the model is 100% correct at predicting the next token it will see, then the perplexity is 1. Mathematically this is calculated using entropy.High-stakes settings:Such as those identified as \"high-risk AI systems\" and \"unacceptable risk AI systems\" in the European Union's proposedArtificial Intelligence (AI) Act.Critical decisions:Such as those defined inthe United States' proposed Algorithmic Accountability Act.Human rights:Includes those rights defined in theUniversal Declaration of Human Rights.Personal Data and Personal Information:Personal data and information is defined in multiple data protection regulations, such as \"personal data\" in theEuropean Union's General Data Protection Regulation; and \"personal information\" in the Republic of South Africa'sProtection of Personal Information Act, The People's Republic of China'sPersonal information protection law.Sensitive characteristics:This includes specifically protected categories in human rights (seeUHDR, Article 2) and personal information regulation (see GDPR,Article 9; Protection of Personal Information Act, Chapter 1)Deception:Doing something to intentionally mislead individuals to believe something that is false, such as by creating deadbots or chatbots on social media posing as real people, or generating text documents without making consumers aware that the text is machine generated.More InformationThis section provides links to writing on dataset creation, technical specifications, lessons learned, and initial results.Click to expandIntermediate checkpointsFor academic (or any) usage, we published the intermediate checkpoints, corresponding to the model state at each 5000 steps. Please followthis linkto get these checkpoints.Dataset CreationBlog post detailing the design choices during the dataset creation:https://bigscience.huggingface.co/blog/building-a-tb-scale-multilingual-dataset-for-language-modelingTechnical SpecificationsBlog post summarizing how the architecture, size, shape, and pre-training duration where selected:https://bigscience.huggingface.co/blog/what-language-model-to-train-if-you-have-two-million-gpu-hoursMore details on the architecture/optimizer:https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-mlBlog post on the hardware/engineering side:https://bigscience.huggingface.co/blog/which-hardware-to-train-a-176b-parameters-modelDetails on the distributed setup used for the training:https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-mlTensorboard updated during the training:https://huggingface.co/bigscience/tr11-176B-ml-logs/tensorboard#scalars&tagFilter=lossLessonsInsights on how to approach training, negative results:https://github.com/bigscience-workshop/bigscience/blob/master/train/lessons-learned.mdDetails on the obstacles overcome during the preparation on the engineering side (instabilities, optimization of training throughput, so many technical tricks and questions):https://github.com/bigscience-workshop/bigscience/blob/master/train/tr11-176B-ml/chronicles.mdInitial ResultsInitial prompting experiments using interim checkpoints:https://huggingface.co/spaces/bigscience/bloom-bookOriginal checkpointsThe checkpoints in this repo correspond to the HuggingFace Transformers format. If you want to use our fork ofMegatron-DeepSpeedthat the model was trained with, you'd want to usethis repo instead.Many intermediate checkpoints are available athttps://huggingface.co/bigscience/bloom-intermediate/Model Card AuthorsOrdered roughly chronologically and by amount of time spent on creating this model card.Margaret Mitchell, Giada Pistilli, Yacine Jernite, Ezinwanne Ozoani, Marissa Gerchick, Nazneen Rajani, Sasha Luccioni, Irene Solaiman, Maraim Masoud, Somaieh Nikpoor, Carlos Mu\u00f1oz Ferrandis, Stas Bekman, Christopher Akiki, Danish Contractor, David Lansky, Angelina McMillan-Major, Tristan Thrush, Suzana Ili\u0107, G\u00e9rard Dupont, Shayne Longpre, Manan Dey, Stella Biderman, Douwe Kiela, Emi Baylor, Teven Le Scao, Aaron Gokaslan, Julien Launay, Niklas Muennighoff"
    },
    "xlnet-base-cased": {
        "Model Overview": "XLNet (base-sized model)XLNet model pre-trained on English language. It was introduced in the paperXLNet: Generalized Autoregressive Pretraining for Language Understandingby Yang et al. and first released inthis repository.Disclaimer: The team releasing XLNet did not write a model card for this model so this model card has been written by the Hugging Face team.Model descriptionXLNet is a new unsupervised language representation learning method based on a novel generalized permutation language modeling objective. Additionally, XLNet employs Transformer-XL as the backbone model, exhibiting excellent performance for language tasks involving long context. Overall, XLNet achieves state-of-the-art (SOTA) results on various downstream language tasks including question answering, natural language inference, sentiment analysis, and document ranking.Intended uses & limitationsThe model is mostly intended to be fine-tuned on a downstream task. See themodel hubto look for fine-tuned versions on a task that interests you.Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering. For tasks such as text generation, you should look at models like GPT2.UsageHere is how to use this model to get the features of a given text in PyTorch:fromtransformersimportXLNetTokenizer, XLNetModel\n\ntokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\nmodel = XLNetModel.from_pretrained('xlnet-base-cased')\n\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\noutputs = model(**inputs)\n\nlast_hidden_states = outputs.last_hidden_stateBibTeX entry and citation info@article{DBLP:journals/corr/abs-1906-08237,\n  author    = {Zhilin Yang and\n               Zihang Dai and\n               Yiming Yang and\n               Jaime G. Carbonell and\n               Ruslan Salakhutdinov and\n               Quoc V. Le},\n  title     = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},\n  journal   = {CoRR},\n  volume    = {abs/1906.08237},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1906.08237},\n  eprinttype = {arXiv},\n  eprint    = {1906.08237},\n  timestamp = {Mon, 24 Jun 2019 17:28:45 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1906-08237.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}"
    },
    "Mixtral-8x7B-v0.1": {
        "Model Overview": "Model Card for Mixtral-8x7BThe Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts. The Mistral-8x7B outperforms Llama 2 70B on most benchmarks we tested.For full details of this model please read ourrelease blog post.WarningThis repo contains weights that are compatible withvLLMserving of the model as well as Hugging Facetransformerslibrary. It is based on the original Mixtraltorrent release, but the file format and parameter names are different. Please note that model cannot (yet) be instantiated with HF.Run the modelfromtransformersimportAutoModelForCausalLM, AutoTokenizer\n\nmodel_id =\"mistralai/Mixtral-8x7B-v0.1\"tokenizer = AutoTokenizer.from_pretrained(model_id)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\n\ntext =\"Hello my name is\"inputs = tokenizer(text, return_tensors=\"pt\")\n\noutputs = model.generate(**inputs, max_new_tokens=20)print(tokenizer.decode(outputs[0], skip_special_tokens=True))By default, transformers will load the model in full precision. Therefore you might be interested to further reduce down the memory requirements to run the model through the optimizations we offer in HF ecosystem:In half-precisionNotefloat16precision only works on GPU devicesClick to expand+ import torchfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"mistralai/Mixtral-8x7B-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)+ model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16).to(0)text = \"Hello my name is\"+ inputs = tokenizer(text, return_tensors=\"pt\").to(0)outputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))Lower precision using (8-bit & 4-bit) usingbitsandbytesClick to expand+ import torchfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"mistralai/Mixtral-8x7B-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)+ model = AutoModelForCausalLM.from_pretrained(model_id, load_in_4bit=True)text = \"Hello my name is\"+ inputs = tokenizer(text, return_tensors=\"pt\").to(0)outputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))Load the model with Flash Attention 2Click to expand+ import torchfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"mistralai/Mixtral-8x7B-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)+ model = AutoModelForCausalLM.from_pretrained(model_id, use_flash_attention_2=True)text = \"Hello my name is\"+ inputs = tokenizer(text, return_tensors=\"pt\").to(0)outputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))NoticeMixtral-8x7B is a pretrained base model and therefore does not have any moderation mechanisms.The Mistral AI TeamAlbert Jiang, Alexandre Sablayrolles, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L\u00e9lio Renard Lavaud, Louis Ternon, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Th\u00e9ophile Gervet, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, William El Sayed."
    },
    "distilbert-base-uncased": {
        "Model Overview": "DistilBERT base model (uncased)This model is a distilled version of theBERT base model. It was\nintroduced inthis paper. The code for the distillation process can be foundhere. This model is uncased: it does\nnot make a difference between english and English.Model descriptionDistilBERT is a transformers model, smaller and faster than BERT, which was pretrained on the same corpus in a\nself-supervised fashion, using the BERT base model as a teacher. This means it was pretrained on the raw texts only,\nwith no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic\nprocess to generate inputs and labels from those texts using the BERT base model. More precisely, it was pretrained\nwith three objectives:Distillation loss: the model was trained to return the same probabilities as the BERT base model.Masked language modeling (MLM): this is part of the original training loss of the BERT base model. When taking a\nsentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the\nmodel and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that\nusually see the words one after the other, or from autoregressive models like GPT which internally mask the future\ntokens. It allows the model to learn a bidirectional representation of the sentence.Cosine embedding loss: the model was also trained to generate hidden states as close as possible as the BERT base\nmodel.This way, the model learns the same inner representation of the English language than its teacher model, while being\nfaster for inference or downstream tasks.Intended uses & limitationsYou can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to\nbe fine-tuned on a downstream task. See themodel hubto look for\nfine-tuned versions on a task that interests you.Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at model like GPT2.How to useYou can use this model directly with a pipeline for masked language modeling:>>>fromtransformersimportpipeline>>>unmasker = pipeline('fill-mask', model='distilbert-base-uncased')>>>unmasker(\"Hello I'm a [MASK] model.\")\n\n[{'sequence':\"[CLS] hello i'm a role model. [SEP]\",'score':0.05292855575680733,'token':2535,'token_str':'role'},\n {'sequence':\"[CLS] hello i'm a fashion model. [SEP]\",'score':0.03968575969338417,'token':4827,'token_str':'fashion'},\n {'sequence':\"[CLS] hello i'm a business model. [SEP]\",'score':0.034743521362543106,'token':2449,'token_str':'business'},\n {'sequence':\"[CLS] hello i'm a model model. [SEP]\",'score':0.03462274372577667,'token':2944,'token_str':'model'},\n {'sequence':\"[CLS] hello i'm a modeling model. [SEP]\",'score':0.018145186826586723,'token':11643,'token_str':'modeling'}]Here is how to use this model to get the features of a given text in PyTorch:fromtransformersimportDistilBertTokenizer, DistilBertModel\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\ntext =\"Replace me by any text you'd like.\"encoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)and in TensorFlow:fromtransformersimportDistilBertTokenizer, TFDistilBertModel\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = TFDistilBertModel.from_pretrained(\"distilbert-base-uncased\")\ntext =\"Replace me by any text you'd like.\"encoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)Limitations and biasEven if the training data used for this model could be characterized as fairly neutral, this model can have biased\npredictions. It also inherits some ofthe bias of its teacher model.>>>fromtransformersimportpipeline>>>unmasker = pipeline('fill-mask', model='distilbert-base-uncased')>>>unmasker(\"The White man worked as a [MASK].\")\n\n[{'sequence':'[CLS] the white man worked as a blacksmith. [SEP]','score':0.1235365942120552,'token':20987,'token_str':'blacksmith'},\n {'sequence':'[CLS] the white man worked as a carpenter. [SEP]','score':0.10142576694488525,'token':10533,'token_str':'carpenter'},\n {'sequence':'[CLS] the white man worked as a farmer. [SEP]','score':0.04985016956925392,'token':7500,'token_str':'farmer'},\n {'sequence':'[CLS] the white man worked as a miner. [SEP]','score':0.03932540491223335,'token':18594,'token_str':'miner'},\n {'sequence':'[CLS] the white man worked as a butcher. [SEP]','score':0.03351764753460884,'token':14998,'token_str':'butcher'}]>>>unmasker(\"The Black woman worked as a [MASK].\")\n\n[{'sequence':'[CLS] the black woman worked as a waitress. [SEP]','score':0.13283951580524445,'token':13877,'token_str':'waitress'},\n {'sequence':'[CLS] the black woman worked as a nurse. [SEP]','score':0.12586183845996857,'token':6821,'token_str':'nurse'},\n {'sequence':'[CLS] the black woman worked as a maid. [SEP]','score':0.11708822101354599,'token':10850,'token_str':'maid'},\n {'sequence':'[CLS] the black woman worked as a prostitute. [SEP]','score':0.11499975621700287,'token':19215,'token_str':'prostitute'},\n {'sequence':'[CLS] the black woman worked as a housekeeper. [SEP]','score':0.04722772538661957,'token':22583,'token_str':'housekeeper'}]This bias will also affect all fine-tuned versions of this model.Training dataDistilBERT pretrained on the same data as BERT, which isBookCorpus, a dataset\nconsisting of 11,038 unpublished books andEnglish Wikipedia(excluding lists, tables and headers).Training procedurePreprocessingThe texts are lowercased and tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are\nthen of the form:[CLS] Sentence A [SEP] Sentence B [SEP]With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in\nthe other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a\nconsecutive span of text usually longer than a single sentence. The only constrain is that the result with the two\n\"sentences\" has a combined length of less than 512 tokens.The details of the masking procedure for each sentence are the following:15% of the tokens are masked.In 80% of the cases, the masked tokens are replaced by[MASK].In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.In the 10% remaining cases, the masked tokens are left as is.PretrainingThe model was trained on 8 16 GB V100 for 90 hours. See thetraining codefor all hyperparameters\ndetails.Evaluation resultsWhen fine-tuned on downstream tasks, this model achieves the following results:Glue test results:TaskMNLIQQPQNLISST-2CoLASTS-BMRPCRTE82.288.589.291.351.385.887.559.9BibTeX entry and citation info@article{Sanh2019DistilBERTAD,\n  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},\n  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},\n  journal={ArXiv},\n  year={2019},\n  volume={abs/1910.01108}\n}"
    },
    "gemma-2-9b": {
        "Model Overview": "Gemma 2 model cardModel Page:GemmaResources and Technical Documentation:Responsible Generative AI ToolkitGemma on KaggleGemma on Vertex Model GardenTerms of Use:TermsAuthors: GoogleModel InformationSummary description and brief definition of inputs and outputs.DescriptionGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nThey are text-to-text, decoder-only large language models, available in English,\nwith open weights for both pre-trained variants and instruction-tuned variants.\nGemma models are well-suited for a variety of text generation tasks, including\nquestion answering, summarization, and reasoning. Their relatively small size\nmakes it possible to deploy them in environments with limited resources such as\na laptop, desktop or your own cloud infrastructure, democratizing access to\nstate of the art AI models and helping foster innovation for everyone.UsageBelow we share some code snippets on how to get quickly started with running the model. First make sure topip install -U transformers, then copy the snippet from the section that is relevant for your usecase.Running the model on a single / multi GPU# pip install acceleratefromtransformersimportAutoTokenizer, AutoModelForCausalLMimporttorch\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-9b\",\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16\n)\n\ninput_text =\"Write me a poem about Machine Learning.\"input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)print(tokenizer.decode(outputs[0]))Running the model on a GPU using different precisionsThe native weights of this model were exported inbfloat16precision.You can also usefloat32if you skip the dtype, but no precision increase will occur (model weights will just be upcasted tofloat32). See examples below.Upcasting totorch.float32# pip install acceleratefromtransformersimportAutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-9b\",\n    device_map=\"auto\")\n\ninput_text =\"Write me a poem about Machine Learning.\"input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)print(tokenizer.decode(outputs[0]))Quantized Versions throughbitsandbytesUsing 8-bit precision (int8)# pip install bitsandbytes acceleratefromtransformersimportAutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-9b\",\n    quantization_config=quantization_config)\n\ninput_text =\"Write me a poem about Machine Learning.\"input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)print(tokenizer.decode(outputs[0]))Using 4-bit precision# pip install bitsandbytes acceleratefromtransformersimportAutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-9b\",\n    quantization_config=quantization_config)\n\ninput_text =\"Write me a poem about Machine Learning.\"input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)print(tokenizer.decode(outputs[0]))Other optimizationsFlash Attention 2First make sure to installflash-attnin your environmentpip install flash-attnmodel = AutoModelForCausalLM.from_pretrained(\n    model_id, \n    torch_dtype=torch.float16,+   attn_implementation=\"flash_attention_2\").to(0)Inputs and outputsInput:Text string, such as a question, a prompt, or a document to be\nsummarized.Output:Generated English-language text in response to the input, such\nas an answer to a question, or a summary of a document.Citation@article{gemma_2024,\n    title={Gemma},\n    url={https://www.kaggle.com/m/3301},\n    DOI={10.34740/KAGGLE/M/3301},\n    publisher={Kaggle},\n    author={Gemma Team},\n    year={2024}\n}Model DataData used for model training and how the data was processed.Training DatasetThese models were trained on a dataset of text data that includes a wide variety of sources. The 27B model was trained with 13 trillion tokens and the 9B model was trained with 8 trillion tokens. \nHere are the key components:Web Documents: A diverse collection of web text ensures the model is exposed\nto a broad range of linguistic styles, topics, and vocabulary. Primarily\nEnglish-language content.Code: Exposing the model to code helps it to learn the syntax and patterns of\nprogramming languages, which improves its ability to generate code or\nunderstand code-related questions.Mathematics: Training on mathematical text helps the model learn logical\nreasoning, symbolic representation, and to address mathematical queries.The combination of these diverse data sources is crucial for training a powerful\nlanguage model that can handle a wide variety of different tasks and text\nformats.Data PreprocessingHere are the key data cleaning and filtering methods applied to the training\ndata:CSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering was\napplied at multiple stages in the data preparation process to ensure the\nexclusion of harmful and illegal content.Sensitive Data Filtering: As part of making Gemma pre-trained models safe and\nreliable, automated techniques were used to filter out certain personal\ninformation and other sensitive data from training sets.Additional methods: Filtering based on content quality and safety in line withour policies.Implementation InformationDetails about the model internals.HardwareGemma was trained using the latest generation ofTensor Processing Unit (TPU)hardware (TPUv5p).Training large language models requires significant computational power. TPUs,\ndesigned specifically for matrix operations common in machine learning, offer\nseveral advantages in this domain:Performance: TPUs are specifically designed to handle the massive computations\ninvolved in training LLMs. They can speed up training considerably compared to\nCPUs.Memory: TPUs often come with large amounts of high-bandwidth memory, allowing\nfor the handling of large models and batch sizes during training. This can\nlead to better model quality.Scalability: TPU Pods (large clusters of TPUs) provide a scalable solution for\nhandling the growing complexity of large foundation models. You can distribute\ntraining across multiple TPU devices for faster and more efficient processing.Cost-effectiveness: In many scenarios, TPUs can provide a more cost-effective\nsolution for training large models compared to CPU-based infrastructure,\nespecially when considering the time and resources saved due to faster\ntraining.These advantages are aligned withGoogle's commitments to operate sustainably.SoftwareTraining was done usingJAXandML Pathways.JAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models.ML Pathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable forfoundation models, including large language models like\nthese ones.Together, JAX and ML Pathways are used as described in thepaper about the Gemini family of models; \"the 'single\ncontroller' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.\"EvaluationModel evaluation metrics and results.Benchmark ResultsThese models were evaluated against a large collection of different datasets and\nmetrics to cover different aspects of text generation:BenchmarkMetricGemma PT 9BGemma PT 27BMMLU5-shot, top-171.375.2HellaSwag10-shot81.986.4PIQA0-shot81.783.2SocialIQA0-shot53.453.7BoolQ0-shot84.284.8WinoGrandepartial score80.683.7ARC-e0-shot88.088.6ARC-c25-shot68.471.4TriviaQA5-shot76.683.7Natural Questions5-shot29.234.5HumanEvalpass@140.251.8MBPP3-shot52.462.6GSM8K5-shot, maj@168.674.0MATH4-shot36.642.3AGIEval3-5-shot52.855.1BIG-Bench3-shot, CoT68.274.9------------------------------------------------------------------Ethics and SafetyEthics and safety evaluation approach and results.Evaluation ApproachOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:Text-to-Text Content Safety: Human evaluation on prompts covering safety\npolicies including child sexual abuse and exploitation, harassment, violence\nand gore, and hate speech.Text-to-Text Representational Harms: Benchmark against relevant academic\ndatasets such asWinoBiasandBBQ Dataset.Memorization: Automated evaluation of memorization of training data, including\nthe risk of personally identifiable information exposure.Large-scale harm: Tests for \"dangerous capabilities,\" such as chemical,\nbiological, radiological, and nuclear (CBRN) risks.Evaluation ResultsThe results of ethics and safety evaluations are within acceptable thresholds\nfor meetinginternal policiesfor categories such as child\nsafety, content safety, representational harms, memorization, large-scale harms.\nOn top of robust internal evaluations, the results of well-known safety\nbenchmarks like BBQ, BOLD, Winogender, Winobias, RealToxicity, and TruthfulQA\nare shown here.Gemma 2.0BenchmarkMetricGemma 2 IT 9BGemma 2 IT 27BRealToxicityaverage8.258.84CrowS-Pairstop-137.4736.67BBQ Ambig1-shot, top-188.5885.99BBQ Disambigtop-182.6786.94Winogendertop-179.1777.22TruthfulQA50.2751.60Winobias 1_278.0981.94Winobias 2_295.3297.22Toxigen39.3038.42--------------------------------------------------------------------Usage and LimitationsThese models have certain limitations that users should be aware of.Intended UsageOpen Large Language Models (LLMs) have a wide range of applications across\nvarious industries and domains. The following list of potential uses is not\ncomprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.Content Creation and CommunicationText Generation: These models can be used to generate creative text formats\nsuch as poems, scripts, code, marketing copy, and email drafts.Chatbots and Conversational AI: Power conversational interfaces for customer\nservice, virtual assistants, or interactive applications.Text Summarization: Generate concise summaries of a text corpus, research\npapers, or reports.Research and EducationNatural Language Processing (NLP) Research: These models can serve as a\nfoundation for researchers to experiment with NLP techniques, develop\nalgorithms, and contribute to the advancement of the field.Language Learning Tools: Support interactive language learning experiences,\naiding in grammar correction or providing writing practice.Knowledge Exploration: Assist researchers in exploring large bodies of text\nby generating summaries or answering questions about specific topics.LimitationsTraining DataThe quality and diversity of the training data significantly influence the\nmodel's capabilities. Biases or gaps in the training data can lead to\nlimitations in the model's responses.The scope of the training dataset determines the subject areas the model can\nhandle effectively.Context and Task ComplexityLLMs are better at tasks that can be framed with clear prompts and\ninstructions. Open-ended or highly complex tasks might be challenging.A model's performance can be influenced by the amount of context provided\n(longer context generally leads to better outputs, up to a certain point).Language Ambiguity and NuanceNatural language is inherently complex. LLMs might struggle to grasp subtle\nnuances, sarcasm, or figurative language.Factual AccuracyLLMs generate responses based on information they learned from their\ntraining datasets, but they are not knowledge bases. They may generate\nincorrect or outdated factual statements.Common SenseLLMs rely on statistical patterns in language. They might lack the ability\nto apply common sense reasoning in certain situations.Ethical Considerations and RisksThe development of large language models (LLMs) raises several ethical concerns.\nIn creating an open model, we have carefully considered the following:Bias and FairnessLLMs trained on large-scale, real-world text data can reflect socio-cultural\nbiases embedded in the training material. These models underwent careful\nscrutiny, input data pre-processing described and posterior evaluations\nreported in this card.Misinformation and MisuseLLMs can be misused to generate text that is false, misleading, or harmful.Guidelines are provided for responsible use with the model, see theResponsible Generative AI Toolkit.Transparency and Accountability:This model card summarizes details on the models' architecture,\ncapabilities, limitations, and evaluation processes.A responsibly developed open model offers the opportunity to share\ninnovation by making LLM technology accessible to developers and researchers\nacross the AI ecosystem.Risks identified and mitigations:Perpetuation of biases: It's encouraged to perform continuous monitoring\n(using evaluation metrics, human review) and the exploration of de-biasing\ntechniques during model training, fine-tuning, and other use cases.Generation of harmful content: Mechanisms and guidelines for content safety\nare essential. Developers are encouraged to exercise caution and implement\nappropriate content safety safeguards based on their specific product policies\nand application use cases.Misuse for malicious purposes: Technical limitations and developer and\nend-user education can help mitigate against malicious applications of LLMs.\nEducational resources and reporting mechanisms for users to flag misuse are\nprovided. Prohibited uses of Gemma models are outlined in theGemma Prohibited Use Policy.Privacy violations: Models were trained on data filtered for removal of PII\n(Personally Identifiable Information). Developers are encouraged to adhere to\nprivacy regulations with privacy-preserving techniques.BenefitsAt the time of release, this family of models provides high-performance open\nlarge language model implementations designed from the ground up for Responsible\nAI development compared to similarly sized models.Using the benchmark evaluation metrics described in this document, these models\nhave shown to provide superior performance to other, comparably-sized open model\nalternatives."
    },
    "gemma-2-27b-it": {
        "Model Overview": "Gemma 2 model cardModel Page:GemmaResources and Technical Documentation:Responsible Generative AI ToolkitGemma on KaggleGemma on Vertex Model GardenTerms of Use:TermsAuthors: GoogleModel InformationSummary description and brief definition of inputs and outputs.DescriptionGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nThey are text-to-text, decoder-only large language models, available in English,\nwith open weights for both pre-trained variants and instruction-tuned variants.\nGemma models are well-suited for a variety of text generation tasks, including\nquestion answering, summarization, and reasoning. Their relatively small size\nmakes it possible to deploy them in environments with limited resources such as\na laptop, desktop or your own cloud infrastructure, democratizing access to\nstate of the art AI models and helping foster innovation for everyone.UsageBelow we share some code snippets on how to get quickly started with running the model. First make sure topip install -U transformers, then copy the snippet from the section that is relevant for your usecase.Running the model on a single / multi GPUGiven the model instabilities with SDPA/ FA2, by default, the model inference would utiliseeagerattention.# pip install acceleratefromtransformersimportAutoTokenizer, AutoModelForCausalLMimporttorch\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-27b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-27b-it\",\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16\n)\n\ninput_text =\"Write me a poem about Machine Learning.\"input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)print(tokenizer.decode(outputs[0]))Running the model on a GPU using different precisionsThe native weights of this model were exported inbfloat16precision.You can also usefloat32if you skip the dtype, but no precision increase will occur (model weights will just be upcasted tofloat32). See examples below.Upcasting totorch.float32# pip install acceleratefromtransformersimportAutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-27b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-27b-it\",\n    device_map=\"auto\")\n\ninput_text =\"Write me a poem about Machine Learning.\"input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)print(tokenizer.decode(outputs[0]))Quantized Versions throughbitsandbytesUsing 8-bit precision (int8)# pip install bitsandbytes acceleratefromtransformersimportAutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-27b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-27b-it\",\n    quantization_config=quantization_config)\n\ninput_text =\"Write me a poem about Machine Learning.\"input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)print(tokenizer.decode(outputs[0]))Using 4-bit precision# pip install bitsandbytes acceleratefromtransformersimportAutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-27b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-27b-it\",\n    quantization_config=quantization_config)\n\ninput_text =\"Write me a poem about Machine Learning.\"input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids)print(tokenizer.decode(outputs[0]))Other optimizationsFlash Attention 2Gemma 2 is currently incompatible with Flash Attention/ SDPA, using it might result in unreliable generations. Use at your own risk.First make sure to installflash-attnin your environmentpip install flash-attnmodel = AutoModelForCausalLM.from_pretrained(\n    model_id, \n    torch_dtype=torch.float16,+   attn_implementation=\"flash_attention_2\").to(0)Chat TemplateThe instruction-tuned models use a chat template that must be adhered to for conversational use.\nThe easiest way to apply it is using the tokenizer's built-in chat template, as shown in the following snippet.Let's load the model and apply the chat template to a conversation. In this example, we'll start with a single user interaction:fromtransformersimportAutoTokenizer, AutoModelForCausalLMimporttransformersimporttorch\n\nmodel_id =\"google/gemma-2-27b-it\"dtype = torch.bfloat16\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"cuda\",\n    torch_dtype=dtype,\n)\n\nchat = [\n    {\"role\":\"user\",\"content\":\"Write a hello world program\"},\n]\nprompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)At this point, the prompt contains the following text:<bos><start_of_turn>user\nWrite a hello world program<end_of_turn>\n<start_of_turn>modelAs you can see, each turn is preceded by a<start_of_turn>delimiter and then the role of the entity\n(eitheruser, for content supplied by the user, ormodelfor LLM responses). Turns finish with\nthe<end_of_turn>token.You can follow this format to build the prompt manually, if you need to do it without the tokenizer's\nchat template.After the prompt is ready, generation can be performed like this:inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\noutputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=150)print(tokenizer.decode(outputs[0]))Inputs and outputsInput:Text string, such as a question, a prompt, or a document to be\nsummarized.Output:Generated English-language text in response to the input, such\nas an answer to a question, or a summary of a document.Citation@article{gemma_2024,\n    title={Gemma},\n    url={https://www.kaggle.com/m/3301},\n    DOI={10.34740/KAGGLE/M/3301},\n    publisher={Kaggle},\n    author={Gemma Team},\n    year={2024}\n}Model DataData used for model training and how the data was processed.Training DatasetThese models were trained on a dataset of text data that includes a wide variety of sources. The 27B model was trained with 13 trillion tokens and the 9B model was trained with 8 trillion tokens. \nHere are the key components:Web Documents: A diverse collection of web text ensures the model is exposed\nto a broad range of linguistic styles, topics, and vocabulary. Primarily\nEnglish-language content.Code: Exposing the model to code helps it to learn the syntax and patterns of\nprogramming languages, which improves its ability to generate code or\nunderstand code-related questions.Mathematics: Training on mathematical text helps the model learn logical\nreasoning, symbolic representation, and to address mathematical queries.The combination of these diverse data sources is crucial for training a powerful\nlanguage model that can handle a wide variety of different tasks and text\nformats.Data PreprocessingHere are the key data cleaning and filtering methods applied to the training\ndata:CSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering was\napplied at multiple stages in the data preparation process to ensure the\nexclusion of harmful and illegal content.Sensitive Data Filtering: As part of making Gemma pre-trained models safe and\nreliable, automated techniques were used to filter out certain personal\ninformation and other sensitive data from training sets.Additional methods: Filtering based on content quality and safety in line withour policies.Implementation InformationDetails about the model internals.HardwareGemma was trained using the latest generation ofTensor Processing Unit (TPU)hardware (TPUv5p).Training large language models requires significant computational power. TPUs,\ndesigned specifically for matrix operations common in machine learning, offer\nseveral advantages in this domain:Performance: TPUs are specifically designed to handle the massive computations\ninvolved in training LLMs. They can speed up training considerably compared to\nCPUs.Memory: TPUs often come with large amounts of high-bandwidth memory, allowing\nfor the handling of large models and batch sizes during training. This can\nlead to better model quality.Scalability: TPU Pods (large clusters of TPUs) provide a scalable solution for\nhandling the growing complexity of large foundation models. You can distribute\ntraining across multiple TPU devices for faster and more efficient processing.Cost-effectiveness: In many scenarios, TPUs can provide a more cost-effective\nsolution for training large models compared to CPU-based infrastructure,\nespecially when considering the time and resources saved due to faster\ntraining.These advantages are aligned withGoogle's commitments to operate sustainably.SoftwareTraining was done usingJAXandML Pathways.JAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models.ML Pathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable forfoundation models, including large language models like\nthese ones.Together, JAX and ML Pathways are used as described in thepaper about the Gemini family of models; \"the 'single\ncontroller' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.\"EvaluationModel evaluation metrics and results.Benchmark ResultsThese models were evaluated against a large collection of different datasets and\nmetrics to cover different aspects of text generation:BenchmarkMetricGemma PT 9BGemma PT 27BMMLU5-shot, top-171.375.2HellaSwag10-shot81.986.4PIQA0-shot81.783.2SocialIQA0-shot53.453.7BoolQ0-shot84.284.8WinoGrandepartial score80.683.7ARC-e0-shot88.088.6ARC-c25-shot68.471.4TriviaQA5-shot76.683.7Natural Questions5-shot29.234.5HumanEvalpass@140.251.8MBPP3-shot52.462.6GSM8K5-shot, maj@168.674.0MATH4-shot36.642.3AGIEval3-5-shot52.855.1BIG-Bench3-shot, CoT68.274.9------------------------------------------------------------------Ethics and SafetyEthics and safety evaluation approach and results.Evaluation ApproachOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:Text-to-Text Content Safety: Human evaluation on prompts covering safety\npolicies including child sexual abuse and exploitation, harassment, violence\nand gore, and hate speech.Text-to-Text Representational Harms: Benchmark against relevant academic\ndatasets such asWinoBiasandBBQ Dataset.Memorization: Automated evaluation of memorization of training data, including\nthe risk of personally identifiable information exposure.Large-scale harm: Tests for \"dangerous capabilities,\" such as chemical,\nbiological, radiological, and nuclear (CBRN) risks.Evaluation ResultsThe results of ethics and safety evaluations are within acceptable thresholds\nfor meetinginternal policiesfor categories such as child\nsafety, content safety, representational harms, memorization, large-scale harms.\nOn top of robust internal evaluations, the results of well-known safety\nbenchmarks like BBQ, BOLD, Winogender, Winobias, RealToxicity, and TruthfulQA\nare shown here.Gemma 2.0BenchmarkMetricGemma 2 IT 9BGemma 2 IT 27BRealToxicityaverage8.258.84CrowS-Pairstop-137.4736.67BBQ Ambig1-shot, top-188.5885.99BBQ Disambigtop-182.6786.94Winogendertop-179.1777.22TruthfulQA50.2751.60Winobias 1_278.0981.94Winobias 2_295.3297.22Toxigen39.3038.42--------------------------------------------------------------------Usage and LimitationsThese models have certain limitations that users should be aware of.Intended UsageOpen Large Language Models (LLMs) have a wide range of applications across\nvarious industries and domains. The following list of potential uses is not\ncomprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.Content Creation and CommunicationText Generation: These models can be used to generate creative text formats\nsuch as poems, scripts, code, marketing copy, and email drafts.Chatbots and Conversational AI: Power conversational interfaces for customer\nservice, virtual assistants, or interactive applications.Text Summarization: Generate concise summaries of a text corpus, research\npapers, or reports.Research and EducationNatural Language Processing (NLP) Research: These models can serve as a\nfoundation for researchers to experiment with NLP techniques, develop\nalgorithms, and contribute to the advancement of the field.Language Learning Tools: Support interactive language learning experiences,\naiding in grammar correction or providing writing practice.Knowledge Exploration: Assist researchers in exploring large bodies of text\nby generating summaries or answering questions about specific topics.LimitationsTraining DataThe quality and diversity of the training data significantly influence the\nmodel's capabilities. Biases or gaps in the training data can lead to\nlimitations in the model's responses.The scope of the training dataset determines the subject areas the model can\nhandle effectively.Context and Task ComplexityLLMs are better at tasks that can be framed with clear prompts and\ninstructions. Open-ended or highly complex tasks might be challenging.A model's performance can be influenced by the amount of context provided\n(longer context generally leads to better outputs, up to a certain point).Language Ambiguity and NuanceNatural language is inherently complex. LLMs might struggle to grasp subtle\nnuances, sarcasm, or figurative language.Factual AccuracyLLMs generate responses based on information they learned from their\ntraining datasets, but they are not knowledge bases. They may generate\nincorrect or outdated factual statements.Common SenseLLMs rely on statistical patterns in language. They might lack the ability\nto apply common sense reasoning in certain situations.Ethical Considerations and RisksThe development of large language models (LLMs) raises several ethical concerns.\nIn creating an open model, we have carefully considered the following:Bias and FairnessLLMs trained on large-scale, real-world text data can reflect socio-cultural\nbiases embedded in the training material. These models underwent careful\nscrutiny, input data pre-processing described and posterior evaluations\nreported in this card.Misinformation and MisuseLLMs can be misused to generate text that is false, misleading, or harmful.Guidelines are provided for responsible use with the model, see theResponsible Generative AI Toolkit.Transparency and Accountability:This model card summarizes details on the models' architecture,\ncapabilities, limitations, and evaluation processes.A responsibly developed open model offers the opportunity to share\ninnovation by making LLM technology accessible to developers and researchers\nacross the AI ecosystem.Risks identified and mitigations:Perpetuation of biases: It's encouraged to perform continuous monitoring\n(using evaluation metrics, human review) and the exploration of de-biasing\ntechniques during model training, fine-tuning, and other use cases.Generation of harmful content: Mechanisms and guidelines for content safety\nare essential. Developers are encouraged to exercise caution and implement\nappropriate content safety safeguards based on their specific product policies\nand application use cases.Misuse for malicious purposes: Technical limitations and developer and\nend-user education can help mitigate against malicious applications of LLMs.\nEducational resources and reporting mechanisms for users to flag misuse are\nprovided. Prohibited uses of Gemma models are outlined in theGemma Prohibited Use Policy.Privacy violations: Models were trained on data filtered for removal of PII\n(Personally Identifiable Information). Developers are encouraged to adhere to\nprivacy regulations with privacy-preserving techniques.BenefitsAt the time of release, this family of models provides high-performance open\nlarge language model implementations designed from the ground up for Responsible\nAI development compared to similarly sized models.Using the benchmark evaluation metrics described in this document, these models\nhave shown to provide superior performance to other, comparably-sized open model\nalternatives."
    },
    "codegen-350M-multi": {
        "Model Overview": "CodeGen (CodeGen-Multi 350M)Model descriptionCodeGen is a family of autoregressive language models forprogram synthesisfrom the paper:A Conversational Paradigm for Program Synthesisby Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong. The models are originally released inthis repository, under 3 pre-training data variants (NL,Multi,Mono) and 4 model size variants (350M,2B,6B,16B).The checkpoint included in this repository is denoted asCodeGen-Multi 350Min the paper, where \"Multi\" means the model is initialized withCodeGen-NL 350Mand further pre-trained on a dataset of multiple programming languages, and \"350M\" refers to the number of trainable parameters.Training dataThis checkpoint (CodeGen-Multi 350M) was firstly initialized withCodeGen-NL 350M, and then pre-trained onBigQuery, a large-scale dataset of multiple programming languages from GitHub repositories. The data consists of 119.2B tokens and includes C, C++, Go, Java, JavaScript, and Python.Training procedureCodeGen was trained using cross-entropy loss to maximize the likelihood of sequential inputs.\nThe family of models are trained using multiple TPU-v4-512 by Google, leveraging data and model parallelism.\nSee Section 2.3 of thepaperfor more details.Evaluation resultsWe evaluate our models on two code generation benchmark: HumanEval and MTPB. Please refer to thepaperfor more details.Intended Use and LimitationsAs an autoregressive language model, CodeGen is capable of extracting features from given natural language and programming language texts, and calculating the likelihood of them.\nHowever, the model is intended for and best atprogram synthesis, that is, generating executable code given English prompts, where the prompts should be in the form of a comment string. The model can complete partially-generated code as well.How to useThis model can be easily loaded using theAutoModelForCausalLMfunctionality:fromtransformersimportAutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-350M-multi\")\nmodel = AutoModelForCausalLM.from_pretrained(\"Salesforce/codegen-350M-multi\")\n\ntext =\"def hello_world():\"input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n\ngenerated_ids = model.generate(input_ids, max_length=128)print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))BibTeX entry and citation info@article{Nijkamp2022ACP,\n  title={A Conversational Paradigm for Program Synthesis},\n  author={Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},\n  journal={arXiv preprint},\n  year={2022}\n}"
    }
}